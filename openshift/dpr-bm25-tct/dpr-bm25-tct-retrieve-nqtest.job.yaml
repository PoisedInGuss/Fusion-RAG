# Incoming: shared-datasets (nq_test.json), shared-indexes (BM25 index, corpus TSV) --- {Dict,str}
# Processing: BM25 >> TCT-ColBERT two-stage retrieval --- {2 jobs: PyTerrier BM25, TCT bi-encoder reranking}
# Outgoing: shared-indexes (BM25_TCT_nqtest_all.res) --- {TREC runfile,str}

apiVersion: batch/v1
kind: Job
metadata:
  name: dpr-bm25-tct-retrieve-nqtest
  namespace: 425krish
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: bm25-tct
        image: image-registry.openshift-image-registry.svc:5000/425krish/bm25-monot5:latest
        imagePullPolicy: Always
        workingDir: /tmp
        env:
        - name: HOME
          value: /tmp
        - name: TMPDIR
          value: /tmp
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: CUDA_VISIBLE_DEVICES
          value: ""
        - name: NQ_PATH
          value: /mnt/datasets/dpr/nq_test.json
        - name: INDEX_DIR
          value: /mnt/index/dpr/index/pyterrier
        - name: TSV_PATH
          value: /mnt/datasets/dpr/data/wikipedia_split/psgs_w100/psgs_w100.tsv
        - name: RUN_PATH
          value: /mnt/index/dpr/runs/BM25_TCT_nqtest_all.res
        - name: TOP_K
          value: "100"
        - name: FIRST_STAGE_K
          value: "100"
        - name: BATCH_SIZE
          value: "64"
        - name: MINI_BATCH
          value: "50"
        - name: MODEL_NAME
          value: castorini/tct_colbert-v2-hnp-msmarco
        command: ["/bin/bash", "-lc"]
        args:
        - |
          set -euo pipefail
          echo "[bm25-tct] nq_test: ${NQ_PATH}"
          echo "[bm25-tct] index: ${INDEX_DIR}"
          echo "[bm25-tct] tsv: ${TSV_PATH}"
          
          python3 - << 'PY'
          import os
          import re
          import json
          import time
          from pathlib import Path
          
          import pandas as pd
          import numpy as np
          from sentence_transformers import SentenceTransformer
          import pyterrier as pt
          
          NQ_PATH = Path(os.environ["NQ_PATH"])
          INDEX_DIR = os.environ["INDEX_DIR"]
          TSV_PATH = Path(os.environ["TSV_PATH"])
          RUN_PATH = Path(os.environ["RUN_PATH"])
          TOP_K = int(os.environ.get("TOP_K", "100"))
          FIRST_STAGE_K = int(os.environ.get("FIRST_STAGE_K", "1000"))
          TCT_BATCH = int(os.environ.get("BATCH_SIZE", "64"))
          MINI_BATCH = int(os.environ.get("MINI_BATCH", "20"))
          MODEL_NAME = os.environ.get("MODEL_NAME", "castorini/tct_colbert-v2-hnp-msmarco")
          
          os.makedirs(os.environ.get("PYTERRIER_HOME", "/tmp/.pyterrier"), exist_ok=True)
          RUN_PATH.parent.mkdir(parents=True, exist_ok=True)
          
          # Initialize PyTerrier
          pt.init()
          print("[bm25-tct] PyTerrier initialized")
          
          def sanitize_query(q: str) -> str:
              """Remove special chars that break PyTerrier query parser."""
              q = re.sub(r"[^a-zA-Z0-9\s]", " ", q)
              q = re.sub(r"\s+", " ", q)
              return q.strip()
          
          # Load queries from nq_test.json
          print(f"[bm25-tct] loading queries from {NQ_PATH}...")
          data = json.load(NQ_PATH.open("r", encoding="utf-8"))
          items = data.get("data", [])
          queries = []
          for ex in items:
              q = (ex.get("question") or "").strip()
              if not q:
                  continue
              qid = str(ex.get("example_id"))
              if qid == "None":
                  continue
              queries.append({"qid": qid, "query": sanitize_query(q), "query_orig": q})
          
          print(f"[bm25-tct] {len(queries)} queries loaded")
          
          # Build corpus offset index for fast docid -> text lookup
          print(f"[bm25-tct] building corpus offset index...")
          corpus_offsets = {}
          with TSV_PATH.open("r", encoding="utf-8", errors="ignore") as f:
              offset = f.tell()
              header = f.readline()
              offset = f.tell()
              while True:
                  line_start = offset
                  line = f.readline()
                  if not line:
                      break
                  parts = line.split("\t", 1)
                  if parts:
                      docid = parts[0]
                      corpus_offsets[docid] = line_start
                  offset = f.tell()
          
          print(f"[bm25-tct] indexed {len(corpus_offsets):,} documents")
          
          # BM25 first stage - load index metadata into memory (eliminates I/O bottleneck)
          print(f"[bm25-tct] initializing BM25 retriever with in-memory index...")
          index = pt.IndexFactory.of(INDEX_DIR, memory=True)
          print(f"[bm25-tct] index loaded into memory")
          bm25 = pt.BatchRetrieve(
              index,
              wmodel="BM25",
              num_results=FIRST_STAGE_K,
              metadata=["docno"],
              controls={"bm25.k_1": "0.9", "bm25.b": "0.4"}
          )
          
          # TCT-ColBERT encoder
          print(f"[bm25-tct] loading TCT-ColBERT {MODEL_NAME} on CPU...")
          device = "cpu"
          model = SentenceTransformer(MODEL_NAME, device=device)
          
          # Process in mini-batches
          tmp_path = RUN_PATH.with_suffix(".part")
          if tmp_path.exists():
              tmp_path.unlink()
          
          total_time = time.time()
          with tmp_path.open("w", encoding="utf-8") as out_f:
              for batch_idx in range(0, len(queries), MINI_BATCH):
                  batch_q = queries[batch_idx:batch_idx + MINI_BATCH]
                  batch_df = pd.DataFrame(batch_q)
                  
                  # Stage 1: BM25
                  t0 = time.time()
                  bm25_results = bm25.transform(batch_df)
                  print(f"[bm25-tct] batch {batch_idx//MINI_BATCH + 1}: BM25 retrieved {len(bm25_results)} results in {time.time()-t0:.1f}s")
                  
                  if len(bm25_results) == 0:
                      continue
                  
                  # Fetch document texts
                  t0 = time.time()
                  tsv_file = TSV_PATH.open("r", encoding="utf-8", errors="ignore")
                  texts = []
                  for _, row in bm25_results.iterrows():
                      docid = row["docno"]
                      if docid not in corpus_offsets:
                          texts.append("")
                          continue
                      tsv_file.seek(corpus_offsets[docid])
                      line = tsv_file.readline()
                      parts = line.rstrip("\n").split("\t")
                      if len(parts) >= 3:
                          text_content = parts[1]
                          title = parts[2]
                          combined = (title + " " + text_content).strip()
                          texts.append(combined)
                      else:
                          texts.append("")
                  tsv_file.close()
                  bm25_results["text"] = texts
                  print(f"[bm25-tct] batch {batch_idx//MINI_BATCH + 1}: fetched texts in {time.time()-t0:.1f}s")
                  
                  # Stage 2: TCT-ColBERT bi-encoder reranking
                  t0 = time.time()
                  # Encode queries
                  query_texts = [row["query_orig"] for _, row in batch_df.iterrows()]
                  query_embs = model.encode(
                      query_texts,
                      batch_size=TCT_BATCH,
                      convert_to_numpy=True,
                      normalize_embeddings=True,
                      show_progress_bar=False,
                  ).astype("float32")
                  
                  # Encode documents in batches
                  doc_texts = bm25_results["text"].tolist()
                  doc_embs = model.encode(
                      doc_texts,
                      batch_size=TCT_BATCH,
                      convert_to_numpy=True,
                      normalize_embeddings=True,
                      show_progress_bar=False,
                  ).astype("float32")
                  
                  # Compute scores (dot product between query and doc embeddings)
                  scores = []
                  for idx, (_, row) in enumerate(bm25_results.iterrows()):
                      qid = row["qid"]
                      q_idx = batch_df[batch_df["qid"] == qid].index[0] - batch_df.index[0]
                      score = float(np.dot(query_embs[q_idx], doc_embs[idx]))
                      scores.append(score)
                  
                  bm25_results["tct_score"] = scores
                  bm25_results = bm25_results.sort_values(["qid", "tct_score"], ascending=[True, False])
                  print(f"[bm25-tct] batch {batch_idx//MINI_BATCH + 1}: reranked in {time.time()-t0:.1f}s")
                  
                  # Write top-k
                  for qid in batch_df["qid"]:
                      qid_results = bm25_results[bm25_results["qid"] == qid].head(TOP_K)
                      for rank, (_, row) in enumerate(qid_results.iterrows(), start=1):
                          out_f.write(f"{qid} Q0 {row['docno']} {rank} {row['tct_score']:.6f} BM25_TCT\n")
                  
                  print(f"[bm25-tct] batch {batch_idx//MINI_BATCH + 1} done ({batch_idx + len(batch_q)}/{len(queries)})")
          
          tmp_path.rename(RUN_PATH)
          print(f"[bm25-tct] total time: {time.time() - total_time:.1f}s")
          print(f"[bm25-tct] run written to {RUN_PATH}")
          PY
          echo "[bm25-tct] DONE"
        resources:
          requests:
            cpu: "24"
            memory: "48Gi"
          limits:
            cpu: "48"
            memory: "64Gi"
        volumeMounts:
        - name: datasets
          mountPath: /mnt/datasets
        - name: indexes
          mountPath: /mnt/index
      volumes:
      - name: datasets
        persistentVolumeClaim:
          claimName: shared-datasets
      - name: indexes
        persistentVolumeClaim:
          claimName: shared-indexes


