{"qid": "3012568508229512715", "question": "when does like cage season 2 come out", "gold_answers": ["June 22 , 2018"], "shots": {"0": {"k": 0, "answer": "2024", "latency_ms": 121.12998962402344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "June 22, 2018", "latency_ms": 155.92145919799805, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 632}, "2": {"k": 2, "answer": "June 21, 2018", "latency_ms": 475.46911239624023, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1267}, "3": {"k": 3, "answer": "June 21, 2018", "latency_ms": 462.76330947875977, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1891}, "4": {"k": 4, "answer": "June 21, 2018", "latency_ms": 210.6318473815918, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2497}, "5": {"k": 5, "answer": "June 21, 2018", "latency_ms": 358.4418296813965, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3106}, "6": {"k": 6, "answer": "June 21, 2018", "latency_ms": 525.3336429595947, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3761}}}
{"qid": "839669511198247322", "question": "types of skiing in the winter olympics 2018", "gold_answers": ["Super-G", "Slalom -- ( SC )", "Downhill", "Giant slalom", "Slalom"], "shots": {"0": {"k": 0, "answer": "Alpine, Cross-Country, Biathlon, Freestyle, Snowboard", "latency_ms": 669.9259281158447, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Freestyle skiing", "latency_ms": 119.61054801940918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 653}, "2": {"k": 2, "answer": "Freestyle skiing, Cross-country skiing", "latency_ms": 540.7695770263672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1487}, "3": {"k": 3, "answer": "Freestyle skiing, Cross-country skiing", "latency_ms": 227.67376899719238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2139}, "4": {"k": 4, "answer": "Freestyle, Alpine, Cross-country, Ski cross", "latency_ms": 296.4591979980469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2771}, "5": {"k": 5, "answer": "Freestyle skiing, Alpine skiing", "latency_ms": 216.72296524047852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3533}, "6": {"k": 6, "answer": "Freestyle skiing, Alpine skiing", "latency_ms": 188.04001808166504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4196}}}
{"qid": "-5881961149303861412", "question": "who played gino in a place to call home", "gold_answers": ["Aldo Mignone"], "shots": {"0": {"k": 0, "answer": "Derek Luke", "latency_ms": 100.25882720947266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Aldo Mignone", "latency_ms": 158.1428050994873, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 671}, "2": {"k": 2, "answer": "Aldo Mignone", "latency_ms": 228.06715965270996, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1367}, "3": {"k": 3, "answer": "Aldo Mignone", "latency_ms": 296.77796363830566, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1897}, "4": {"k": 4, "answer": "Aldo Mignone", "latency_ms": 248.7332820892334, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2511}, "5": {"k": 5, "answer": "Aldo Mignone", "latency_ms": 265.103816986084, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3064}, "6": {"k": 6, "answer": "Aldo Mignone", "latency_ms": 263.78631591796875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3684}}}
{"qid": "-9077495306870424234", "question": "who did the 49ers beat in the super bowls", "gold_answers": ["San Diego Chargers", "Miami Dolphins", "Denver Broncos", "Cincinnati Bengals"], "shots": {"0": {"k": 0, "answer": "Pittsburgh Steelers", "latency_ms": 111.52219772338867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Denver Broncos", "latency_ms": 67.85440444946289, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 661}, "2": {"k": 2, "answer": "Broncos, Bengals", "latency_ms": 94.91944313049316, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1279}, "3": {"k": 3, "answer": "Broncos, Bengals, Chargers", "latency_ms": 180.20296096801758, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1894}, "4": {"k": 4, "answer": "Denver Broncos, Cincinnati Bengals, San Diego Chargers", "latency_ms": 482.2824001312256, "em": 0.0, "f1": 0.6, "containment": 1.0, "context_chars": 2543}, "5": {"k": 5, "answer": "Denver Broncos, Cincinnati Bengals, San Diego Chargers, Miami Dolphins", "latency_ms": 344.21229362487793, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3162}, "6": {"k": 6, "answer": "Denver Broncos, Cincinnati Bengals, San Diego Chargers", "latency_ms": 432.55066871643066, "em": 0.0, "f1": 0.6, "containment": 1.0, "context_chars": 3790}}}
{"qid": "-5623229691925051533", "question": "when was the south asian association for regional co-operation (saarc) formed", "gold_answers": ["December 1985", "8 December 1985"], "shots": {"0": {"k": 0, "answer": "1985", "latency_ms": 147.34411239624023, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "8 December 1985", "latency_ms": 402.1589756011963, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 727}, "2": {"k": 2, "answer": "8 December 1985", "latency_ms": 314.9299621582031, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1444}, "3": {"k": 3, "answer": "8 December 1985", "latency_ms": 376.6751289367676, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2083}, "4": {"k": 4, "answer": "8 December 1985", "latency_ms": 190.5345916748047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2891}, "5": {"k": 5, "answer": "8 December 1985", "latency_ms": 680.9873580932617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3592}, "6": {"k": 6, "answer": "8 December 1985", "latency_ms": 331.2034606933594, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4298}}}
{"qid": "8707879945343660137", "question": "what hangs from the trees in the bayou", "gold_answers": ["Spanish moss"], "shots": {"0": {"k": 0, "answer": "cypress knees", "latency_ms": 167.9685115814209, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 203.30357551574707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 642}, "2": {"k": 2, "answer": "Spanish moss", "latency_ms": 181.63275718688965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1295}, "3": {"k": 3, "answer": "Spanish moss", "latency_ms": 106.31418228149414, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1947}, "4": {"k": 4, "answer": "Spanish moss", "latency_ms": 97.99575805664062, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2605}, "5": {"k": 5, "answer": "Spanish moss", "latency_ms": 318.9373016357422, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3206}, "6": {"k": 6, "answer": "Spanish moss", "latency_ms": 166.61405563354492, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3839}}}
{"qid": "-731256923033768782", "question": "new york heart association functional classification of angina", "gold_answers": ["II"], "shots": {"0": {"k": 0, "answer": "NYHA Class I-IV", "latency_ms": 317.25478172302246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "NYHA Functional Classification", "latency_ms": 246.66380882263184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 730}, "2": {"k": 2, "answer": "NYHA Functional Classification", "latency_ms": 160.59184074401855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1494}, "3": {"k": 3, "answer": "NYHA Functional Classification", "latency_ms": 164.5493507385254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2250}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 164.52836990356445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2897}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 232.4373722076416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3686}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 201.12037658691406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4395}}}
{"qid": "-5350413673278085389", "question": "who wrote the song would i lie to you", "gold_answers": ["Mike Leeson", "Peter Vale"], "shots": {"0": {"k": 0, "answer": "Dolly Parton", "latency_ms": 231.73952102661133, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 85.14618873596191, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 584}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 130.27429580688477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1158}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 151.8537998199463, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1829}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 337.8567695617676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2462}, "5": {"k": 5, "answer": "Eurythmics", "latency_ms": 290.88640213012695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3079}, "6": {"k": 6, "answer": "Kim Carnes", "latency_ms": 349.7440814971924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3723}}}
{"qid": "-6190173184813176768", "question": "number of films in lord of the rings", "gold_answers": ["three"], "shots": {"0": {"k": 0, "answer": "3", "latency_ms": 173.21276664733887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "3", "latency_ms": 87.2800350189209, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 604}, "2": {"k": 2, "answer": "3", "latency_ms": 102.70166397094727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1230}, "3": {"k": 3, "answer": "3", "latency_ms": 64.73350524902344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1860}, "4": {"k": 4, "answer": "3", "latency_ms": 89.67423439025879, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2507}, "5": {"k": 5, "answer": "3", "latency_ms": 84.12361145019531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3130}, "6": {"k": 6, "answer": "3", "latency_ms": 147.7503776550293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3814}}}
{"qid": "8239463385147663167", "question": "is jaffa cakes a biscuit or a cake", "gold_answers": ["cakes", "Cake", "biscuit - sized cakes"], "shots": {"0": {"k": 0, "answer": "biscuit", "latency_ms": 212.37516403198242, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "biscuit-sized cake", "latency_ms": 300.0216484069824, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 604}, "2": {"k": 2, "answer": "biscuit-sized cake", "latency_ms": 221.7557430267334, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1184}, "3": {"k": 3, "answer": "cake", "latency_ms": 104.65455055236816, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1764}, "4": {"k": 4, "answer": "a cake", "latency_ms": 88.03606033325195, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2345}, "5": {"k": 5, "answer": "a cake", "latency_ms": 116.89615249633789, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3040}, "6": {"k": 6, "answer": "a cake", "latency_ms": 151.92103385925293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3681}}}
{"qid": "756385103791298508", "question": "who does the voice of stewie family guy", "gold_answers": ["Seth MacFarlane"], "shots": {"0": {"k": 0, "answer": "Dan Castellaneta", "latency_ms": 79.59794998168945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Seth MacFarlane", "latency_ms": 280.00473976135254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 639}, "2": {"k": 2, "answer": "Seth MacFarlane", "latency_ms": 146.17633819580078, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1267}, "3": {"k": 3, "answer": "Seth MacFarlane", "latency_ms": 308.199405670166, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1881}, "4": {"k": 4, "answer": "Seth MacFarlane", "latency_ms": 280.4703712463379, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2554}, "5": {"k": 5, "answer": "Seth MacFarlane", "latency_ms": 212.13459968566895, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3178}, "6": {"k": 6, "answer": "Seth MacFarlane", "latency_ms": 219.3901538848877, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3812}}}
{"qid": "7355457342276623885", "question": "what drug does the mom use in requiem for a dream", "gold_answers": ["weight - loss amphetamines", "amphetamines"], "shots": {"0": {"k": 0, "answer": "cocaine", "latency_ms": 154.07109260559082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "amphetamines", "latency_ms": 136.2133026123047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 609}, "2": {"k": 2, "answer": "amphetamines", "latency_ms": 166.94188117980957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1289}, "3": {"k": 3, "answer": "amphetamines", "latency_ms": 224.5168685913086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1866}, "4": {"k": 4, "answer": "amphetamines", "latency_ms": 310.2414608001709, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2506}, "5": {"k": 5, "answer": "amphetamines", "latency_ms": 308.92252922058105, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3114}, "6": {"k": 6, "answer": "amphetamines", "latency_ms": 234.11846160888672, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3719}}}
{"qid": "-251770629620567718", "question": "how long was dark side of the moon number 1", "gold_answers": ["42 : 49", "a week"], "shots": {"0": {"k": 0, "answer": "19 minutes 38 seconds", "latency_ms": 255.1867961883545, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1 week", "latency_ms": 62.780141830444336, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 566}, "2": {"k": 2, "answer": "1 week", "latency_ms": 174.69501495361328, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1169}, "3": {"k": 3, "answer": "1 week", "latency_ms": 167.08803176879883, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1781}, "4": {"k": 4, "answer": "1 week", "latency_ms": 236.21797561645508, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2400}, "5": {"k": 5, "answer": "1 week", "latency_ms": 102.11420059204102, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2973}, "6": {"k": 6, "answer": "1 week", "latency_ms": 120.90110778808594, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3528}}}
{"qid": "3227997019205769806", "question": "where is the snooker world open currently held", "gold_answers": ["Hainan International Exhibition Center", "Haikou"], "shots": {"0": {"k": 0, "answer": "London, UK", "latency_ms": 218.3969020843506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 141.42084121704102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 343.6741828918457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1250}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 282.057523727417, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1898}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 160.86435317993164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2527}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 255.19800186157227, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3143}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 248.35491180419922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3752}}}
{"qid": "8499576329103738172", "question": "who is the actor that plays jt on the young and the restless", "gold_answers": ["Thaddeus Rowe Luckinbill"], "shots": {"0": {"k": 0, "answer": "Joshua \"JT\" Williams", "latency_ms": 329.11038398742676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 177.7358055114746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 199.52058792114258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1277}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 296.0231304168701, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1951}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 312.46352195739746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2623}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 134.87982749938965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3276}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 211.38906478881836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3906}}}
{"qid": "4230593664190705298", "question": "when was the first commercial cell phone released", "gold_answers": ["1983", "1973"], "shots": {"0": {"k": 0, "answer": "1973", "latency_ms": 92.1623706817627, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 243.8335418701172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "1983", "latency_ms": 125.78725814819336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1308}, "3": {"k": 3, "answer": "1983", "latency_ms": 108.02960395812988, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1922}, "4": {"k": 4, "answer": "1983", "latency_ms": 296.54979705810547, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2597}, "5": {"k": 5, "answer": "1983", "latency_ms": 189.63909149169922, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3254}, "6": {"k": 6, "answer": "1983", "latency_ms": 242.2776222229004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3893}}}
{"qid": "2241516377343726249", "question": "who sings the theme song for degrassi the next generation", "gold_answers": ["The Degrassi Junior Strings", "Jakalope"], "shots": {"0": {"k": 0, "answer": "Lori Michaels", "latency_ms": 90.90948104858398, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Alexz Johnson", "latency_ms": 239.02297019958496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 640}, "2": {"k": 2, "answer": "Lisa Dalbello", "latency_ms": 136.7056369781494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1291}, "3": {"k": 3, "answer": "Alexz Johnson, Jim McGrath, Jody Colero, Stephen Stohn, Lisa Dalbello, Damhnait Doyle", "latency_ms": 783.921480178833, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1938}, "4": {"k": 4, "answer": "Lisa Dalbello", "latency_ms": 348.726749420166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2598}, "5": {"k": 5, "answer": "Lisa Dalbello", "latency_ms": 241.26052856445312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3289}, "6": {"k": 6, "answer": "Lisa Dalbello", "latency_ms": 332.3805332183838, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3936}}}
{"qid": "8179100082271836759", "question": "when did the first ice age come out", "gold_answers": ["March 15 , 2002", "2002"], "shots": {"0": {"k": 0, "answer": "2.6 million years ago", "latency_ms": 217.33760833740234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2002", "latency_ms": 328.9065361022949, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 629}, "2": {"k": 2, "answer": "March 15, 2002", "latency_ms": 178.5564422607422, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1257}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 200.12617111206055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1906}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 242.9525852203369, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2557}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 168.75815391540527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3174}, "6": {"k": 6, "answer": "March 15, 2002", "latency_ms": 565.8960342407227, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3847}}}
{"qid": "-1946900884888632776", "question": "what is a e8 in the marine corps", "gold_answers": ["Master sergeant", "First sergeant"], "shots": {"0": {"k": 0, "answer": "E-8 in the Marine Corps is a rank of Colonel.", "latency_ms": 762.2838020324707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "master sergeant", "latency_ms": 239.87674713134766, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 718}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 151.42512321472168, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 131.3610076904297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1858}, "4": {"k": 4, "answer": "master sergeant", "latency_ms": 187.96825408935547, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2476}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 387.6762390136719, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3052}, "6": {"k": 6, "answer": "E8 is the pay grade equivalent to First Sergeant in the United States Marine Corps.", "latency_ms": 632.6413154602051, "em": 0.0, "f1": 0.2666666666666667, "containment": 1.0, "context_chars": 3721}}}
{"qid": "8538836642175821278", "question": "who is given credit for the gnu initiative", "gold_answers": ["Richard Stallman"], "shots": {"0": {"k": 0, "answer": "Richard Stallman", "latency_ms": 149.23667907714844, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Richard Stallman", "latency_ms": 347.3501205444336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 697}, "2": {"k": 2, "answer": "Richard Stallman", "latency_ms": 429.1105270385742, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1378}, "3": {"k": 3, "answer": "Richard Stallman", "latency_ms": 178.82752418518066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2056}, "4": {"k": 4, "answer": "Richard Stallman", "latency_ms": 112.70570755004883, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2701}, "5": {"k": 5, "answer": "Richard Stallman", "latency_ms": 205.98602294921875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3340}, "6": {"k": 6, "answer": "Richard Stallman", "latency_ms": 126.24597549438477, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3970}}}
{"qid": "-170392909557030937", "question": "who won the 10m air pistol gold medal at commonwealth shooting championship in brisbane australia", "gold_answers": ["Shahzar Rizvi"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 128.80969047546387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 288.8317108154297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 656}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 102.24437713623047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1239}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 109.44080352783203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1839}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 180.2806854248047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2476}, "5": {"k": 5, "answer": "Heena Sidhu", "latency_ms": 226.4258861541748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3058}, "6": {"k": 6, "answer": "Heena Sidhu", "latency_ms": 272.5048065185547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3706}}}
{"qid": "-7137073055537137289", "question": "who is the lead singer of collective soul", "gold_answers": ["Ed Roland"], "shots": {"0": {"k": 0, "answer": "Chris Layton", "latency_ms": 172.0132827758789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ed Roland", "latency_ms": 125.65326690673828, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 693}, "2": {"k": 2, "answer": "Ed Roland", "latency_ms": 137.97569274902344, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1321}, "3": {"k": 3, "answer": "Ed Roland", "latency_ms": 249.12023544311523, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1906}, "4": {"k": 4, "answer": "Ed Roland", "latency_ms": 124.21154975891113, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2582}, "5": {"k": 5, "answer": "Ed Roland", "latency_ms": 281.6348075866699, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3242}, "6": {"k": 6, "answer": "Ed Roland", "latency_ms": 334.26952362060547, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3880}}}
{"qid": "7198381700174721586", "question": "who does dwyane wade play for right now", "gold_answers": ["the Miami Heat", "Miami Heat"], "shots": {"0": {"k": 0, "answer": "Miami Heat", "latency_ms": 79.49304580688477, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Miami Heat", "latency_ms": 105.0717830657959, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 631}, "2": {"k": 2, "answer": "Miami Heat", "latency_ms": 204.4203281402588, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1253}, "3": {"k": 3, "answer": "Miami Heat", "latency_ms": 245.53847312927246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1849}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 216.7503833770752, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2452}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 212.2664451599121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3059}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 179.6267032623291, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3656}}}
{"qid": "-4036207256798544363", "question": "what is the value of the currency in paraguay", "gold_answers": ["126 PYG to 1 USD"], "shots": {"0": {"k": 0, "answer": "Guarani", "latency_ms": 65.67144393920898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "guarani (PYG)", "latency_ms": 223.7250804901123, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 656}, "2": {"k": 2, "answer": "PYG", "latency_ms": 72.18170166015625, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 1293}, "3": {"k": 3, "answer": "PYG", "latency_ms": 85.95681190490723, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 1862}, "4": {"k": 4, "answer": "guaraní (PYG)", "latency_ms": 448.9710330963135, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2431}, "5": {"k": 5, "answer": "guaraní (PYG)", "latency_ms": 174.10016059875488, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3069}, "6": {"k": 6, "answer": "PYG", "latency_ms": 141.9544219970703, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 3701}}}
{"qid": "222526478564019881", "question": "a drug that might be used in surgery for its amnesic properties is", "gold_answers": ["benzodiazepines", "Benzodiazepines"], "shots": {"0": {"k": 0, "answer": "ketamine", "latency_ms": 147.76992797851562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "benzodiazepines", "latency_ms": 202.18443870544434, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 678}, "2": {"k": 2, "answer": "midazolam", "latency_ms": 107.5739860534668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1362}, "3": {"k": 3, "answer": "benzodiazepines", "latency_ms": 265.2630805969238, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2076}, "4": {"k": 4, "answer": "benzodiazepines", "latency_ms": 135.2403163909912, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2758}, "5": {"k": 5, "answer": "benzodiazepines", "latency_ms": 356.27174377441406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3473}, "6": {"k": 6, "answer": "benzodiazepines", "latency_ms": 290.2092933654785, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4153}}}
{"qid": "-9216522514171777028", "question": "a single period of precession of earth's axis is completed in about", "gold_answers": ["26,000 years", "approximately 26,000 years"], "shots": {"0": {"k": 0, "answer": "26,000 years", "latency_ms": 121.19889259338379, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "25,772 years", "latency_ms": 340.9862518310547, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 664}, "2": {"k": 2, "answer": "25,772 years", "latency_ms": 305.10592460632324, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1302}, "3": {"k": 3, "answer": "26,000 years", "latency_ms": 198.87852668762207, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1930}, "4": {"k": 4, "answer": "25,772 years", "latency_ms": 373.7037181854248, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2511}, "5": {"k": 5, "answer": "25,772 years", "latency_ms": 400.30598640441895, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3183}, "6": {"k": 6, "answer": "26,000 years", "latency_ms": 263.35597038269043, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3864}}}
{"qid": "8838750614698973059", "question": "who did johnny manziel play college football for", "gold_answers": ["Texas A&M University", "Texas A&M Aggies", "Texas A&M"], "shots": {"0": {"k": 0, "answer": "Texas A&M", "latency_ms": 178.6201000213623, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Texas A&M Aggies", "latency_ms": 188.65132331848145, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 645}, "2": {"k": 2, "answer": "Texas A&M", "latency_ms": 183.20703506469727, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1275}, "3": {"k": 3, "answer": "Texas A&M", "latency_ms": 184.7093105316162, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1917}, "4": {"k": 4, "answer": "Texas A&M", "latency_ms": 156.52966499328613, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2541}, "5": {"k": 5, "answer": "Texas A&M", "latency_ms": 223.8776683807373, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3161}, "6": {"k": 6, "answer": "Texas A&M", "latency_ms": 251.60884857177734, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3878}}}
{"qid": "7579233308152571691", "question": "natural resources can be divided into what two categories", "gold_answers": ["Biotic", "Abiotic"], "shots": {"0": {"k": 0, "answer": "Renewable and non-renewable", "latency_ms": 422.6510524749756, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "renewable and non-renewable", "latency_ms": 129.7595500946045, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 697}, "2": {"k": 2, "answer": "renewable and non-renewable", "latency_ms": 136.80624961853027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1428}, "3": {"k": 3, "answer": "renewable and non-renewable", "latency_ms": 297.4393367767334, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2178}, "4": {"k": 4, "answer": "renewable and non-renewable", "latency_ms": 384.08637046813965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2910}, "5": {"k": 5, "answer": "renewable and non-renewable", "latency_ms": 437.2668266296387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3588}, "6": {"k": 6, "answer": "renewable and non-renewable", "latency_ms": 567.1234130859375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4288}}}
{"qid": "1106103350744797710", "question": "what is the white stork a symbol for in western culture", "gold_answers": ["bringing babies to new parents"], "shots": {"0": {"k": 0, "answer": "Good luck and prosperity", "latency_ms": 336.06410026550293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "parental devotion", "latency_ms": 96.18735313415527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 640}, "2": {"k": 2, "answer": "filial piety", "latency_ms": 153.46503257751465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1249}, "3": {"k": 3, "answer": "filial piety and mercy", "latency_ms": 427.01148986816406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1855}, "4": {"k": 4, "answer": "purity and filial piety", "latency_ms": 255.24044036865234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2504}, "5": {"k": 5, "answer": "filial piety, purity, mercy, parental devotion", "latency_ms": 535.243034362793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3139}, "6": {"k": 6, "answer": "piety, filial devotion, purity, mercy, good fortune", "latency_ms": 625.140905380249, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3780}}}
{"qid": "-8361853086037854880", "question": "india south africa test series 2018 highest runs", "gold_answers": ["AB de Villiers", "286"], "shots": {"0": {"k": 0, "answer": "Rohit Sharma", "latency_ms": 183.9280128479004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 107.92064666748047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 642}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 171.33426666259766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1238}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 316.3735866546631, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1887}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 218.86944770812988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2473}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 290.1957035064697, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3092}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 353.2421588897705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3703}}}
{"qid": "-5202207990213031495", "question": "the oligodynamic effect is a phenomenon that describes", "gold_answers": ["a biocidal effect of metals"], "shots": {"0": {"k": 0, "answer": "killing of microorganisms by low concentrations of metals", "latency_ms": 231.86516761779785, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "biocidal effect of metals in low concentrations", "latency_ms": 451.84326171875, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 715}, "2": {"k": 2, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 390.89417457580566, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 1412}, "3": {"k": 3, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 1079.230546951294, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 2103}, "4": {"k": 4, "answer": "the biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 838.381290435791, "em": 0.0, "f1": 0.4705882352941176, "containment": 0.0, "context_chars": 2841}, "5": {"k": 5, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 682.044267654419, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 3568}, "6": {"k": 6, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 877.647876739502, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 4258}}}
{"qid": "6391370451004919214", "question": "who was last person to be executed in us", "gold_answers": ["Ruben Cardenas Ramirez"], "shots": {"0": {"k": 0, "answer": "2006", "latency_ms": 102.85711288452148, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "James Coburn", "latency_ms": 128.2639503479004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 638}, "2": {"k": 2, "answer": "James Coburn", "latency_ms": 167.44637489318848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1294}, "3": {"k": 3, "answer": "Harold Van Venison", "latency_ms": 183.34245681762695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1922}, "4": {"k": 4, "answer": "Billy Bailey", "latency_ms": 85.71219444274902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2516}, "5": {"k": 5, "answer": "Ronald Wolfe", "latency_ms": 139.99342918395996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3177}, "6": {"k": 6, "answer": "James French", "latency_ms": 204.30493354797363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3780}}}
{"qid": "-8069963735453146166", "question": "when do you declare honors in contract bridge", "gold_answers": ["any time after the auction"], "shots": {"0": {"k": 0, "answer": "After the opening lead and before the first round is played.", "latency_ms": 472.47815132141113, "em": 0.0, "f1": 0.15384615384615383, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "at the conclusion of play", "latency_ms": 212.03994750976562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 550}, "2": {"k": 2, "answer": "at the conclusion of play", "latency_ms": 229.78973388671875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1122}, "3": {"k": 3, "answer": "at the conclusion of play", "latency_ms": 111.33217811584473, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1728}, "4": {"k": 4, "answer": "at the conclusion of play", "latency_ms": 288.76566886901855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2310}, "5": {"k": 5, "answer": "at the conclusion of play", "latency_ms": 294.10290718078613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2926}, "6": {"k": 6, "answer": "at the conclusion of play", "latency_ms": 227.64229774475098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3511}}}
{"qid": "-2154502691082554319", "question": "which country is the last member of saarc", "gold_answers": ["Afghanistan"], "shots": {"0": {"k": 0, "answer": "Sri Lanka", "latency_ms": 197.1583366394043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 189.61858749389648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 694}, "2": {"k": 2, "answer": "Sri Lanka", "latency_ms": 177.86073684692383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1395}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 162.75811195373535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2124}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 226.2895107269287, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2785}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 131.73818588256836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3509}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 172.1203327178955, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4243}}}
{"qid": "-8498955431733322253", "question": "describe the three phases of the normal blood clotting process", "gold_answers": ["blood coagulation", "vasoconstriction ,"], "shots": {"0": {"k": 0, "answer": "Vascular spasm, platelet plug formation, coagulation", "latency_ms": 263.7484073638916, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 87.75115013122559, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "Primary hemostasis, secondary hemostasis, final common pathway", "latency_ms": 580.364465713501, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1361}, "3": {"k": 3, "answer": "hemostasis, inflammation, tissue growth", "latency_ms": 232.33723640441895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2082}, "4": {"k": 4, "answer": "hemostasis, inflammation, tissue growth", "latency_ms": 262.5889778137207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2807}, "5": {"k": 5, "answer": "hemostasis, fibrin formation, fibrinolysis", "latency_ms": 501.4965534210205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3521}, "6": {"k": 6, "answer": "hemostasis, inflammation, tissue growth (proliferation), and tissue remodeling (maturation)", "latency_ms": 638.8778686523438, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4173}}}
{"qid": "7392498509104714323", "question": "who played sonny in lemony snicket's a series of unfortunate events", "gold_answers": ["Presley Smith"], "shots": {"0": {"k": 0, "answer": "Jesse McCartney", "latency_ms": 68.10593605041504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 91.66264533996582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 690}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 174.79634284973145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1374}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 114.19177055358887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2068}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 459.200382232666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2768}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 320.3125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3398}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 345.98374366760254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4044}}}
{"qid": "2911493370352980057", "question": "when was united nations convention on the rights of the child created", "gold_answers": ["20 November 1989"], "shots": {"0": {"k": 0, "answer": "1989", "latency_ms": 96.95267677307129, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "20 November 1989", "latency_ms": 228.95455360412598, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 664}, "2": {"k": 2, "answer": "20 November 1989", "latency_ms": 171.75984382629395, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1295}, "3": {"k": 3, "answer": "20 November 1989", "latency_ms": 271.6660499572754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1890}, "4": {"k": 4, "answer": "20 November 1989", "latency_ms": 192.2781467437744, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2551}, "5": {"k": 5, "answer": "20 November 1989", "latency_ms": 352.0379066467285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3220}, "6": {"k": 6, "answer": "20 November 1989", "latency_ms": 475.88348388671875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3861}}}
{"qid": "2495228512593253768", "question": "when did we decide to leave the eu", "gold_answers": ["23 June 2016"], "shots": {"0": {"k": 0, "answer": "2016", "latency_ms": 223.35124015808105, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "23 June 2016", "latency_ms": 332.0803642272949, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 672}, "2": {"k": 2, "answer": "23 June 2016", "latency_ms": 385.3719234466553, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1348}, "3": {"k": 3, "answer": "23 June 2016", "latency_ms": 351.0861396789551, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2020}, "4": {"k": 4, "answer": "23 June 2016", "latency_ms": 257.490873336792, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2685}, "5": {"k": 5, "answer": "23 June 2016", "latency_ms": 235.54420471191406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3299}, "6": {"k": 6, "answer": "23 June 2016", "latency_ms": 499.6621608734131, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3971}}}
{"qid": "95636137517606056", "question": "tallest building in the world of all time", "gold_answers": ["Burj Khalifa"], "shots": {"0": {"k": 0, "answer": "Burj Khalifa", "latency_ms": 460.7985019683838, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Burj Khalifa", "latency_ms": 83.19997787475586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 679}, "2": {"k": 2, "answer": "Burj Khalifa", "latency_ms": 197.67189025878906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1360}, "3": {"k": 3, "answer": "Burj Khalifa", "latency_ms": 126.1284351348877, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2002}, "4": {"k": 4, "answer": "Burj Khalifa", "latency_ms": 263.0481719970703, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2612}, "5": {"k": 5, "answer": "Burj Khalifa", "latency_ms": 277.07576751708984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3253}, "6": {"k": 6, "answer": "Burj Khalifa", "latency_ms": 416.5995121002197, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3944}}}
{"qid": "-4020097033891103749", "question": "who plays chummy's mother in call the midwife", "gold_answers": ["Cheryl Campbell"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 250.23198127746582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Jenny Agutter", "latency_ms": 199.19657707214355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 653}, "2": {"k": 2, "answer": "Linda Bassett", "latency_ms": 199.27501678466797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1308}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 255.46956062316895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 345.88050842285156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2480}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 150.5265235900879, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3101}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 160.37273406982422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3767}}}
{"qid": "-7690316790133138931", "question": "who created separation of powers and checks and balances", "gold_answers": ["Montesquieu in the Enlightenment"], "shots": {"0": {"k": 0, "answer": "Baron de Montesquieu", "latency_ms": 318.5746669769287, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Montesquieu", "latency_ms": 115.40389060974121, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 647}, "2": {"k": 2, "answer": "Montesquieu", "latency_ms": 274.3666172027588, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1308}, "3": {"k": 3, "answer": "Montesquieu", "latency_ms": 205.5201530456543, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2009}, "4": {"k": 4, "answer": "Montesquieu", "latency_ms": 224.47681427001953, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2690}, "5": {"k": 5, "answer": "Montesquieu", "latency_ms": 166.94045066833496, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3328}, "6": {"k": 6, "answer": "Montesquieu", "latency_ms": 198.12870025634766, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3955}}}
{"qid": "914815550191007566", "question": "who was the nfl first draft pick 2017", "gold_answers": ["Myles Garrett"], "shots": {"0": {"k": 0, "answer": "Kyler Murray", "latency_ms": 154.16598320007324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Sam Bradford", "latency_ms": 347.0640182495117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "Sam Bradford", "latency_ms": 64.42904472351074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1295}, "3": {"k": 3, "answer": "Myles Garrett", "latency_ms": 283.170223236084, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1924}, "4": {"k": 4, "answer": "Myles Garrett", "latency_ms": 199.56612586975098, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2507}, "5": {"k": 5, "answer": "Myles Garrett", "latency_ms": 219.08831596374512, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3083}, "6": {"k": 6, "answer": "Myles Garrett", "latency_ms": 167.3259735107422, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3743}}}
{"qid": "972568347460262100", "question": "when was nepal declared a secular state in bs", "gold_answers": ["January 15 , 2007"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 81.16388320922852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2008 BS", "latency_ms": 255.35845756530762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 643}, "2": {"k": 2, "answer": "2008", "latency_ms": 251.19543075561523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1284}, "3": {"k": 3, "answer": "2006", "latency_ms": 287.4138355255127, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1948}, "4": {"k": 4, "answer": "2006", "latency_ms": 171.36669158935547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2646}, "5": {"k": 5, "answer": "2006", "latency_ms": 488.4510040283203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3358}, "6": {"k": 6, "answer": "2006", "latency_ms": 162.72568702697754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4016}}}
{"qid": "-4845827823184041115", "question": "who took control of jamestown and made new rules that made everyone work", "gold_answers": ["John Smith"], "shots": {"0": {"k": 0, "answer": "Captain John Smith", "latency_ms": 67.67487525939941, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "John Smith", "latency_ms": 141.6769027709961, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 648}, "2": {"k": 2, "answer": "John Smith", "latency_ms": 149.90878105163574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1281}, "3": {"k": 3, "answer": "John Smith", "latency_ms": 216.31813049316406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1951}, "4": {"k": 4, "answer": "John Smith", "latency_ms": 94.20657157897949, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2627}, "5": {"k": 5, "answer": "John Smith", "latency_ms": 239.01915550231934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3237}, "6": {"k": 6, "answer": "John Smith", "latency_ms": 130.16510009765625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3921}}}
{"qid": "7592863385884727299", "question": "the site of secretion of h+ and cl-", "gold_answers": ["Parietal cells"], "shots": {"0": {"k": 0, "answer": "parietal cells", "latency_ms": 324.21112060546875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "parietal cells", "latency_ms": 354.5644283294678, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 625}, "2": {"k": 2, "answer": "parietal cells", "latency_ms": 422.4517345428467, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1291}, "3": {"k": 3, "answer": "parietal cells", "latency_ms": 104.89678382873535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1878}, "4": {"k": 4, "answer": "parietal cells", "latency_ms": 181.94913864135742, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2588}, "5": {"k": 5, "answer": "parietal cells", "latency_ms": 199.8906135559082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3257}, "6": {"k": 6, "answer": "parietal cells", "latency_ms": 175.23908615112305, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3912}}}
{"qid": "-217684522847197793", "question": "when does body temperature tend to be lowest", "gold_answers": ["11 p.m. to 3 a.m."], "shots": {"0": {"k": 0, "answer": "between 2 and 4 a.m.", "latency_ms": 279.080867767334, "em": 0.0, "f1": 0.20000000000000004, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "two hours before the person normally wakes up", "latency_ms": 172.32084274291992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 709}, "2": {"k": 2, "answer": "4 a.m.", "latency_ms": 209.55801010131836, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1372}, "3": {"k": 3, "answer": "two hours before waking up or around 4 a.m.", "latency_ms": 609.562873840332, "em": 0.0, "f1": 0.14285714285714285, "containment": 0.0, "context_chars": 2012}, "4": {"k": 4, "answer": "two hours before waking up or around 4 a.m.", "latency_ms": 697.005033493042, "em": 0.0, "f1": 0.14285714285714285, "containment": 0.0, "context_chars": 2676}, "5": {"k": 5, "answer": "two hours before waking up or around 4 a.m.", "latency_ms": 419.1768169403076, "em": 0.0, "f1": 0.14285714285714285, "containment": 0.0, "context_chars": 3360}, "6": {"k": 6, "answer": "around 4 a.m.", "latency_ms": 443.00270080566406, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 4081}}}
{"qid": "3926678456788602613", "question": "what us state forms the western boundary of montana", "gold_answers": ["Idaho"], "shots": {"0": {"k": 0, "answer": "Idaho", "latency_ms": 84.60640907287598, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Nebraska", "latency_ms": 79.96511459350586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 613}, "2": {"k": 2, "answer": "Idaho", "latency_ms": 144.55533027648926, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1306}, "3": {"k": 3, "answer": "Idaho", "latency_ms": 125.03790855407715, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1943}, "4": {"k": 4, "answer": "Idaho", "latency_ms": 190.31023979187012, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2603}, "5": {"k": 5, "answer": "Idaho", "latency_ms": 208.74810218811035, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3262}, "6": {"k": 6, "answer": "Idaho", "latency_ms": 109.97939109802246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3933}}}
{"qid": "-444951262186580688", "question": "what is the pirates of the caribbean in order", "gold_answers": ["At World 's End", "Dead Men Tell No Tales", "Dead Man 's Chest", "On Stranger Tides"], "shots": {"0": {"k": 0, "answer": "1. The Curse of the Black Pearl  \n2. Dead Man's Chest  \n3. At World's End  \n4. On Stranger Tides  \n5. Pirates of the Caribbean: Salazar's Revenge  \n6. Dead Men Tell No Tales  \n7. The Legend of the Lost Gold  \n8. The Curse of the Black Pearl (reboot)  \n9. The Curse of the Black Pearl (sequel)  \n10. The Curse of the Black Pearl (final)", "latency_ms": 2346.039295196533, "em": 0.0, "f1": 0.17543859649122806, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1. The Curse of the Black Pearl (2003)  \n2. At World's End (2007)  \n3. Dead Man's Chest (2006)  \n4. On Stranger Tides (2011)  \n5. Dead Men Tell No Tales (2017)  \n6. (upcoming)", "latency_ms": 1934.6153736114502, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 667}, "2": {"k": 2, "answer": "1. The Curse of the Black Pearl (2003)  \n2. Dead Man's Chest (2006)  \n3. At World's End (2007)  \n4. On Stranger Tides (2011)  \n5. Dead Men Tell No Tales (2017)  \n6. (upcoming)", "latency_ms": 1830.4753303527832, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 1309}, "3": {"k": 3, "answer": "1. The Curse of the Black Pearl (2003)  \n2. Dead Man's Chest (2006)  \n3. At World's End (2007)  \n4. On Stranger Tides (2011)  \n5. Dead Men Tell No Tales (2017)  \n6. (in development)", "latency_ms": 3006.9708824157715, "em": 0.0, "f1": 0.27777777777777773, "containment": 1.0, "context_chars": 1969}, "4": {"k": 4, "answer": "1. The Curse of the Black Pearl  \n2. Dead Man's Chest  \n3. At World's End  \n4. On Stranger Tides  \n5. Dead Men Tell No Tales", "latency_ms": 1305.098533630371, "em": 0.0, "f1": 0.3571428571428571, "containment": 1.0, "context_chars": 2645}, "5": {"k": 5, "answer": "1. The Curse of the Black Pearl (2003)  \n2. Pirates of the Caribbean: Dead Man's Chest (2006)  \n3. Pirates of the Caribbean: At World's End (2007)  \n4. Pirates of the Caribbean: On Stranger Tides (2011)  \n5. Pirates of the Caribbean: Dead Men Tell No Tales (2017)", "latency_ms": 2267.4663066864014, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 3306}, "6": {"k": 6, "answer": "1. The Curse of the Black Pearl (2003)  \n2. Dead Man's Chest (2006)  \n3. At World's End (2007)  \n4. On Stranger Tides (2011)  \n5. Dead Men Tell No Tales (2017)  \n6. (in development, 2018)", "latency_ms": 2617.2282695770264, "em": 0.0, "f1": 0.2702702702702703, "containment": 1.0, "context_chars": 3991}}}
{"qid": "-822810697742332337", "question": "if a piece of music is perceived to have changed key then we say the piece has", "gold_answers": ["transposed", "transposition"], "shots": {"0": {"k": 0, "answer": "modulated", "latency_ms": 123.9476203918457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "progressive tonality", "latency_ms": 352.2050380706787, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 691}, "2": {"k": 2, "answer": "modulated", "latency_ms": 307.59668350219727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1303}, "3": {"k": 3, "answer": "modulated", "latency_ms": 157.5784683227539, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1945}, "4": {"k": 4, "answer": "modulated", "latency_ms": 169.81172561645508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2556}, "5": {"k": 5, "answer": "modulated", "latency_ms": 161.65757179260254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3201}, "6": {"k": 6, "answer": "modulated", "latency_ms": 209.733247756958, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3846}}}
{"qid": "-4663296615598634791", "question": "who was allowed to vote in the roman republic", "gold_answers": ["unknown", "citizens"], "shots": {"0": {"k": 0, "answer": "Roman citizens of certain property thresholds", "latency_ms": 300.609827041626, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Full Roman citizens (excluding women, slaves, and originally those living outside of Rome)", "latency_ms": 572.0353126525879, "em": 0.0, "f1": 0.14285714285714288, "containment": 1.0, "context_chars": 637}, "2": {"k": 2, "answer": "Full Roman citizens (excluding women, slaves, and originally those living outside of Rome)", "latency_ms": 301.7542362213135, "em": 0.0, "f1": 0.14285714285714288, "containment": 1.0, "context_chars": 1296}, "3": {"k": 3, "answer": "Full Roman citizens", "latency_ms": 209.74349975585938, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1941}, "4": {"k": 4, "answer": "Men who were citizens and owned a certain amount of real property", "latency_ms": 753.8144588470459, "em": 0.0, "f1": 0.16666666666666669, "containment": 1.0, "context_chars": 2598}, "5": {"k": 5, "answer": "adult male citizens who owned a certain amount of real property", "latency_ms": 308.4099292755127, "em": 0.0, "f1": 0.18181818181818182, "containment": 1.0, "context_chars": 3278}, "6": {"k": 6, "answer": "adult male citizens who owned a certain amount of real property", "latency_ms": 449.80335235595703, "em": 0.0, "f1": 0.18181818181818182, "containment": 1.0, "context_chars": 3977}}}
{"qid": "3695813920971622504", "question": "when does the next warrior book come out", "gold_answers": ["April 10 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 278.92422676086426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 196.84910774230957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 635}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 159.66343879699707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1278}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 202.15535163879395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1925}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 134.1855525970459, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2611}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 271.82555198669434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3253}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 346.65894508361816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3894}}}
{"qid": "8533079479108336590", "question": "blood alcohol concentration means the parts of alcohol in the blood in relation to what", "gold_answers": ["volume of blood", "ethanol"], "shots": {"0": {"k": 0, "answer": "blood volume", "latency_ms": 61.470746994018555, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "blood volume", "latency_ms": 205.43432235717773, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "volume of blood", "latency_ms": 81.66241645812988, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1302}, "3": {"k": 3, "answer": "volume of blood", "latency_ms": 132.5399875640869, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1899}, "4": {"k": 4, "answer": "volume of blood", "latency_ms": 144.90532875061035, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2563}, "5": {"k": 5, "answer": "volume of blood", "latency_ms": 115.81707000732422, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3260}, "6": {"k": 6, "answer": "volume of blood", "latency_ms": 145.97225189208984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3981}}}
{"qid": "-1531676396763282931", "question": "which greek god flew too close to the sun", "gold_answers": ["Icarus"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 297.3806858062744, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Icarus", "latency_ms": 349.6565818786621, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 587}, "2": {"k": 2, "answer": "Icarus", "latency_ms": 232.52248764038086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1173}, "3": {"k": 3, "answer": "Icarus", "latency_ms": 109.53879356384277, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1839}, "4": {"k": 4, "answer": "Icarus", "latency_ms": 140.2878761291504, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2411}, "5": {"k": 5, "answer": "Phaëton", "latency_ms": 136.99913024902344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3008}, "6": {"k": 6, "answer": "Phaëton", "latency_ms": 395.6465721130371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3645}}}
{"qid": "-3682350378604306871", "question": "when did the song rapper's delight come out", "gold_answers": ["September 16 , 1979", "1979"], "shots": {"0": {"k": 0, "answer": "1979", "latency_ms": 141.88432693481445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 16, 1979", "latency_ms": 335.2799415588379, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 615}, "2": {"k": 2, "answer": "September 16, 1979", "latency_ms": 185.61220169067383, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1223}, "3": {"k": 3, "answer": "September 16, 1979", "latency_ms": 395.77293395996094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1782}, "4": {"k": 4, "answer": "September 16, 1979", "latency_ms": 317.3649311065674, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2363}, "5": {"k": 5, "answer": "September 16, 1979", "latency_ms": 430.5768013000488, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2958}, "6": {"k": 6, "answer": "September 16, 1979", "latency_ms": 515.1412487030029, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3587}}}
{"qid": "-3646291394492263644", "question": "when does the stock market open in hong kong", "gold_answers": ["09 : 15"], "shots": {"0": {"k": 0, "answer": "9:30 AM Hong Kong Time", "latency_ms": 177.40535736083984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 126.24263763427734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 656}, "2": {"k": 2, "answer": "2 April 1986", "latency_ms": 267.2867774963379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1296}, "3": {"k": 3, "answer": "2 April 1986", "latency_ms": 258.8210105895996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1916}, "4": {"k": 4, "answer": "2 April 1986", "latency_ms": 372.4215030670166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2588}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 217.3764705657959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3207}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 217.6649570465088, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3825}}}
{"qid": "9187994979510737197", "question": "who's the model on let's make a deal", "gold_answers": ["Tiffany Adams Coyne"], "shots": {"0": {"k": 0, "answer": "Monty Hall", "latency_ms": 154.1593074798584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Tiffany Coyne", "latency_ms": 177.490234375, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 584}, "2": {"k": 2, "answer": "Tiffany Coyne", "latency_ms": 123.44717979431152, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1160}, "3": {"k": 3, "answer": "Tiffany Coyne", "latency_ms": 148.44155311584473, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1744}, "4": {"k": 4, "answer": "Tiffany Coyne", "latency_ms": 344.39611434936523, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2372}, "5": {"k": 5, "answer": "Tiffany Coyne", "latency_ms": 261.3189220428467, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2986}, "6": {"k": 6, "answer": "Tiffany Coyne", "latency_ms": 414.17741775512695, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3619}}}
{"qid": "5078709962400196312", "question": "what mlb teams did deion sanders play for", "gold_answers": ["San Francisco Giants", "Cincinnati Reds", "Atlanta Braves", "New York Yankees"], "shots": {"0": {"k": 0, "answer": "Atlanta Braves, New York Mets", "latency_ms": 226.43685340881348, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 301.0413646697998, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 639}, "2": {"k": 2, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 203.44805717468262, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1250}, "3": {"k": 3, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 359.84182357788086, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1853}, "4": {"k": 4, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 346.99130058288574, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 2464}, "5": {"k": 5, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 524.2347717285156, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3099}, "6": {"k": 6, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 332.26704597473145, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3813}}}
{"qid": "-4910305402988079742", "question": "how much money did the film titanic make", "gold_answers": ["$2.18 billion", "$2.187 billion"], "shots": {"0": {"k": 0, "answer": "$2.2 billion", "latency_ms": 290.84205627441406, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "over $1.84 billion", "latency_ms": 181.00810050964355, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 650}, "2": {"k": 2, "answer": "over $1.84 billion worldwide initially; over $2 billion worldwide during its 3D re-release", "latency_ms": 453.2756805419922, "em": 0.0, "f1": 0.13333333333333336, "containment": 0.0, "context_chars": 1312}, "3": {"k": 3, "answer": "over $1.84 billion initially; $343.4 million from 3D re-release worldwide", "latency_ms": 816.5314197540283, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 1938}, "4": {"k": 4, "answer": "over $1.84 billion initially; more than $2 billion in total with 3D re-release", "latency_ms": 770.9729671478271, "em": 0.0, "f1": 0.13333333333333336, "containment": 0.0, "context_chars": 2565}, "5": {"k": 5, "answer": "over $1.84 billion initially; more than $2 billion in total with 3D re-release", "latency_ms": 931.8854808807373, "em": 0.0, "f1": 0.13333333333333336, "containment": 0.0, "context_chars": 3247}, "6": {"k": 6, "answer": "over $1.84 billion worldwide initially; more than $2 billion worldwide during its 3D re-release", "latency_ms": 993.2138919830322, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 3890}}}
{"qid": "1636956235349274030", "question": "when does it's a wonderful life comes on", "gold_answers": ["Christmas season"], "shots": {"0": {"k": 0, "answer": "December 25th", "latency_ms": 235.8853816986084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 224.00188446044922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 384.07230377197266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1304}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 115.18073081970215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1899}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 183.60257148742676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2519}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 292.8953170776367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3152}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 347.2609519958496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3795}}}
{"qid": "5608421991404219229", "question": "where does the last name hansen come from", "gold_answers": ["the Faroe Islands", "Norway", "Denmark", "Scandinavian"], "shots": {"0": {"k": 0, "answer": "Denmark/Norway", "latency_ms": 184.78131294250488, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Denmark", "latency_ms": 199.629545211792, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 645}, "2": {"k": 2, "answer": "Norway", "latency_ms": 177.28257179260254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1259}, "3": {"k": 3, "answer": "Germany", "latency_ms": 89.22886848449707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1873}, "4": {"k": 4, "answer": "The context does not provide information on the origin of the last name Hansen. I cannot answer.", "latency_ms": 413.2211208343506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2559}, "5": {"k": 5, "answer": "The context does not specify the origin of the last name Hansen. I cannot answer.", "latency_ms": 440.04011154174805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3195}, "6": {"k": 6, "answer": "Danish", "latency_ms": 275.7875919342041, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3813}}}
{"qid": "2577140274301537828", "question": "what is the symbol for christmas in brazil", "gold_answers": ["Nativity scene", "Christmas Tree"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 268.2948112487793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "oplatek", "latency_ms": 273.2508182525635, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 650}, "2": {"k": 2, "answer": "Christmas tree", "latency_ms": 90.43669700622559, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1279}, "3": {"k": 3, "answer": "Christmas tree", "latency_ms": 261.4419460296631, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1917}, "4": {"k": 4, "answer": "Christmas tree", "latency_ms": 91.59040451049805, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2571}, "5": {"k": 5, "answer": "Christmas tree", "latency_ms": 102.57840156555176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3193}, "6": {"k": 6, "answer": "Christmas tree", "latency_ms": 140.1963233947754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3790}}}
{"qid": "-6762654750812139940", "question": "who sings she's like the wind lyrics", "gold_answers": ["Wendy Fraser", "Patrick Swayze"], "shots": {"0": {"k": 0, "answer": "Linda Ronstadt", "latency_ms": 304.21948432922363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Swayze", "latency_ms": 173.54846000671387, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "Swayze", "latency_ms": 338.5016918182373, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1268}, "3": {"k": 3, "answer": "Swayze", "latency_ms": 182.34562873840332, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1874}, "4": {"k": 4, "answer": "Swayze", "latency_ms": 130.9192180633545, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2480}, "5": {"k": 5, "answer": "Swayze", "latency_ms": 134.58967208862305, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3125}, "6": {"k": 6, "answer": "Swayze", "latency_ms": 183.17294120788574, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3767}}}
{"qid": "-3640066593401557276", "question": "where does the this is us family live", "gold_answers": ["Pittsburgh"], "shots": {"0": {"k": 0, "answer": "Rhode Island", "latency_ms": 148.49042892456055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 143.75710487365723, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 744}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 183.31503868103027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1438}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 343.98937225341797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2130}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 129.00328636169434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2735}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 133.6519718170166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3461}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 277.65512466430664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4132}}}
{"qid": "-4546405080702371934", "question": "when was the term prime minister first used", "gold_answers": ["18th century", "1624"], "shots": {"0": {"k": 0, "answer": "1827", "latency_ms": 148.01764488220215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1624", "latency_ms": 374.1564750671387, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 655}, "2": {"k": 2, "answer": "1624", "latency_ms": 233.41774940490723, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1314}, "3": {"k": 3, "answer": "1905", "latency_ms": 108.978271484375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2009}, "4": {"k": 4, "answer": "1905", "latency_ms": 177.02412605285645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2706}, "5": {"k": 5, "answer": "1905", "latency_ms": 318.648099899292, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3428}, "6": {"k": 6, "answer": "1905", "latency_ms": 370.9547519683838, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4096}}}
{"qid": "3653635084331615973", "question": "who won the mens single ice skating 2018", "gold_answers": ["Javier Fernández", "Shoma Uno", "Yuzuru Hanyu"], "shots": {"0": {"k": 0, "answer": "Yuzuru Hanyu", "latency_ms": 323.5175609588623, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Yuzuru Hanyu", "latency_ms": 200.31404495239258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 627}, "2": {"k": 2, "answer": "Yuzuru Hanyu", "latency_ms": 341.3712978363037, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1288}, "3": {"k": 3, "answer": "Yuzuru Hanyu", "latency_ms": 473.58202934265137, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1940}, "4": {"k": 4, "answer": "Yuzuru Hanyu", "latency_ms": 231.62269592285156, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2607}, "5": {"k": 5, "answer": "Yuzuru Hanyu", "latency_ms": 246.9336986541748, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3275}, "6": {"k": 6, "answer": "Yuzuru Hanyu", "latency_ms": 297.3053455352783, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3956}}}
{"qid": "-8943432852681116531", "question": "what is cain and abel software used for", "gold_answers": ["password recovery"], "shots": {"0": {"k": 0, "answer": "Cain and Abel is a network scanning and password cracking tool used for ethical hacking and penetration testing.", "latency_ms": 594.1410064697266, "em": 0.0, "f1": 0.10526315789473684, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "password recovery", "latency_ms": 63.874006271362305, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 695}, "2": {"k": 2, "answer": "password recovery", "latency_ms": 245.3896999359131, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1391}, "3": {"k": 3, "answer": "password recovery", "latency_ms": 224.69806671142578, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2034}, "4": {"k": 4, "answer": "password recovery", "latency_ms": 100.89755058288574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2714}, "5": {"k": 5, "answer": "password recovery for Microsoft Windows", "latency_ms": 147.44329452514648, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3379}, "6": {"k": 6, "answer": "password recovery for Microsoft Windows", "latency_ms": 292.59490966796875, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3964}}}
{"qid": "3562729819386821776", "question": "what is the name of governor of maharashtra", "gold_answers": ["Chennamaneni Vidyasagar Rao"], "shots": {"0": {"k": 0, "answer": "Eknath Shinde", "latency_ms": 201.19071006774902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "C. Vidyasagar Rao", "latency_ms": 128.2656192779541, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "C. Vidyasagar Rao", "latency_ms": 316.4644241333008, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1385}, "3": {"k": 3, "answer": "Ch. Vidyasagar Rao", "latency_ms": 356.7805290222168, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2058}, "4": {"k": 4, "answer": "Ch. Vidyasagar Rao", "latency_ms": 264.4953727722168, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2686}, "5": {"k": 5, "answer": "Ch. Vidyasagar Rao", "latency_ms": 326.2958526611328, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3326}, "6": {"k": 6, "answer": "Ch. Vidyasagar Rao", "latency_ms": 308.5594177246094, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3955}}}
{"qid": "-2040762149245803825", "question": "what channel is abc on in phoenix arizona", "gold_answers": ["15"], "shots": {"0": {"k": 0, "answer": "ABC 50.1", "latency_ms": 94.39611434936523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "15", "latency_ms": 61.8746280670166, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 682}, "2": {"k": 2, "answer": "15", "latency_ms": 77.73971557617188, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "15", "latency_ms": 163.20419311523438, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1884}, "4": {"k": 4, "answer": "KNXV-TV", "latency_ms": 135.1797580718994, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2486}, "5": {"k": 5, "answer": "15", "latency_ms": 118.04747581481934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3118}, "6": {"k": 6, "answer": "KNXV 15", "latency_ms": 493.0534362792969, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3738}}}
{"qid": "-1378070576053000718", "question": "who played jennifer in back to the future", "gold_answers": ["Claudia Grace Wells"], "shots": {"0": {"k": 0, "answer": "Michael J. Fox", "latency_ms": 356.9176197052002, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Claudia Wells", "latency_ms": 327.5895118713379, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "Claudia Wells", "latency_ms": 272.0630168914795, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1207}, "3": {"k": 3, "answer": "Claudia Wells", "latency_ms": 174.17502403259277, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1829}, "4": {"k": 4, "answer": "Claudia Wells", "latency_ms": 128.23987007141113, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2477}, "5": {"k": 5, "answer": "Claudia Wells", "latency_ms": 171.26703262329102, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3062}, "6": {"k": 6, "answer": "Claudia Wells", "latency_ms": 207.14330673217773, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3693}}}
{"qid": "-8946826352712324685", "question": "where was the summer olympics held in 2012", "gold_answers": ["London", "London , United Kingdom"], "shots": {"0": {"k": 0, "answer": "London, United Kingdom", "latency_ms": 113.56067657470703, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "London, United Kingdom", "latency_ms": 162.17327117919922, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 639}, "2": {"k": 2, "answer": "London, United Kingdom", "latency_ms": 192.24286079406738, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1286}, "3": {"k": 3, "answer": "London, United Kingdom", "latency_ms": 131.5298080444336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1913}, "4": {"k": 4, "answer": "London, United Kingdom", "latency_ms": 168.84684562683105, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2585}, "5": {"k": 5, "answer": "London, United Kingdom", "latency_ms": 127.0303726196289, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3220}, "6": {"k": 6, "answer": "London, United Kingdom", "latency_ms": 196.41518592834473, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3847}}}
{"qid": "-5578685052965147220", "question": "who cooks for the president of the united states", "gold_answers": ["Cristeta Comerford", "The White House Executive Chef", "White House Executive Chef"], "shots": {"0": {"k": 0, "answer": "The First Lady or designated chef", "latency_ms": 106.0950756072998, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "White House Executive Chef", "latency_ms": 130.17535209655762, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 686}, "2": {"k": 2, "answer": "White House Executive Chef", "latency_ms": 103.51014137268066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1355}, "3": {"k": 3, "answer": "White House Executive Chef", "latency_ms": 118.07131767272949, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2038}, "4": {"k": 4, "answer": "White House Executive Chef", "latency_ms": 209.30218696594238, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2650}, "5": {"k": 5, "answer": "White House Executive Chef", "latency_ms": 343.3666229248047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3311}, "6": {"k": 6, "answer": "White House Executive Chef", "latency_ms": 253.38339805603027, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3974}}}
{"qid": "-7323892214876884658", "question": "who was the first indian to be appointed as a judge in the international court of justice", "gold_answers": ["Sir Benegal Narsing Rau"], "shots": {"0": {"k": 0, "answer": "Justice M. S. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A. A", "latency_ms": 6817.391395568848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 89.4937515258789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 613}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 169.22664642333984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1248}, "3": {"k": 3, "answer": "B. N. Rao", "latency_ms": 184.098482131958, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1818}, "4": {"k": 4, "answer": "B. N. Rao", "latency_ms": 216.72797203063965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2487}, "5": {"k": 5, "answer": "B. N. Rao", "latency_ms": 312.9453659057617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3111}, "6": {"k": 6, "answer": "B. N. Rao", "latency_ms": 209.32722091674805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3691}}}
{"qid": "-7530191989994326613", "question": "where was the film coming home for christmas filmed", "gold_answers": ["British Columbia"], "shots": {"0": {"k": 0, "answer": "Canada", "latency_ms": 41.18084907531738, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "British Columbia", "latency_ms": 62.912940979003906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 656}, "2": {"k": 2, "answer": "British Columbia", "latency_ms": 69.33951377868652, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1311}, "3": {"k": 3, "answer": "British Columbia", "latency_ms": 111.77682876586914, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1960}, "4": {"k": 4, "answer": "British Columbia", "latency_ms": 218.2002067565918, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2584}, "5": {"k": 5, "answer": "British Columbia", "latency_ms": 136.22713088989258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3229}, "6": {"k": 6, "answer": "British Columbia", "latency_ms": 116.48344993591309, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3910}}}
{"qid": "1036801998292429298", "question": "when do the new episodes of supernatural start", "gold_answers": ["May 10 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 79.3449878692627, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "October 11, 2018", "latency_ms": 316.97654724121094, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 707}, "2": {"k": 2, "answer": "October 11, 2018", "latency_ms": 212.13483810424805, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1359}, "3": {"k": 3, "answer": "October 11, 2018", "latency_ms": 308.3822727203369, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2020}, "4": {"k": 4, "answer": "October 11, 2018", "latency_ms": 394.72436904907227, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2619}, "5": {"k": 5, "answer": "October 11, 2018", "latency_ms": 373.3651638031006, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3267}, "6": {"k": 6, "answer": "October 11, 2018", "latency_ms": 319.4112777709961, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3912}}}
{"qid": "-7296166998534064378", "question": "who played the middle sister on full house", "gold_answers": ["Jodie Sweetin"], "shots": {"0": {"k": 0, "answer": "Dana Delany", "latency_ms": 77.1169662475586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 139.92977142333984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 153.9156436920166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1215}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 178.9238452911377, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1874}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 119.34113502502441, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2550}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 183.70294570922852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3179}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 143.8138484954834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3852}}}
{"qid": "-4173722566701480014", "question": "a 58-story skyscraper in san francisco is tilting and sinking", "gold_answers": ["Millennium Tower"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 88.37318420410156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Millennium Tower", "latency_ms": 107.09547996520996, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 650}, "2": {"k": 2, "answer": "Millennium Tower", "latency_ms": 271.5449333190918, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Millennium Tower", "latency_ms": 145.0045108795166, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1966}, "4": {"k": 4, "answer": "Millennium Tower", "latency_ms": 144.35744285583496, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2661}, "5": {"k": 5, "answer": "Millennium Tower", "latency_ms": 361.27185821533203, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3357}, "6": {"k": 6, "answer": "Millennium Tower", "latency_ms": 185.2719783782959, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4043}}}
{"qid": "8741253661372783389", "question": "when was the first season of when calls the heart", "gold_answers": ["2014", "January 11 , 2014"], "shots": {"0": {"k": 0, "answer": "2014", "latency_ms": 89.04838562011719, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "2015", "latency_ms": 217.87023544311523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 626}, "2": {"k": 2, "answer": "2014", "latency_ms": 123.14653396606445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1234}, "3": {"k": 3, "answer": "2015", "latency_ms": 161.1311435699463, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1862}, "4": {"k": 4, "answer": "2014", "latency_ms": 150.01177787780762, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2535}, "5": {"k": 5, "answer": "2014", "latency_ms": 282.24658966064453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3149}, "6": {"k": 6, "answer": "2014", "latency_ms": 143.7203884124756, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3807}}}
{"qid": "-2244798821441304446", "question": "who said one man's vulgarity is another's lyric", "gold_answers": ["Justice Harlan"], "shots": {"0": {"k": 0, "answer": "T.S. Eliot", "latency_ms": 110.5499267578125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "John Marshall Harlan", "latency_ms": 137.44282722473145, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "John Marshall Harlan", "latency_ms": 275.44426918029785, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1390}, "3": {"k": 3, "answer": "John Marshall Harlan", "latency_ms": 305.0038814544678, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2069}, "4": {"k": 4, "answer": "John Marshall Harlan II", "latency_ms": 329.03146743774414, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2750}, "5": {"k": 5, "answer": "John Marshall Harlan II", "latency_ms": 182.8913688659668, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3453}, "6": {"k": 6, "answer": "John Marshall Harlan II", "latency_ms": 284.82699394226074, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 4112}}}
{"qid": "4764100575661627990", "question": "what football player is called the honey badger", "gold_answers": ["Tyrann Devine Mathieu"], "shots": {"0": {"k": 0, "answer": "Chris Berman", "latency_ms": 76.17402076721191, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Tyrann Mathieu", "latency_ms": 137.31145858764648, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "Tyrann Mathieu", "latency_ms": 211.11702919006348, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1292}, "3": {"k": 3, "answer": "Tyrann Mathieu", "latency_ms": 265.28120040893555, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1931}, "4": {"k": 4, "answer": "Tyrann Mathieu", "latency_ms": 401.0801315307617, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2568}, "5": {"k": 5, "answer": "Tyrann Mathieu", "latency_ms": 218.9922332763672, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3181}, "6": {"k": 6, "answer": "Tyrann Mathieu", "latency_ms": 144.36769485473633, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3816}}}
{"qid": "-2273381713182307888", "question": "how many episodes of the killing on netflix", "gold_answers": ["six", "6"], "shots": {"0": {"k": 0, "answer": "100 episodes", "latency_ms": 176.95021629333496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "6", "latency_ms": 70.8928108215332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 660}, "2": {"k": 2, "answer": "6", "latency_ms": 58.86054039001465, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1367}, "3": {"k": 3, "answer": "6", "latency_ms": 190.35840034484863, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2042}, "4": {"k": 4, "answer": "6", "latency_ms": 77.86798477172852, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2715}, "5": {"k": 5, "answer": "6", "latency_ms": 100.95763206481934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3373}, "6": {"k": 6, "answer": "6", "latency_ms": 108.99591445922852, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4016}}}
{"qid": "7214445857680379083", "question": "from whose perspective is the story of all quiet on the western front told", "gold_answers": ["Paul Baumer"], "shots": {"0": {"k": 0, "answer": "Paul Baumer's", "latency_ms": 181.9443702697754, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "German perspective", "latency_ms": 116.65534973144531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "German perspective", "latency_ms": 93.34945678710938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1285}, "3": {"k": 3, "answer": "young soldiers' perspective", "latency_ms": 251.16467475891113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1930}, "4": {"k": 4, "answer": "German infantryman", "latency_ms": 115.04149436950684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2571}, "5": {"k": 5, "answer": "German infantryman's perspective", "latency_ms": 314.2678737640381, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3180}, "6": {"k": 6, "answer": "German infantryman", "latency_ms": 256.5450668334961, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3798}}}
{"qid": "4905519627710399354", "question": "who was the person who escaped from alcatraz", "gold_answers": ["John Anglin", "Clarence Anglin", "Frank Morris"], "shots": {"0": {"k": 0, "answer": "Frank Morris and the Anglin brothers (Sundberg) — but only Frank Morris is confirmed to have escaped.  \nFrank Morris", "latency_ms": 548.6745834350586, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Frank Morris, John Anglin, and Clarence Anglin", "latency_ms": 491.88780784606934, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 662}, "2": {"k": 2, "answer": "Frank Morris, John Anglin, and Clarence Anglin", "latency_ms": 205.49559593200684, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 1347}, "3": {"k": 3, "answer": "Frank Morris, John Anglin, and Clarence Anglin", "latency_ms": 645.5698013305664, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 1986}, "4": {"k": 4, "answer": "Frank Morris, John Anglin, and Clarence Anglin", "latency_ms": 590.848445892334, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 2624}, "5": {"k": 5, "answer": "Frank Morris, John Anglin, and Clarence Anglin", "latency_ms": 694.7085857391357, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 3244}, "6": {"k": 6, "answer": "Frank Morris, John Anglin, and Clarence Anglin", "latency_ms": 351.5586853027344, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 3898}}}
{"qid": "-5199162398326752157", "question": "where does trick or treat for unicef money go", "gold_answers": ["UNICEF 's global programing"], "shots": {"0": {"k": 0, "answer": "UNICEF programs worldwide", "latency_ms": 227.5693416595459, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "UNICEF's global programming and domestic causes like Hurricane Katrina victims", "latency_ms": 555.5233955383301, "em": 0.0, "f1": 0.14285714285714288, "containment": 0.0, "context_chars": 692}, "2": {"k": 2, "answer": "UNICEF's global programming, including domestic causes like Hurricane Katrina aid and therapeutic food for malnourished children.", "latency_ms": 724.8430252075195, "em": 0.0, "f1": 0.1, "containment": 0.0, "context_chars": 1350}, "3": {"k": 3, "answer": "UNICEF's global programming, including domestic causes like Hurricane Katrina aid and therapeutic food for malnourished children.", "latency_ms": 589.6656513214111, "em": 0.0, "f1": 0.1, "containment": 0.0, "context_chars": 2070}, "4": {"k": 4, "answer": "UNICEF's global programming, including domestic causes like Hurricane Katrina aid and therapeutic food for malnourished children.", "latency_ms": 648.2436656951904, "em": 0.0, "f1": 0.1, "containment": 0.0, "context_chars": 2800}, "5": {"k": 5, "answer": "UNICEF's global programming, including domestic causes like Hurricane Katrina victims and therapeutic food for severely malnourished children.", "latency_ms": 675.8148670196533, "em": 0.0, "f1": 0.09523809523809523, "containment": 0.0, "context_chars": 3488}, "6": {"k": 6, "answer": "UNICEF's global programming, including domestic causes like Hurricane Katrina aid and therapeutic food for severely malnourished children.", "latency_ms": 701.3552188873291, "em": 0.0, "f1": 0.09523809523809523, "containment": 0.0, "context_chars": 4114}}}
{"qid": "9200381329776083757", "question": "where is the inscription on the statue of liberty", "gold_answers": ["The New Colossus"], "shots": {"0": {"k": 0, "answer": "Statue's base in New York Harbor", "latency_ms": 423.22325706481934, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Base", "latency_ms": 45.2115535736084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 628}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 177.07562446594238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1313}, "3": {"k": 3, "answer": "base of the Statue of Liberty", "latency_ms": 259.68122482299805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1930}, "4": {"k": 4, "answer": "on the base of the Statue of Liberty", "latency_ms": 188.89665603637695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2578}, "5": {"k": 5, "answer": "on the base of the Statue of Liberty", "latency_ms": 310.2755546569824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3195}, "6": {"k": 6, "answer": "Liberty Island", "latency_ms": 234.97653007507324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3899}}}
{"qid": "9179702110514757818", "question": "where are the next two winter olympics going to be held", "gold_answers": ["Beijing", "TBD"], "shots": {"0": {"k": 0, "answer": "2026 Milano-Cortina, 2030 Beijing", "latency_ms": 635.2739334106445, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 211.37022972106934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "Beijing and Hebei, China", "latency_ms": 274.5029926300049, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1231}, "3": {"k": 3, "answer": "Beijing 2022, then next after that is not specified in context. I cannot answer.", "latency_ms": 821.2189674377441, "em": 0.0, "f1": 0.13333333333333333, "containment": 1.0, "context_chars": 1823}, "4": {"k": 4, "answer": "Beijing 2022, then next after that is not specified in context. I cannot answer.", "latency_ms": 727.5357246398926, "em": 0.0, "f1": 0.13333333333333333, "containment": 1.0, "context_chars": 2496}, "5": {"k": 5, "answer": "Beijing 2022, then next after that is not specified in context. I cannot answer.", "latency_ms": 758.4152221679688, "em": 0.0, "f1": 0.13333333333333333, "containment": 1.0, "context_chars": 3180}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 188.61961364746094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3852}}}
{"qid": "-5410471527866417159", "question": "when did i'm coming out come out", "gold_answers": ["August 22 , 1980"], "shots": {"0": {"k": 0, "answer": "1988", "latency_ms": 112.18786239624023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 180.18198013305664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "1997", "latency_ms": 191.97916984558105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1291}, "3": {"k": 3, "answer": "1997", "latency_ms": 134.68194007873535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1981}, "4": {"k": 4, "answer": "1997", "latency_ms": 141.6020393371582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2643}, "5": {"k": 5, "answer": "1997", "latency_ms": 339.0219211578369, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3262}, "6": {"k": 6, "answer": "1997", "latency_ms": 314.0079975128174, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3859}}}
{"qid": "-3621585537083644585", "question": "what type of bridge is the charles bridge", "gold_answers": ["bow bridge", "a bow bridge", "Stone"], "shots": {"0": {"k": 0, "answer": "Stone arch bridge", "latency_ms": 216.6295051574707, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "single-leaf, through-truss, rolling bascule bridge", "latency_ms": 369.8141574859619, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 678}, "2": {"k": 2, "answer": "single-leaf, through-truss, rolling bascule bridges", "latency_ms": 267.73858070373535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1379}, "3": {"k": 3, "answer": "rolling bascule bridge", "latency_ms": 256.5782070159912, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2038}, "4": {"k": 4, "answer": "historic bridge", "latency_ms": 87.0506763458252, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2671}, "5": {"k": 5, "answer": "historic bridge", "latency_ms": 103.45840454101562, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3308}, "6": {"k": 6, "answer": "stone bridge", "latency_ms": 192.274808883667, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3975}}}
{"qid": "-6934435686987287378", "question": "who makes the important government decisions in an autocracy", "gold_answers": ["one person"], "shots": {"0": {"k": 0, "answer": "The ruler or ruling party", "latency_ms": 88.13285827636719, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "one person", "latency_ms": 104.24447059631348, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 657}, "2": {"k": 2, "answer": "one person", "latency_ms": 169.41118240356445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1345}, "3": {"k": 3, "answer": "one person", "latency_ms": 124.08232688903809, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2018}, "4": {"k": 4, "answer": "one person", "latency_ms": 200.1636028289795, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2672}, "5": {"k": 5, "answer": "one person", "latency_ms": 143.36824417114258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3336}, "6": {"k": 6, "answer": "one person", "latency_ms": 140.2747631072998, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4005}}}
{"qid": "-7488727646515698670", "question": "what was the main characteristic of post-world war ii american society", "gold_answers": ["strong economic growth"], "shots": {"0": {"k": 0, "answer": "Consumerism", "latency_ms": 124.75109100341797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "affluence and pride in private property", "latency_ms": 284.9745750427246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 726}, "2": {"k": 2, "answer": "pride in affluence and proprietary attitude toward lawns", "latency_ms": 271.81243896484375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1399}, "3": {"k": 3, "answer": "other-directedness", "latency_ms": 155.3037166595459, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2115}, "4": {"k": 4, "answer": "other-directedness", "latency_ms": 119.32897567749023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2743}, "5": {"k": 5, "answer": "other-directedness", "latency_ms": 208.6167335510254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3483}, "6": {"k": 6, "answer": "other-directedness", "latency_ms": 195.05047798156738, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4192}}}
{"qid": "5760101138584878549", "question": "which way does the earth orbit the sun", "gold_answers": ["counter clockwise direction", "counter clockwise"], "shots": {"0": {"k": 0, "answer": "Counterclockwise (as viewed from above the North Pole)", "latency_ms": 466.5660858154297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "counterclockwise", "latency_ms": 142.7927017211914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 566}, "2": {"k": 2, "answer": "counterclockwise", "latency_ms": 258.9709758758545, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1163}, "3": {"k": 3, "answer": "counterclockwise", "latency_ms": 188.33303451538086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1770}, "4": {"k": 4, "answer": "counterclockwise", "latency_ms": 178.49421501159668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2344}, "5": {"k": 5, "answer": "counterclockwise", "latency_ms": 183.88032913208008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3019}, "6": {"k": 6, "answer": "counterclockwise", "latency_ms": 152.49276161193848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3590}}}
{"qid": "-7003244893871973796", "question": "what is on top of us capitol building", "gold_answers": ["The Statue of Freedom"], "shots": {"0": {"k": 0, "answer": "Dome", "latency_ms": 188.91263008117676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "dome", "latency_ms": 72.97730445861816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "Statue of Freedom", "latency_ms": 93.3527946472168, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1177}, "3": {"k": 3, "answer": "Statue of Freedom", "latency_ms": 174.74699020385742, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1791}, "4": {"k": 4, "answer": "Statue of Freedom", "latency_ms": 152.4367332458496, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2367}, "5": {"k": 5, "answer": "Statue of Freedom", "latency_ms": 174.68500137329102, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3022}, "6": {"k": 6, "answer": "Statue of Freedom", "latency_ms": 223.22773933410645, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3682}}}
{"qid": "-3587434056219625939", "question": "who did dwight howard play for last year", "gold_answers": ["Charlotte Hornets", "Atlanta Hawks"], "shots": {"0": {"k": 0, "answer": "Los Angeles Lakers", "latency_ms": 93.29843521118164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 99.92289543151855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 602}, "2": {"k": 2, "answer": "Washington Wizards", "latency_ms": 70.36519050598145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1288}, "3": {"k": 3, "answer": "Washington Wizards", "latency_ms": 153.32412719726562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1848}, "4": {"k": 4, "answer": "Washington Wizards", "latency_ms": 153.7027359008789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2454}, "5": {"k": 5, "answer": "Washington Wizards", "latency_ms": 213.09947967529297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3113}, "6": {"k": 6, "answer": "Washington Wizards", "latency_ms": 204.3590545654297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3773}}}
{"qid": "-6209397953683103736", "question": "who lasted the longest in the royal rumble", "gold_answers": ["Rey Mysterio"], "shots": {"0": {"k": 0, "answer": "Steve Austin", "latency_ms": 81.45666122436523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Chris Benoit", "latency_ms": 219.74658966064453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 610}, "2": {"k": 2, "answer": "Daniel Bryan", "latency_ms": 73.76265525817871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1228}, "3": {"k": 3, "answer": "Rey Mysterio", "latency_ms": 127.90989875793457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1829}, "4": {"k": 4, "answer": "Rey Mysterio", "latency_ms": 173.7675666809082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2491}, "5": {"k": 5, "answer": "Rey Mysterio", "latency_ms": 229.4783592224121, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3119}, "6": {"k": 6, "answer": "Rey Mysterio", "latency_ms": 208.69827270507812, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3729}}}
{"qid": "-5424723408850352324", "question": "what is the largest man made lake in the us", "gold_answers": ["Lake Oahe"], "shots": {"0": {"k": 0, "answer": "Lake Mead", "latency_ms": 66.91336631774902, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lake of the Ozarks", "latency_ms": 191.86687469482422, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "Lake Mead", "latency_ms": 166.96643829345703, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1223}, "3": {"k": 3, "answer": "Lake Mead", "latency_ms": 187.61086463928223, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1810}, "4": {"k": 4, "answer": "Lake Mead", "latency_ms": 130.14769554138184, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2438}, "5": {"k": 5, "answer": "Lake Mead", "latency_ms": 162.98723220825195, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3122}, "6": {"k": 6, "answer": "Lake Mead", "latency_ms": 287.93954849243164, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3705}}}
{"qid": "7225785319983716635", "question": "gabbar is back is a remake of which telugu movie", "gold_answers": ["Tagore"], "shots": {"0": {"k": 0, "answer": "Gabbar Is Back (2023) is a remake of the Telugu movie \"Gabbar Singh\" (2012).", "latency_ms": 460.1480960845947, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 163.19942474365234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "Tagore", "latency_ms": 122.81489372253418, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1209}, "3": {"k": 3, "answer": "Tagore", "latency_ms": 132.41243362426758, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1830}, "4": {"k": 4, "answer": "Tagore", "latency_ms": 183.70437622070312, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2516}, "5": {"k": 5, "answer": "Tagore", "latency_ms": 134.95135307312012, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3134}, "6": {"k": 6, "answer": "Tagore", "latency_ms": 114.47548866271973, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3767}}}
{"qid": "8466196474705624263", "question": "who was running as vice president in 1984", "gold_answers": ["Ferraro", "Vice President George H.W. Bush", "Congresswoman Ferraro", "George H.W. Bush"], "shots": {"0": {"k": 0, "answer": "John Baker", "latency_ms": 52.8264045715332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "George H.W. Bush", "latency_ms": 143.13292503356934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 737}, "2": {"k": 2, "answer": "Geraldine Ferraro", "latency_ms": 158.95318984985352, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1452}, "3": {"k": 3, "answer": "Geraldine Ferraro", "latency_ms": 248.72732162475586, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2141}, "4": {"k": 4, "answer": "Geraldine Ferraro", "latency_ms": 155.76720237731934, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2825}, "5": {"k": 5, "answer": "Geraldine Ferraro", "latency_ms": 187.6997947692871, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3549}, "6": {"k": 6, "answer": "Geraldine Ferraro", "latency_ms": 186.05351448059082, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 4311}}}
{"qid": "7711594393122809900", "question": "where is the largest salt mine in the us", "gold_answers": ["Central New York", "Western New York"], "shots": {"0": {"k": 0, "answer": "Utah", "latency_ms": 54.688215255737305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Hampton Corners, Town of Groveland", "latency_ms": 515.6404972076416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Hampton Corners, Town of Groveland", "latency_ms": 237.31255531311035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1263}, "3": {"k": 3, "answer": "Hampton Corners, New York", "latency_ms": 274.17469024658203, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1870}, "4": {"k": 4, "answer": "Hampton Corners, New York", "latency_ms": 152.55045890808105, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 2487}, "5": {"k": 5, "answer": "Hampton Corners, New York", "latency_ms": 402.8153419494629, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3216}, "6": {"k": 6, "answer": "Hampton Corners, New York", "latency_ms": 239.0613555908203, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3842}}}
{"qid": "-7011448454774306489", "question": "who took the first steps on the moon in 1969", "gold_answers": ["Neil Armstrong"], "shots": {"0": {"k": 0, "answer": "Neil Armstrong", "latency_ms": 61.63763999938965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Neil Armstrong", "latency_ms": 160.96973419189453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 618}, "2": {"k": 2, "answer": "Neil Armstrong", "latency_ms": 72.89409637451172, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1208}, "3": {"k": 3, "answer": "Neil Armstrong", "latency_ms": 173.31671714782715, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1870}, "4": {"k": 4, "answer": "Neil Armstrong", "latency_ms": 319.1049098968506, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2469}, "5": {"k": 5, "answer": "Neil Armstrong", "latency_ms": 120.99266052246094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3106}, "6": {"k": 6, "answer": "Neil Armstrong", "latency_ms": 282.0127010345459, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3673}}}
{"qid": "3125559316824073534", "question": "state with highest population growth rate in india 2011", "gold_answers": ["Dadra and Nagar Haveli", "Meghalaya"], "shots": {"0": {"k": 0, "answer": "Uttar Pradesh", "latency_ms": 105.48925399780273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Meghalaya", "latency_ms": 207.95249938964844, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 654}, "2": {"k": 2, "answer": "Meghalaya", "latency_ms": 214.5533561706543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1326}, "3": {"k": 3, "answer": "Meghalaya", "latency_ms": 106.42027854919434, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1953}, "4": {"k": 4, "answer": "Nagaland", "latency_ms": 188.83800506591797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2522}, "5": {"k": 5, "answer": "Nagaland", "latency_ms": 372.8814125061035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3167}, "6": {"k": 6, "answer": "Nagaland", "latency_ms": 261.9926929473877, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3848}}}
{"qid": "400438603377436284", "question": "which city is called the manchester of north india", "gold_answers": ["Ahmedabad"], "shots": {"0": {"k": 0, "answer": "Lucknow", "latency_ms": 233.08444023132324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kanpur", "latency_ms": 169.06452178955078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "Kanpur", "latency_ms": 135.1463794708252, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1257}, "3": {"k": 3, "answer": "Kanpur", "latency_ms": 226.18651390075684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1824}, "4": {"k": 4, "answer": "Kanpur", "latency_ms": 294.3291664123535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2472}, "5": {"k": 5, "answer": "Kanpur", "latency_ms": 333.3737850189209, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3053}, "6": {"k": 6, "answer": "Kanpur", "latency_ms": 310.06574630737305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3691}}}
