{"qid": "175824256524792922", "question": "who plays harry's mom in harry potter", "gold_answers": ["Geraldine Margaret Agnew - Somerville"], "shots": {"0": {"k": 0, "answer": "Rita Skeeter", "latency_ms": 133.18800926208496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 119.79269981384277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 185.3013038635254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1321}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 249.8171329498291, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1956}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 137.03584671020508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2588}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 143.50366592407227, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3207}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 223.9842414855957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3824}}}
{"qid": "-8223571798173928629", "question": "who sang you'll be a woman soon in pulp fiction", "gold_answers": ["Urge Overkill", "rock band Urge Overkill"], "shots": {"0": {"k": 0, "answer": "Minnie Riperton", "latency_ms": 299.78251457214355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Urge Overkill", "latency_ms": 133.75353813171387, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 653}, "2": {"k": 2, "answer": "Urge Overkill", "latency_ms": 301.9745349884033, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1316}, "3": {"k": 3, "answer": "Urge Overkill", "latency_ms": 112.44964599609375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1940}, "4": {"k": 4, "answer": "Urge Overkill", "latency_ms": 292.266845703125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2597}, "5": {"k": 5, "answer": "Urge Overkill", "latency_ms": 266.33238792419434, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3219}, "6": {"k": 6, "answer": "Urge Overkill", "latency_ms": 239.03822898864746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3892}}}
{"qid": "-8095180930653230478", "question": "where does a roadrunner live in the desert", "gold_answers": ["arid lowland", "mountainous shrubland"], "shots": {"0": {"k": 0, "answer": "Southwestern United States and Mexico", "latency_ms": 342.53883361816406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 482.1155071258545, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 684}, "2": {"k": 2, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 450.88744163513184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1329}, "3": {"k": 3, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 321.3837146759033, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1978}, "4": {"k": 4, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 664.3853187561035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2592}, "5": {"k": 5, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 535.0642204284668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3262}, "6": {"k": 6, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 459.8417282104492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3932}}}
{"qid": "-5489426796364143729", "question": "who was the first president of the constituent assembly", "gold_answers": ["Dr. Rajendra Prasad"], "shots": {"0": {"k": 0, "answer": "Dr. Rajendra Prasad", "latency_ms": 143.61047744750977, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dr. Rajendra Prasad", "latency_ms": 420.11094093322754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 743}, "2": {"k": 2, "answer": "Dr. Rajendra Prasad", "latency_ms": 176.65338516235352, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1429}, "3": {"k": 3, "answer": "Dr. Rajendra Prasad", "latency_ms": 335.2813720703125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2125}, "4": {"k": 4, "answer": "Dr. Rajendra Prasad", "latency_ms": 361.63330078125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2814}, "5": {"k": 5, "answer": "Dr. Rajendra Prasad", "latency_ms": 179.9168586730957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3509}, "6": {"k": 6, "answer": "Dr. Sachchidananda Sinha", "latency_ms": 261.7835998535156, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 4207}}}
{"qid": "7835345385704208729", "question": "when does the new episode of scorpion come on", "gold_answers": ["January 15 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 134.9952220916748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 234.86971855163574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 89.04480934143066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1253}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 341.62211418151855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1912}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 237.78629302978516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2573}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 219.30694580078125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3179}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 281.09192848205566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3861}}}
{"qid": "907238223200158003", "question": "when did first fast and furious come out", "gold_answers": ["2001"], "shots": {"0": {"k": 0, "answer": "2001", "latency_ms": 153.7179946899414, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "2001", "latency_ms": 101.24731063842773, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 640}, "2": {"k": 2, "answer": "2001", "latency_ms": 202.1484375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1259}, "3": {"k": 3, "answer": "2001", "latency_ms": 334.1023921966553, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1878}, "4": {"k": 4, "answer": "2001", "latency_ms": 441.9054985046387, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2536}, "5": {"k": 5, "answer": "June 22, 2001", "latency_ms": 430.802583694458, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3180}, "6": {"k": 6, "answer": "June 22, 2001", "latency_ms": 273.8785743713379, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3795}}}
{"qid": "-6782664238831495856", "question": "when does season 5 of the blacklist resume", "gold_answers": ["January 31 , 2018"], "shots": {"0": {"k": 0, "answer": "2018", "latency_ms": 370.7253932952881, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wednesday at 8:00 PM", "latency_ms": 538.7051105499268, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 103.44171524047852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1361}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 188.80486488342285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 219.7117805480957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2705}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 265.1183605194092, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3344}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 283.2772731781006, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4040}}}
{"qid": "3492797766982308854", "question": "when do mr schuester and emma get together", "gold_answers": ["the fourth season"], "shots": {"0": {"k": 0, "answer": "Season 4, Episode 10 \"The Power of Love\"", "latency_ms": 631.4969062805176, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "February 14, 2013", "latency_ms": 225.07905960083008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 684}, "2": {"k": 2, "answer": "February 14, 2013", "latency_ms": 331.70437812805176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1318}, "3": {"k": 3, "answer": "February 14, 2013", "latency_ms": 501.7709732055664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1987}, "4": {"k": 4, "answer": "February 14, 2013", "latency_ms": 371.94228172302246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2648}, "5": {"k": 5, "answer": "January 17, 2012", "latency_ms": 485.1105213165283, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3314}, "6": {"k": 6, "answer": "January 17, 2012", "latency_ms": 310.90569496154785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3975}}}
{"qid": "-2654989306076235475", "question": "when does the next episode of izombie air", "gold_answers": ["May 14 , 2018", "May 7 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 126.10363960266113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 221.57573699951172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 596}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 189.42880630493164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1192}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 270.2441215515137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1837}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 197.2026824951172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2473}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 305.8185577392578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3115}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 392.84777641296387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3815}}}
{"qid": "3066393401707107613", "question": "where was the u.s.s maine when it exploded in 1898", "gold_answers": ["Havana Harbor"], "shots": {"0": {"k": 0, "answer": "Havana, Cuba", "latency_ms": 246.18124961853027, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Havana harbor, Cuba", "latency_ms": 121.85406684875488, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 620}, "2": {"k": 2, "answer": "Havana Harbor, Cuba", "latency_ms": 244.52590942382812, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1254}, "3": {"k": 3, "answer": "Havana Harbor, Cuba", "latency_ms": 137.60066032409668, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1910}, "4": {"k": 4, "answer": "Havana Harbor, Cuba", "latency_ms": 385.80846786499023, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2533}, "5": {"k": 5, "answer": "Havana Harbor, Cuba", "latency_ms": 243.1046962738037, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3164}, "6": {"k": 6, "answer": "Havana Harbor, Cuba", "latency_ms": 207.66115188598633, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3769}}}
{"qid": "6972056626325669866", "question": "in the honour of which god is anant chaturdashi celebrated", "gold_answers": ["Ganesh"], "shots": {"0": {"k": 0, "answer": "Shiva", "latency_ms": 135.6983184814453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 219.6800708770752, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "Jainism", "latency_ms": 132.8125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1280}, "3": {"k": 3, "answer": "Lord Vasupujya", "latency_ms": 209.6109390258789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1942}, "4": {"k": 4, "answer": "Lord Vasupujya", "latency_ms": 344.3865776062012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2575}, "5": {"k": 5, "answer": "Lord Ganesha", "latency_ms": 139.16969299316406, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3221}, "6": {"k": 6, "answer": "Ganesha", "latency_ms": 233.61802101135254, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3870}}}
{"qid": "6043389452532567502", "question": "which central american nations border the pacific ocean and the caribbean sea", "gold_answers": ["Costa Rica", "Nicaragua"], "shots": {"0": {"k": 0, "answer": "Belize, Guatemala, Honduras, Nicaragua, Costa Rica", "latency_ms": 258.47792625427246, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Belize, Guatemala, Honduras, Nicaragua, Panama", "latency_ms": 192.29626655578613, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 662}, "2": {"k": 2, "answer": "Belize, Guatemala, Honduras, Nicaragua, Panama", "latency_ms": 428.78150939941406, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1294}, "3": {"k": 3, "answer": "Belize, Guatemala, Honduras, Nicaragua, Panama", "latency_ms": 709.0399265289307, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1901}, "4": {"k": 4, "answer": "Belize, Guatemala, Honduras, Nicaragua, Panama", "latency_ms": 200.11496543884277, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 2494}, "5": {"k": 5, "answer": "Belize, Guatemala, Honduras, Nicaragua, Panama", "latency_ms": 293.41673851013184, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3189}, "6": {"k": 6, "answer": "Belize, Guatemala, Honduras, Nicaragua, Panama", "latency_ms": 572.1516609191895, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3823}}}
{"qid": "2541316470469570888", "question": "what kind of sentence contains an independent clause and a dependent clause", "gold_answers": ["A complex sentence", "complex sentence"], "shots": {"0": {"k": 0, "answer": "complex sentence", "latency_ms": 123.91185760498047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "complex sentence", "latency_ms": 175.7042407989502, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 662}, "2": {"k": 2, "answer": "complex sentence", "latency_ms": 76.18546485900879, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1350}, "3": {"k": 3, "answer": "complex sentence", "latency_ms": 113.76214027404785, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2037}, "4": {"k": 4, "answer": "complex sentence", "latency_ms": 214.43724632263184, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2743}, "5": {"k": 5, "answer": "complex sentence", "latency_ms": 277.51660346984863, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3425}, "6": {"k": 6, "answer": "complex sentence", "latency_ms": 200.4406452178955, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4084}}}
{"qid": "3239205946903665891", "question": "what was the book wealth of nations about", "gold_answers": ["what builds nations ' wealth"], "shots": {"0": {"k": 0, "answer": "economic systems and trade principles", "latency_ms": 205.34873008728027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "The division of labour, productivity, and free markets", "latency_ms": 343.7538146972656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "The Wealth of Nations was about the nature and causes of the wealth of nations, including division of labour, productivity, and free markets.", "latency_ms": 985.4335784912109, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 1326}, "3": {"k": 3, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of national wealth, including the division of labor, productivity, and free markets.", "latency_ms": 576.5416622161865, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 2000}, "4": {"k": 4, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of national wealth, including the division of labor, productivity, and free markets.", "latency_ms": 1003.143310546875, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 2681}, "5": {"k": 5, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of national wealth, including the division of labor, productivity, free markets, and the three factors of production: land, labor, and capital.", "latency_ms": 1083.1046104431152, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 3408}, "6": {"k": 6, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of national wealth, including the division of labor, productivity, free markets, and the three factors of production: land, labor, and capital.", "latency_ms": 1268.282175064087, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 4069}}}
{"qid": "-4659426898177460954", "question": "when will the flash season 4 be released", "gold_answers": ["October 10 , 2017"], "shots": {"0": {"k": 0, "answer": "2024", "latency_ms": 160.78495979309082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "October 10, 2017", "latency_ms": 228.10816764831543, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "October 10, 2017", "latency_ms": 461.7941379547119, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "August 28, 2018", "latency_ms": 314.9700164794922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1852}, "4": {"k": 4, "answer": "August 23, 2018", "latency_ms": 326.68542861938477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2494}, "5": {"k": 5, "answer": "August 23, 2018", "latency_ms": 363.56163024902344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3109}, "6": {"k": 6, "answer": "August 23, 2018", "latency_ms": 267.7886486053467, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3763}}}
{"qid": "-1204722541964024977", "question": "what is australia's location in the world and region", "gold_answers": ["Oceania"], "shots": {"0": {"k": 0, "answer": "Southern Hemisphere, Eastern Hemisphere, Oceania region", "latency_ms": 349.84707832336426, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Oceania", "latency_ms": 232.58233070373535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 626}, "2": {"k": 2, "answer": "Southern Hemisphere, Eastern Hemisphere, Oceania region", "latency_ms": 298.1712818145752, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 1254}, "3": {"k": 3, "answer": "southern hemisphere, Eastern Hemisphere, Oceania", "latency_ms": 447.24154472351074, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1907}, "4": {"k": 4, "answer": "southern hemisphere, eastern hemisphere, Oceania region", "latency_ms": 531.4562320709229, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 2616}, "5": {"k": 5, "answer": "Southern Hemisphere, Eastern Hemisphere; Oceania region", "latency_ms": 576.045036315918, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 3248}, "6": {"k": 6, "answer": "Southern Hemisphere, Eastern Hemisphere, Oceania region", "latency_ms": 582.1146965026855, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 3956}}}
{"qid": "-6291722830448463311", "question": "who does luke skywalker fall in love with", "gold_answers": ["Mara Jade"], "shots": {"0": {"k": 0, "answer": "Padmé Amidala", "latency_ms": 141.74818992614746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 234.86852645874023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "Leia", "latency_ms": 142.57526397705078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1190}, "3": {"k": 3, "answer": "Princess Leia", "latency_ms": 196.81715965270996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1785}, "4": {"k": 4, "answer": "Leia Organa", "latency_ms": 107.05280303955078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2382}, "5": {"k": 5, "answer": "Princess Leia", "latency_ms": 177.88243293762207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2994}, "6": {"k": 6, "answer": "Callista", "latency_ms": 249.30620193481445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3614}}}
{"qid": "-5631283176059213935", "question": "who was the original host of jokers wild", "gold_answers": ["Jack Barry"], "shots": {"0": {"k": 0, "answer": "Ralph Edwards", "latency_ms": 87.54539489746094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Allen Ludden", "latency_ms": 122.23362922668457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 643}, "2": {"k": 2, "answer": "Allen Ludden", "latency_ms": 119.22264099121094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1291}, "3": {"k": 3, "answer": "Barry Cryer", "latency_ms": 340.82889556884766, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1841}, "4": {"k": 4, "answer": "Barry Cryer", "latency_ms": 142.46368408203125, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2471}, "5": {"k": 5, "answer": "Barry Cryer", "latency_ms": 124.45521354675293, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3021}, "6": {"k": 6, "answer": "Barry Cryer", "latency_ms": 178.4214973449707, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3654}}}
{"qid": "5360223163814883105", "question": "which organelle is the site of atp production and storage", "gold_answers": ["mitochondrion"], "shots": {"0": {"k": 0, "answer": "mitochondria", "latency_ms": 78.87744903564453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "mitochondrion", "latency_ms": 222.18728065490723, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 707}, "2": {"k": 2, "answer": "mitochondrion", "latency_ms": 149.41024780273438, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1366}, "3": {"k": 3, "answer": "mitochondrion", "latency_ms": 135.9245777130127, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2064}, "4": {"k": 4, "answer": "mitochondria", "latency_ms": 100.13008117675781, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2715}, "5": {"k": 5, "answer": "mitochondria", "latency_ms": 197.53408432006836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3391}, "6": {"k": 6, "answer": "mitochondria", "latency_ms": 154.6943187713623, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4070}}}
{"qid": "-718217992489656032", "question": "five tools of security management to overcome computer crime", "gold_answers": ["post-evaluation", "operative planning", "implementation", "Pre-evaluation", "strategic planning"], "shots": {"0": {"k": 0, "answer": "Risk assessment, access control, encryption, intrusion detection, employee training", "latency_ms": 393.2344913482666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 136.80148124694824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 707}, "2": {"k": 2, "answer": "Computer virus, Veriato Log Manager, Spector Pro, Reverse Engineering, Incident Response Manager", "latency_ms": 1031.1384201049805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1380}, "3": {"k": 3, "answer": "Incident Response Manager, Veriato Log Manager, Veriato Server Manager, Armitage, Spector Pro", "latency_ms": 1154.3161869049072, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2097}, "4": {"k": 4, "answer": "Veriato, Armitage, Spector Pro, Incident Response Manager, Computer Forensics Tools", "latency_ms": 478.1200885772705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2806}, "5": {"k": 5, "answer": "Veriato, Armitage, Spector Pro, Incident Response Manager, Metasploit Framework", "latency_ms": 854.3753623962402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3562}, "6": {"k": 6, "answer": "Veriato, Armitage, Spector Pro, Incident Response Manager, ISO/IEC 27000-series standards", "latency_ms": 1437.8011226654053, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4251}}}
{"qid": "5584540254904933863", "question": "who sang what are we doing in love", "gold_answers": ["Dottie West", "Kenny Rogers"], "shots": {"0": {"k": 0, "answer": "Luther Vandross", "latency_ms": 75.93083381652832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kenny Rogers and Dottie West", "latency_ms": 247.37262725830078, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 590}, "2": {"k": 2, "answer": "Kenny Rogers and Dottie West", "latency_ms": 252.15458869934082, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1228}, "3": {"k": 3, "answer": "Kenny Rogers and Dottie West", "latency_ms": 365.3542995452881, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1838}, "4": {"k": 4, "answer": "Kenny Rogers and Dottie West", "latency_ms": 355.1619052886963, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2456}, "5": {"k": 5, "answer": "Kenny Rogers", "latency_ms": 147.5694179534912, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3082}, "6": {"k": 6, "answer": "Kenny Rogers and Dottie West", "latency_ms": 180.8924674987793, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3663}}}
{"qid": "97369777493072047", "question": "at what age are you no longer a toddler", "gold_answers": ["36 months"], "shots": {"0": {"k": 0, "answer": "3 years old", "latency_ms": 65.04130363464355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2 years old", "latency_ms": 99.05481338500977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 581}, "2": {"k": 2, "answer": "36 months", "latency_ms": 123.50344657897949, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1200}, "3": {"k": 3, "answer": "36 months", "latency_ms": 196.9752311706543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1835}, "4": {"k": 4, "answer": "3 years old", "latency_ms": 157.60087966918945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2526}, "5": {"k": 5, "answer": "3 years old", "latency_ms": 317.4633979797363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3129}, "6": {"k": 6, "answer": "3 years old", "latency_ms": 217.44775772094727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3836}}}
{"qid": "3537093784371401720", "question": "how many episodes are there in ordeal by innocence", "gold_answers": ["three", "three part"], "shots": {"0": {"k": 0, "answer": "100", "latency_ms": 164.4148826599121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "3", "latency_ms": 101.78303718566895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "three-part", "latency_ms": 64.83197212219238, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1299}, "3": {"k": 3, "answer": "three episodes", "latency_ms": 162.03570365905762, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1947}, "4": {"k": 4, "answer": "three episodes", "latency_ms": 92.09203720092773, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2599}, "5": {"k": 5, "answer": "three episodes", "latency_ms": 174.6962070465088, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3264}, "6": {"k": 6, "answer": "three episodes", "latency_ms": 283.2498550415039, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3904}}}
{"qid": "-2126094349681352450", "question": "who played the hobbits in the lord of the rings", "gold_answers": ["Ian Holm", "Dominic Monaghan", "Elijah Wood as Frodo Baggins", "Sean Astin", "Elijah Wood", "Billy Boyd"], "shots": {"0": {"k": 0, "answer": "Elijah Wood, Vivien Leigh, Ian McKellen, Orlando Bloom, Sean Astin, Dominic Monaghan, and Billy Boyd", "latency_ms": 667.6278114318848, "em": 0.0, "f1": 0.2352941176470588, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Elijah Wood", "latency_ms": 240.2205467224121, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 672}, "2": {"k": 2, "answer": "Elijah Wood", "latency_ms": 95.49474716186523, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1258}, "3": {"k": 3, "answer": "Elijah Wood, Ian McKellen, Hugo Weaving, Orlando Bloom, Ian Holm", "latency_ms": 613.7497425079346, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1919}, "4": {"k": 4, "answer": "Elijah Wood, Ian McKellen, Hugo Weaving, Orlando Bloom, Ian Holm", "latency_ms": 353.60217094421387, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 2567}, "5": {"k": 5, "answer": "Ian McKellen, Andy Serkis, Hugo Weaving, Elijah Wood, Ian Holm, Christopher Lee, Cate Blanchett, Orlando Bloom", "latency_ms": 969.4156646728516, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 3233}, "6": {"k": 6, "answer": "Elijah Wood", "latency_ms": 203.52602005004883, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3862}}}
{"qid": "-6982991192528081677", "question": "what does fancy dress mean in the uk", "gold_answers": ["costumes", "A costume", "guests dress up in costumes"], "shots": {"0": {"k": 0, "answer": "Wearing costumes for a party or event, often for fun or to celebrate a theme.", "latency_ms": 714.6203517913818, "em": 0.0, "f1": 0.14285714285714288, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "fancy dress party", "latency_ms": 132.28607177734375, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "a costume party", "latency_ms": 122.13802337646484, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1299}, "3": {"k": 3, "answer": "a party where guests dress up in costumes", "latency_ms": 409.67464447021484, "em": 0.0, "f1": 0.8333333333333333, "containment": 1.0, "context_chars": 1971}, "4": {"k": 4, "answer": "fancy dress party", "latency_ms": 112.08438873291016, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 2647}, "5": {"k": 5, "answer": "fancy dress party", "latency_ms": 187.1776580810547, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 3310}, "6": {"k": 6, "answer": "fancy dress party", "latency_ms": 131.7119598388672, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 3940}}}
{"qid": "-6751220433242447969", "question": "how many seasons of the bastard executioner are there", "gold_answers": ["one season", "one"], "shots": {"0": {"k": 0, "answer": "3", "latency_ms": 86.61174774169922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 370.75352668762207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 284.60097312927246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1231}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 110.62908172607422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1863}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 113.02495002746582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2527}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 286.21482849121094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3225}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 338.0417823791504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3912}}}
{"qid": "-7607921794902030568", "question": "why did jean valjean take care of cosette", "gold_answers": ["he had promised"], "shots": {"0": {"k": 0, "answer": "To save her from being taken by the authorities and to protect her from suffering.", "latency_ms": 400.04587173461914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 89.78652954101562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 637}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 148.28205108642578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1230}, "3": {"k": 3, "answer": "rescued her and raised her as if she were his own", "latency_ms": 892.5061225891113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1845}, "4": {"k": 4, "answer": "to rescue and raise her from the Thénardiers' abuse", "latency_ms": 299.88622665405273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2476}, "5": {"k": 5, "answer": "to rescue and raise her from the Thénardiers' exploitation", "latency_ms": 635.1602077484131, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3103}, "6": {"k": 6, "answer": "to rescue and raise her from the Thénardiers' abuse", "latency_ms": 663.6710166931152, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3701}}}
{"qid": "-5341729935918652247", "question": "who does the democratic republic of congo trade with", "gold_answers": ["Belgium", "World Bank", "China", "Zambia", "African Development Bank", "SADC", "South Africa", "AU", "WTO", "France", "Kenya", "Zimbabwe", "Group of 77", "IMF"], "shots": {"0": {"k": 0, "answer": "China, Belgium, France, Zambia, Angola, South Africa, Brazil, Russia, United States, Nigeria, Tanzania, Rwanda, Zambia, Uganda, Kenya, Ethiopia, Sudan, Namibia, Botswana, Zimbabwe, Zambia, Malawi, Mozambique, South Africa, Tanzania, Uganda, Kenya, Ethiopia, Sudan, Namibia, Botswana, Zimbabwe", "latency_ms": 2445.005416870117, "em": 0.0, "f1": 0.1081081081081081, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kenya", "latency_ms": 147.28140830993652, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 688}, "2": {"k": 2, "answer": "Kenya", "latency_ms": 124.23229217529297, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1382}, "3": {"k": 3, "answer": "Kenya, India", "latency_ms": 304.3375015258789, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2093}, "4": {"k": 4, "answer": "Kenya", "latency_ms": 88.03725242614746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2788}, "5": {"k": 5, "answer": "Kenya", "latency_ms": 226.23562812805176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3489}, "6": {"k": 6, "answer": "Kenya", "latency_ms": 106.18066787719727, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4162}}}
{"qid": "5518515944351776633", "question": "what was the purpose of the bantu education act", "gold_answers": ["enforcing racially separated educational facilities"], "shots": {"0": {"k": 0, "answer": "To provide a racially segregated and inferior education for Black South Africans.", "latency_ms": 757.4222087860107, "em": 0.0, "f1": 0.12500000000000003, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 932.802677154541, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 490.0517463684082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1311}, "3": {"k": 3, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 284.82699394226074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2027}, "4": {"k": 4, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 366.3654327392578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2704}, "5": {"k": 5, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 532.745361328125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3382}, "6": {"k": 6, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 830.7602405548096, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4130}}}
{"qid": "-4271099756149693907", "question": "what type of snake is kaa in jungle book", "gold_answers": ["Indian rock python"], "shots": {"0": {"k": 0, "answer": "Python", "latency_ms": 80.96814155578613, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kaa is a python.", "latency_ms": 161.82327270507812, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "Kaa is a giant python.", "latency_ms": 384.3400478363037, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1174}, "3": {"k": 3, "answer": "Indian python", "latency_ms": 82.26966857910156, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1830}, "4": {"k": 4, "answer": "Indian python", "latency_ms": 112.06340789794922, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2445}, "5": {"k": 5, "answer": "Indian python", "latency_ms": 312.0267391204834, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3040}, "6": {"k": 6, "answer": "Indian python", "latency_ms": 406.7208766937256, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3650}}}
{"qid": "-6527462410412242988", "question": "what is the snow on mars made of", "gold_answers": ["water ice", "Frozen carbon dioxide"], "shots": {"0": {"k": 0, "answer": "water ice and carbon dioxide ice", "latency_ms": 154.10685539245605, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "water-ice", "latency_ms": 76.2336254119873, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "water-ice", "latency_ms": 175.50277709960938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1200}, "3": {"k": 3, "answer": "water-ice", "latency_ms": 137.12143898010254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1776}, "4": {"k": 4, "answer": "water-ice", "latency_ms": 176.70536041259766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2423}, "5": {"k": 5, "answer": "water-ice", "latency_ms": 188.41075897216797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2996}, "6": {"k": 6, "answer": "water-ice", "latency_ms": 261.60573959350586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3644}}}
{"qid": "-2155345050664800035", "question": "when does elena turn into a vampire in the tv series", "gold_answers": ["season four", "fourth season"], "shots": {"0": {"k": 0, "answer": "Season 1, Episode 4", "latency_ms": 121.97685241699219, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "when Rebekah causes Matt's truck to drive off Wickery Bridge", "latency_ms": 239.00723457336426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 594}, "2": {"k": 2, "answer": "The Departed (The Vampire Diaries)", "latency_ms": 470.92723846435547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1217}, "3": {"k": 3, "answer": "season four premiere", "latency_ms": 96.74477577209473, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1818}, "4": {"k": 4, "answer": "The episode's initial airing attracted 2.53 million viewers.", "latency_ms": 326.60365104675293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2382}, "5": {"k": 5, "answer": "The episode's initial airing attracted 2.53 million viewers.", "latency_ms": 532.5615406036377, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2999}, "6": {"k": 6, "answer": "The episode's initial airing attracted 2.53 million viewers.", "latency_ms": 251.32298469543457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3599}}}
{"qid": "5491319565508469860", "question": "who gets first pick of fa cup games", "gold_answers": ["the BBC", "BBC"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 134.8888874053955, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 346.8804359436035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 589}, "2": {"k": 2, "answer": "BBC Sport", "latency_ms": 215.1033878326416, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1172}, "3": {"k": 3, "answer": "BBC", "latency_ms": 67.53897666931152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1768}, "4": {"k": 4, "answer": "BBC", "latency_ms": 109.56120491027832, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2392}, "5": {"k": 5, "answer": "BBC", "latency_ms": 124.72891807556152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2976}, "6": {"k": 6, "answer": "BBC", "latency_ms": 276.72696113586426, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3568}}}
{"qid": "-816889472900831237", "question": "what part of dc is howard university in", "gold_answers": ["northwest", "northwest Washington", "in northwest Washington"], "shots": {"0": {"k": 0, "answer": "Ward 8", "latency_ms": 225.6155014038086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Northwest Washington, DC", "latency_ms": 423.5565662384033, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 674}, "2": {"k": 2, "answer": "Northwest Washington, DC", "latency_ms": 276.54147148132324, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1346}, "3": {"k": 3, "answer": "Northwest Washington, DC", "latency_ms": 269.6201801300049, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1995}, "4": {"k": 4, "answer": "Northwest Washington, DC", "latency_ms": 163.5439395904541, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2687}, "5": {"k": 5, "answer": "Northwest Washington, DC", "latency_ms": 162.80293464660645, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3299}, "6": {"k": 6, "answer": "Northwest Washington, DC", "latency_ms": 255.0508975982666, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3933}}}
{"qid": "479254158383415496", "question": "what is the first covenant in the old testament", "gold_answers": ["the Noahic Covenant"], "shots": {"0": {"k": 0, "answer": "Covenant of Noah", "latency_ms": 81.75826072692871, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Covenant of Works", "latency_ms": 84.7463607788086, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 606}, "2": {"k": 2, "answer": "Noahic", "latency_ms": 324.3551254272461, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1256}, "3": {"k": 3, "answer": "Covenant of Works", "latency_ms": 186.16771697998047, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1812}, "4": {"k": 4, "answer": "Covenant of Works", "latency_ms": 140.79737663269043, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2423}, "5": {"k": 5, "answer": "Noahic covenant", "latency_ms": 330.2485942840576, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3095}, "6": {"k": 6, "answer": "Covenant of Works", "latency_ms": 386.2783908843994, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3733}}}
{"qid": "-8025421046364238920", "question": "who sings the theme song for living single", "gold_answers": ["Queen Latifah"], "shots": {"0": {"k": 0, "answer": "Linda Ronstadt", "latency_ms": 155.82728385925293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 176.10740661621094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 577}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 260.82921028137207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1210}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 130.37753105163574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1807}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 222.37443923950195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2394}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 183.93588066101074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2969}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 133.50462913513184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3572}}}
{"qid": "-2347826883564742884", "question": "who said life is like a bowl of cherries", "gold_answers": ["Lew Brown"], "shots": {"0": {"k": 0, "answer": "W. C. Fields", "latency_ms": 195.59240341186523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ray Henderson", "latency_ms": 188.34948539733887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 642}, "2": {"k": 2, "answer": "Ray Henderson", "latency_ms": 121.28734588623047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1234}, "3": {"k": 3, "answer": "Ray Henderson", "latency_ms": 106.41217231750488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1845}, "4": {"k": 4, "answer": "Ray Henderson", "latency_ms": 302.6235103607178, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2463}, "5": {"k": 5, "answer": "Ray Henderson", "latency_ms": 97.83434867858887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3127}, "6": {"k": 6, "answer": "Ray Henderson", "latency_ms": 113.4328842163086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3732}}}
{"qid": "2467509477831469982", "question": "where is fort myers located at in florida", "gold_answers": ["Lee County", "the Southwest"], "shots": {"0": {"k": 0, "answer": "South Florida", "latency_ms": 49.92842674255371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lee County, Florida", "latency_ms": 102.39744186401367, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 642}, "2": {"k": 2, "answer": "Fort Myers, Florida is located in Lee County, United States.", "latency_ms": 276.67808532714844, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1259}, "3": {"k": 3, "answer": "Lee County, Florida", "latency_ms": 128.07655334472656, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1906}, "4": {"k": 4, "answer": "Lee County, Florida", "latency_ms": 136.88158988952637, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2529}, "5": {"k": 5, "answer": "Lee County, Florida", "latency_ms": 288.9447212219238, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3176}, "6": {"k": 6, "answer": "Lee County, Florida", "latency_ms": 225.42405128479004, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3789}}}
{"qid": "4440911260005876349", "question": "state the position of india in terms of population size", "gold_answers": ["second"], "shots": {"0": {"k": 0, "answer": "2nd", "latency_ms": 108.81662368774414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 91.38274192810059, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 663}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 198.9119052886963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "11th", "latency_ms": 131.18743896484375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1860}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 140.64788818359375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2440}, "5": {"k": 5, "answer": "11th largest by population", "latency_ms": 400.0523090362549, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3079}, "6": {"k": 6, "answer": "11th largest by population", "latency_ms": 451.64966583251953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3698}}}
{"qid": "3172949410241605868", "question": "who became a leader of the indian national congress", "gold_answers": ["Mahatma Gandhi"], "shots": {"0": {"k": 0, "answer": "Mahatma Gandhi", "latency_ms": 186.95545196533203, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Mahatma Gandhi", "latency_ms": 187.30640411376953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 706}, "2": {"k": 2, "answer": "Mahatma Gandhi", "latency_ms": 166.36157035827637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1429}, "3": {"k": 3, "answer": "Sonia Gandhi", "latency_ms": 238.0390167236328, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2103}, "4": {"k": 4, "answer": "Sonia Gandhi", "latency_ms": 154.4032096862793, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2799}, "5": {"k": 5, "answer": "Sonia Gandhi", "latency_ms": 115.19765853881836, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3509}, "6": {"k": 6, "answer": "Jawaharlal Nehru", "latency_ms": 427.31595039367676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4184}}}
{"qid": "-5136645336730213305", "question": "winner of worst cooks in america season 5", "gold_answers": ["Amber Brauner"], "shots": {"0": {"k": 0, "answer": "Terry Jones", "latency_ms": 104.5534610748291, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "La Toya Jackson", "latency_ms": 85.91747283935547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 619}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 129.55927848815918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1263}, "3": {"k": 3, "answer": "Bobby Flay", "latency_ms": 316.2682056427002, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1862}, "4": {"k": 4, "answer": "Bobby Flay", "latency_ms": 107.29050636291504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2467}, "5": {"k": 5, "answer": "Bobby Flay", "latency_ms": 177.49905586242676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3086}, "6": {"k": 6, "answer": "Bobby Flay", "latency_ms": 139.30892944335938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3703}}}
{"qid": "159123015669900402", "question": "what is the filename extension used for all java source files", "gold_answers": [". java"], "shots": {"0": {"k": 0, "answer": ".java", "latency_ms": 43.15924644470215, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": ".java", "latency_ms": 137.2683048248291, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 601}, "2": {"k": 2, "answer": ".java", "latency_ms": 73.86445999145508, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": ".java", "latency_ms": 185.98008155822754, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1932}, "4": {"k": 4, "answer": ".class", "latency_ms": 107.59377479553223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2592}, "5": {"k": 5, "answer": ".class", "latency_ms": 108.0162525177002, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3262}, "6": {"k": 6, "answer": ".java", "latency_ms": 161.83781623840332, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3883}}}
{"qid": "6860341019198485637", "question": "who cracked the enigma code in world war 2", "gold_answers": ["Turing"], "shots": {"0": {"k": 0, "answer": "Alan Turing", "latency_ms": 97.46336936950684, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Alan Turing", "latency_ms": 81.97903633117676, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 654}, "2": {"k": 2, "answer": "Alan Turing", "latency_ms": 73.48966598510742, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1289}, "3": {"k": 3, "answer": "Alan Turing", "latency_ms": 84.27715301513672, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1899}, "4": {"k": 4, "answer": "Alan Turing", "latency_ms": 155.46894073486328, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2530}, "5": {"k": 5, "answer": "Alan Turing", "latency_ms": 104.70986366271973, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3124}, "6": {"k": 6, "answer": "Alan Turing", "latency_ms": 107.63692855834961, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3756}}}
{"qid": "7990281571644070368", "question": "what is the job of the whip in congress", "gold_answers": ["to ensure party discipline"], "shots": {"0": {"k": 0, "answer": "To ensure party members vote according to party leadership.", "latency_ms": 193.66955757141113, "em": 0.0, "f1": 0.4615384615384615, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "ensure members adhere to party policies and vote as directed", "latency_ms": 254.99677658081055, "em": 0.0, "f1": 0.4285714285714285, "containment": 0.0, "context_chars": 632}, "2": {"k": 2, "answer": "ensure members vote in line with party leadership", "latency_ms": 318.4950351715088, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1276}, "3": {"k": 3, "answer": "Ensure members vote in line with party leadership", "latency_ms": 205.4896354675293, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1926}, "4": {"k": 4, "answer": "ensure members vote in line with party leadership", "latency_ms": 155.19022941589355, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2594}, "5": {"k": 5, "answer": "ensure members vote in line with party leadership", "latency_ms": 158.6172580718994, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3255}, "6": {"k": 6, "answer": "ensure members vote in line with party leadership", "latency_ms": 191.3609504699707, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3850}}}
{"qid": "-3089533754017582743", "question": "when was the last time unc did not make the ncaa tournament", "gold_answers": ["2003"], "shots": {"0": {"k": 0, "answer": "2020", "latency_ms": 320.8789825439453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 169.8322296142578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 662}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 192.72994995117188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1295}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 247.09653854370117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1986}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 308.87913703918457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2628}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 269.09828186035156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3311}, "6": {"k": 6, "answer": "1973", "latency_ms": 267.2576904296875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3866}}}
{"qid": "-3530165048900528552", "question": "which foreign currency option is the​ right but not the​ obligation to buy foreign​ currency", "gold_answers": ["foreign exchange option"], "shots": {"0": {"k": 0, "answer": "Call option", "latency_ms": 121.36721611022949, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "FX option", "latency_ms": 52.187204360961914, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 676}, "2": {"k": 2, "answer": "FX option", "latency_ms": 234.83514785766602, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1364}, "3": {"k": 3, "answer": "FX option", "latency_ms": 356.89353942871094, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2054}, "4": {"k": 4, "answer": "call option", "latency_ms": 132.75456428527832, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2700}, "5": {"k": 5, "answer": "call option", "latency_ms": 205.94000816345215, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3357}, "6": {"k": 6, "answer": "call option", "latency_ms": 99.28488731384277, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 4005}}}
{"qid": "2795932191977151266", "question": "how many house of representative do we have in nigeria", "gold_answers": ["360", "360 members"], "shots": {"0": {"k": 0, "answer": "360", "latency_ms": 63.892364501953125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "360", "latency_ms": 112.01047897338867, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 721}, "2": {"k": 2, "answer": "360", "latency_ms": 81.2983512878418, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1376}, "3": {"k": 3, "answer": "360", "latency_ms": 115.89574813842773, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2070}, "4": {"k": 4, "answer": "360", "latency_ms": 202.0111083984375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2700}, "5": {"k": 5, "answer": "360", "latency_ms": 252.9771327972412, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3342}, "6": {"k": 6, "answer": "360", "latency_ms": 120.3463077545166, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3995}}}
{"qid": "4714895284721516061", "question": "who was the actor who played grizzly adams", "gold_answers": ["`` Dan '' Haggerty"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 93.46795082092285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dan Haggerty", "latency_ms": 281.53443336486816, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 615}, "2": {"k": 2, "answer": "Dan Haggerty", "latency_ms": 386.5830898284912, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1254}, "3": {"k": 3, "answer": "Dan Haggerty", "latency_ms": 233.28924179077148, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1905}, "4": {"k": 4, "answer": "Dan Haggerty", "latency_ms": 316.8964385986328, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2546}, "5": {"k": 5, "answer": "Dan Haggerty", "latency_ms": 155.167818069458, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3183}, "6": {"k": 6, "answer": "Dan Haggerty", "latency_ms": 295.43352127075195, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3835}}}
{"qid": "-6101054679785992133", "question": "who sings blame it on the bossa nova", "gold_answers": ["Eydie Gormé"], "shots": {"0": {"k": 0, "answer": "Bobby Vee", "latency_ms": 176.74016952514648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Eydie Gormé", "latency_ms": 339.0536308288574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 580}, "2": {"k": 2, "answer": "Eydie Gormé", "latency_ms": 233.88123512268066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1156}, "3": {"k": 3, "answer": "Eydie Gormé", "latency_ms": 359.99417304992676, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1757}, "4": {"k": 4, "answer": "Eydie Gormé", "latency_ms": 331.23302459716797, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2334}, "5": {"k": 5, "answer": "Eydie Gormé", "latency_ms": 361.67287826538086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2965}, "6": {"k": 6, "answer": "Eydie Gormé", "latency_ms": 269.14262771606445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3515}}}
{"qid": "1882117704293632224", "question": "who is the supreme court judge was a former chief justice of kerala high court", "gold_answers": ["Konakuppakatil Gopinathan Balakrishnan"], "shots": {"0": {"k": 0, "answer": "Justice D. K. Jain", "latency_ms": 293.170690536499, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Jasti Chelameswar", "latency_ms": 342.5641059875488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 618}, "2": {"k": 2, "answer": "Jasti Chelameswar", "latency_ms": 190.9925937652588, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1229}, "3": {"k": 3, "answer": "Jasti Chelameswar", "latency_ms": 162.3208522796631, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1858}, "4": {"k": 4, "answer": "Jasti Chelameswar", "latency_ms": 283.3378314971924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2481}, "5": {"k": 5, "answer": "Mohan Shantanagoudar", "latency_ms": 397.80282974243164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3093}, "6": {"k": 6, "answer": "Jasti Chelameswar", "latency_ms": 394.99950408935547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3657}}}
{"qid": "2524852627963543698", "question": "who has won more grand slam titles in tennis", "gold_answers": ["Roger Federer"], "shots": {"0": {"k": 0, "answer": "Novak Djokovic", "latency_ms": 217.78059005737305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Roger Federer", "latency_ms": 69.9913501739502, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 635}, "2": {"k": 2, "answer": "Roger Federer", "latency_ms": 120.88847160339355, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1231}, "3": {"k": 3, "answer": "Roger Federer", "latency_ms": 328.85050773620605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1790}, "4": {"k": 4, "answer": "Roger Federer", "latency_ms": 187.47520446777344, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2362}, "5": {"k": 5, "answer": "Roger Federer", "latency_ms": 186.37967109680176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3023}, "6": {"k": 6, "answer": "Roger Federer", "latency_ms": 119.43483352661133, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3597}}}
{"qid": "-6230694167244306035", "question": "how many times has the saints won the super bowl", "gold_answers": ["1"], "shots": {"0": {"k": 0, "answer": "5", "latency_ms": 41.26334190368652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1", "latency_ms": 98.02365303039551, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 644}, "2": {"k": 2, "answer": "1", "latency_ms": 81.67266845703125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1221}, "3": {"k": 3, "answer": "1", "latency_ms": 194.0133571624756, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1834}, "4": {"k": 4, "answer": "1", "latency_ms": 281.85415267944336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2444}, "5": {"k": 5, "answer": "1", "latency_ms": 162.97292709350586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3051}, "6": {"k": 6, "answer": "1", "latency_ms": 227.9677391052246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3682}}}
{"qid": "4186485631454362041", "question": "when was the last time the los angeles lakers won a championship", "gold_answers": ["2010", "in 2010"], "shots": {"0": {"k": 0, "answer": "2020", "latency_ms": 175.23765563964844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2016", "latency_ms": 84.55538749694824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 650}, "2": {"k": 2, "answer": "2002", "latency_ms": 160.3846549987793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "2020", "latency_ms": 184.7841739654541, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1938}, "4": {"k": 4, "answer": "2020", "latency_ms": 275.4654884338379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2619}, "5": {"k": 5, "answer": "2020", "latency_ms": 255.4168701171875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3240}, "6": {"k": 6, "answer": "2020", "latency_ms": 147.5212574005127, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3855}}}
{"qid": "6332740420650663139", "question": "how many gold medals did australia win in the 2000 olympics", "gold_answers": ["16"], "shots": {"0": {"k": 0, "answer": "32", "latency_ms": 71.48456573486328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 269.2229747772217, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 627}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 133.93688201904297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1258}, "3": {"k": 3, "answer": "5", "latency_ms": 105.04436492919922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1909}, "4": {"k": 4, "answer": "5", "latency_ms": 87.77499198913574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2552}, "5": {"k": 5, "answer": "5", "latency_ms": 91.59231185913086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3221}, "6": {"k": 6, "answer": "5", "latency_ms": 232.47575759887695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3896}}}
{"qid": "-2418032234376469176", "question": "where is the eagle creek fire located in oregon", "gold_answers": ["Cascade Locks", "Columbia River Gorge"], "shots": {"0": {"k": 0, "answer": "Wallowa County, Oregon", "latency_ms": 160.70055961608887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Columbia River Gorge", "latency_ms": 130.28836250305176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 624}, "2": {"k": 2, "answer": "Columbia River Gorge", "latency_ms": 293.66064071655273, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1241}, "3": {"k": 3, "answer": "Columbia River Gorge", "latency_ms": 172.33014106750488, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1815}, "4": {"k": 4, "answer": "Columbia River Gorge", "latency_ms": 208.6167335510254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2456}, "5": {"k": 5, "answer": "Columbia River Gorge", "latency_ms": 155.85017204284668, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3082}, "6": {"k": 6, "answer": "Columbia River Gorge", "latency_ms": 257.9507827758789, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3712}}}
{"qid": "-2622854017829301835", "question": "different ways to spell corey for a boy", "gold_answers": ["Kori", "Cory", "Kory", "Coire", "Corrie", "Correy", "Corie", "Corey", "Khouri"], "shots": {"0": {"k": 0, "answer": "Corey, Corry, Kory, Korye, Corbie, Corry, Korye, Corey, Corry, Cory", "latency_ms": 977.9040813446045, "em": 0.0, "f1": 0.18181818181818182, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Corey, Cory", "latency_ms": 261.0471248626709, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 587}, "2": {"k": 2, "answer": "Cory", "latency_ms": 211.84825897216797, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1188}, "3": {"k": 3, "answer": "Cory", "latency_ms": 319.14591789245605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1870}, "4": {"k": 4, "answer": "Corey, Cory", "latency_ms": 217.98419952392578, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2481}, "5": {"k": 5, "answer": "Cory", "latency_ms": 158.19692611694336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3134}, "6": {"k": 6, "answer": "Cory, Corey, Gowdy", "latency_ms": 443.1924819946289, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3755}}}
{"qid": "3480908309420822259", "question": "who sang the song should have been a cowboy", "gold_answers": ["Toby Keith"], "shots": {"0": {"k": 0, "answer": "Linda Ronstadt", "latency_ms": 116.41764640808105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Misty", "latency_ms": 63.5066032409668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 662}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 288.219690322876, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1374}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 158.93793106079102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2002}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 181.1838150024414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2669}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 246.7496395111084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3298}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 282.3619842529297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3901}}}
{"qid": "-4198273171611748168", "question": "how many counties does the state of georgia have", "gold_answers": ["159", "159 counties"], "shots": {"0": {"k": 0, "answer": "159", "latency_ms": 90.18230438232422, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "159", "latency_ms": 164.08705711364746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "159", "latency_ms": 154.96039390563965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1292}, "3": {"k": 3, "answer": "159", "latency_ms": 93.54448318481445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1997}, "4": {"k": 4, "answer": "159", "latency_ms": 329.24413681030273, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2732}, "5": {"k": 5, "answer": "159", "latency_ms": 164.06583786010742, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3379}, "6": {"k": 6, "answer": "159", "latency_ms": 153.08809280395508, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4038}}}
{"qid": "2077711785168359308", "question": "who sang picking up pebbles and throwing them into the sea", "gold_answers": ["Matt Flinders"], "shots": {"0": {"k": 0, "answer": "Tommy Emmanuel", "latency_ms": 64.01586532592773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Edwin McCain", "latency_ms": 143.83339881896973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 581}, "2": {"k": 2, "answer": "Egan", "latency_ms": 156.39615058898926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1193}, "3": {"k": 3, "answer": "Jingwei", "latency_ms": 165.53378105163574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1867}, "4": {"k": 4, "answer": "Jingwei", "latency_ms": 231.24027252197266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2483}, "5": {"k": 5, "answer": "Egan", "latency_ms": 116.95194244384766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3081}, "6": {"k": 6, "answer": "Jingwei", "latency_ms": 131.98304176330566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3737}}}
{"qid": "9062886943819260445", "question": "who made the most free throws in nba history", "gold_answers": ["Karl Malone"], "shots": {"0": {"k": 0, "answer": "Stephen Curry", "latency_ms": 162.5065803527832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Pete Maravich", "latency_ms": 167.3266887664795, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Pete Maravich", "latency_ms": 181.71334266662598, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1285}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 149.92856979370117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1937}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 232.15436935424805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2537}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 236.48524284362793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3150}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 312.18624114990234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3748}}}
{"qid": "1278533364941960992", "question": "which is the ring finger for male in india", "gold_answers": ["the left ring finger", "left ring finger"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 81.17032051086426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "right hand", "latency_ms": 71.34795188903809, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 583}, "2": {"k": 2, "answer": "right hand", "latency_ms": 95.67975997924805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1154}, "3": {"k": 3, "answer": "right hand ring finger", "latency_ms": 113.42406272888184, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1756}, "4": {"k": 4, "answer": "right hand ring finger", "latency_ms": 218.55616569519043, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 2378}, "5": {"k": 5, "answer": "right hand ring finger", "latency_ms": 272.6755142211914, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 2972}, "6": {"k": 6, "answer": "right hand ring finger", "latency_ms": 337.21399307250977, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3526}}}
{"qid": "-8788207958970648395", "question": "what disney cartoon character's middle name is fauntleroy", "gold_answers": ["Donald Fauntleroy Duck", "Donald Duck"], "shots": {"0": {"k": 0, "answer": "Pinocchio", "latency_ms": 112.59984970092773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Donald Duck", "latency_ms": 162.08791732788086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 647}, "2": {"k": 2, "answer": "Donald Duck", "latency_ms": 112.09988594055176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1299}, "3": {"k": 3, "answer": "Donald Duck", "latency_ms": 154.77871894836426, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1938}, "4": {"k": 4, "answer": "Donald Duck", "latency_ms": 120.819091796875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2584}, "5": {"k": 5, "answer": "Donald Duck", "latency_ms": 168.0278778076172, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3205}, "6": {"k": 6, "answer": "Donald Duck", "latency_ms": 324.6879577636719, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3857}}}
{"qid": "-4323000711881657078", "question": "who played king theoden in lord of the rings", "gold_answers": ["Bernard Hill"], "shots": {"0": {"k": 0, "answer": "Waited (no, that's incorrect)  \nCorrect answer: Sean Astin  \nNo, that's incorrect.  \nCorrect answer: John Rhys-Davies  \nNo, that's incorrect.  \nCorrect answer: Ian McKellen  \nNo, that's incorrect.  \nCorrect answer: Sean Astin  \nFinal correct answer: Sean Astin  \nNo, Sean Astin played Faramir.  \nCorrect answer: John Rhys-Davies played Gimli.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian McKellen played Aragorn.  \nCorrect answer: Sir Ian", "latency_ms": 7335.404634475708, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Waited. I cannot answer.", "latency_ms": 277.8584957122803, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 641}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 230.13949394226074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1264}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 196.9609260559082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1910}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 259.2639923095703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2563}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 479.3508052825928, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3192}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 143.23878288269043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3875}}}
{"qid": "-2664158830468622213", "question": "dark parables return of the salt princess release date", "gold_answers": ["September 2017"], "shots": {"0": {"k": 0, "answer": "2023", "latency_ms": 166.43714904785156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "March 15, 2018", "latency_ms": 392.9483890533447, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 573}, "2": {"k": 2, "answer": "March 15, 2018", "latency_ms": 733.8392734527588, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1217}, "3": {"k": 3, "answer": "March 15, 2018", "latency_ms": 304.05116081237793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1847}, "4": {"k": 4, "answer": "March 15, 2018", "latency_ms": 630.2752494812012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2443}, "5": {"k": 5, "answer": "March 15, 2018", "latency_ms": 426.9247055053711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3049}, "6": {"k": 6, "answer": "March 15, 2018", "latency_ms": 474.4234085083008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3691}}}
{"qid": "9078092620239351027", "question": "where was the movie 500 days of summer filmed", "gold_answers": ["Los Angeles"], "shots": {"0": {"k": 0, "answer": "Los Angeles, California", "latency_ms": 97.56112098693848, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "independently", "latency_ms": 85.36076545715332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 618}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 95.01171112060547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1188}, "3": {"k": 3, "answer": "not specified in the context", "latency_ms": 267.49134063720703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1855}, "4": {"k": 4, "answer": "United States", "latency_ms": 187.3342990875244, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2500}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 152.8172492980957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3096}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 161.63158416748047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3706}}}
{"qid": "4747795168824294053", "question": "who played emma in h2o just add water", "gold_answers": ["Claire Rhiannon Holt"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 96.2514877319336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Emma Gilbert", "latency_ms": 215.14415740966797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 618}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 222.49913215637207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1235}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 184.4940185546875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1799}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 230.4089069366455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2435}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 334.5801830291748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3006}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 274.01185035705566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3646}}}
{"qid": "-1368633715963532113", "question": "where can carbon be found in the biosphere", "gold_answers": ["other living organisms", "plants", "soil", "The terrestrial biosphere"], "shots": {"0": {"k": 0, "answer": "atmosphere, oceans, soil, living organisms", "latency_ms": 352.2465229034424, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "in the biosphere", "latency_ms": 251.82175636291504, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 700}, "2": {"k": 2, "answer": "in all land-living organisms, both alive and dead, as well as in soils", "latency_ms": 309.4215393066406, "em": 0.0, "f1": 0.125, "containment": 1.0, "context_chars": 1350}, "3": {"k": 3, "answer": "in all land-living organisms, both alive and dead, and in soils", "latency_ms": 418.3666706085205, "em": 0.0, "f1": 0.14285714285714288, "containment": 1.0, "context_chars": 1964}, "4": {"k": 4, "answer": "in all land-living organisms, both alive and dead, and in soils", "latency_ms": 821.3238716125488, "em": 0.0, "f1": 0.14285714285714288, "containment": 1.0, "context_chars": 2601}, "5": {"k": 5, "answer": "in plants and other living organisms, and in soils", "latency_ms": 394.256591796875, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3308}, "6": {"k": 6, "answer": "in plants and other living organisms, soils, and dead biomass", "latency_ms": 592.6487445831299, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3961}}}
{"qid": "-1173252802989599019", "question": "who is the winner of bigg boss kannada season", "gold_answers": ["rapper Chandan Shetty", "Chandan Shetty"], "shots": {"0": {"k": 0, "answer": "Sneha Ullal", "latency_ms": 402.7268886566162, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Akul Balaji", "latency_ms": 233.86907577514648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "Akul Balaji", "latency_ms": 139.8310661315918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1286}, "3": {"k": 3, "answer": "Akul Balaji", "latency_ms": 225.66747665405273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1899}, "4": {"k": 4, "answer": "Akul Balaji", "latency_ms": 323.8334655761719, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2511}, "5": {"k": 5, "answer": "Akul Balaji", "latency_ms": 174.98278617858887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3154}, "6": {"k": 6, "answer": "Akul Balaji", "latency_ms": 328.7975788116455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3797}}}
{"qid": "-8950878510839541193", "question": "where was the outlaw of josey wales filmed", "gold_answers": ["Utah", "Arizona", "Kanab , Utah", "Lake Powell", "Wyoming", "Oroville , California"], "shots": {"0": {"k": 0, "answer": "Tennessee", "latency_ms": 176.09000205993652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kanab, Utah", "latency_ms": 324.4035243988037, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 621}, "2": {"k": 2, "answer": "Lake Powell, Utah", "latency_ms": 224.63393211364746, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1274}, "3": {"k": 3, "answer": "Lake Powell, Page, Arizona, Kanab, Utah", "latency_ms": 336.2603187561035, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1929}, "4": {"k": 4, "answer": "Utah, Arizona, Wyoming, Oroville, California", "latency_ms": 356.9521903991699, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2574}, "5": {"k": 5, "answer": "Utah, Arizona, Wyoming, and Oroville, California", "latency_ms": 819.8995590209961, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3201}, "6": {"k": 6, "answer": "Utah, Arizona, Wyoming, Oroville, California", "latency_ms": 535.7463359832764, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3819}}}
{"qid": "445108246725869963", "question": "who plays the dad in the new beauty and the beast", "gold_answers": ["Kevin Kline"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 156.00013732910156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kevin Kline", "latency_ms": 112.41745948791504, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 671}, "2": {"k": 2, "answer": "Kevin Kline", "latency_ms": 82.07201957702637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1323}, "3": {"k": 3, "answer": "Kevin Kline", "latency_ms": 181.96868896484375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1957}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 180.6929111480713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2579}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 291.3041114807129, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3241}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 330.9803009033203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3908}}}
{"qid": "-5745040113272333526", "question": "where does the saskatchewan river start and end", "gold_answers": ["the Hudson Bay", "central Saskatchewan", "Saskatchewan River Forks , Saskatchewan", "Saskatchewan Glacier , Alberta"], "shots": {"0": {"k": 0, "answer": "Starts in the Rocky Mountains, ends in the North Dakota-Minnesota border", "latency_ms": 227.33616828918457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Starts: confluence of North and South Saskatchewan Rivers in central Saskatchewan; Ends: Lake Winnipeg", "latency_ms": 464.77603912353516, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 708}, "2": {"k": 2, "answer": "Starts in central Saskatchewan (east of Prince Albert), ends in Lake Winnipeg", "latency_ms": 275.15196800231934, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 1438}, "3": {"k": 3, "answer": "Starts: confluence of North and South Saskatchewan Rivers in central Saskatchewan; Ends: Lake Winnipeg", "latency_ms": 615.5791282653809, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 2136}, "4": {"k": 4, "answer": "Starts at confluence of North and South Saskatchewan Rivers near Saskatchewan River Forks, ends in Lake Winnipeg.", "latency_ms": 522.580623626709, "em": 0.0, "f1": 0.38095238095238093, "containment": 0.0, "context_chars": 2791}, "5": {"k": 5, "answer": "Starts at confluence of North and South Saskatchewan Rivers near Saskatchewan River Forks, ends in Lake Winnipeg.", "latency_ms": 495.1810836791992, "em": 0.0, "f1": 0.38095238095238093, "containment": 0.0, "context_chars": 3435}, "6": {"k": 6, "answer": "Starts at confluence of North and South Saskatchewan Rivers in Saskatchewan River Forks; ends in Lake Winnipeg.", "latency_ms": 653.7408828735352, "em": 0.0, "f1": 0.38095238095238093, "containment": 0.0, "context_chars": 4040}}}
{"qid": "2370076891269438265", "question": "where are the spanish steps located in italy", "gold_answers": ["Rome"], "shots": {"0": {"k": 0, "answer": "Rome", "latency_ms": 87.52036094665527, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Rome, Italy", "latency_ms": 166.75043106079102, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 611}, "2": {"k": 2, "answer": "Rome, Italy", "latency_ms": 163.15436363220215, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1243}, "3": {"k": 3, "answer": "Rome, Italy", "latency_ms": 162.54115104675293, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1874}, "4": {"k": 4, "answer": "Rome, Italy", "latency_ms": 238.0075454711914, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2480}, "5": {"k": 5, "answer": "Rome, Italy", "latency_ms": 373.42381477355957, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3108}, "6": {"k": 6, "answer": "Rome, Italy", "latency_ms": 276.1554718017578, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3743}}}
{"qid": "-1701212376306091221", "question": "what does the m number mean on a pint glass", "gold_answers": ["imperial fluid ounces", "the manufacturing company or site"], "shots": {"0": {"k": 0, "answer": "M number refers to the volume of liquid in milliliters.", "latency_ms": 339.9510383605957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "manufacturing company or site", "latency_ms": 124.33218955993652, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "manufacturing company or site", "latency_ms": 282.60040283203125, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1255}, "3": {"k": 3, "answer": "manufacturing company or site", "latency_ms": 213.8671875, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1862}, "4": {"k": 4, "answer": "manufacturing company or site", "latency_ms": 230.46278953552246, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2406}, "5": {"k": 5, "answer": "manufacturing company or site", "latency_ms": 451.02834701538086, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2980}, "6": {"k": 6, "answer": "manufacturing company or site", "latency_ms": 166.9464111328125, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3601}}}
{"qid": "3631867991857999976", "question": "other than water what else has hydrogen bonds", "gold_answers": ["inorganic molecules such as water"], "shots": {"0": {"k": 0, "answer": "ammonia, alcohols, carboxylic acids", "latency_ms": 452.41570472717285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 178.5562038421631, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 144.12188529968262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1340}, "3": {"k": 3, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 160.66265106201172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1986}, "4": {"k": 4, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 164.01982307434082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2596}, "5": {"k": 5, "answer": "hydrogen fluoride, ammonia, methanol, DNA, proteins, polymers", "latency_ms": 787.7697944641113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3200}, "6": {"k": 6, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 557.7561855316162, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3885}}}
{"qid": "-1665528830781573810", "question": "when boy meets girl waiting for a star to fall", "gold_answers": ["1988"], "shots": {"0": {"k": 0, "answer": "1988", "latency_ms": 212.21518516540527, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1988", "latency_ms": 96.07672691345215, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 584}, "2": {"k": 2, "answer": "1988", "latency_ms": 189.2554759979248, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1196}, "3": {"k": 3, "answer": "1988", "latency_ms": 147.81618118286133, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1799}, "4": {"k": 4, "answer": "1988", "latency_ms": 238.01779747009277, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2390}, "5": {"k": 5, "answer": "1988", "latency_ms": 280.54356575012207, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3000}, "6": {"k": 6, "answer": "1988", "latency_ms": 164.0632152557373, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3619}}}
{"qid": "4289442278067074619", "question": "trick taking card game name derived from spanish for man", "gold_answers": ["Skat"], "shots": {"0": {"k": 0, "answer": "Trick-taking card game name derived from Spanish for man: Ombre", "latency_ms": 610.710620880127, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Manille", "latency_ms": 219.35153007507324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "Manille", "latency_ms": 134.75561141967773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1193}, "3": {"k": 3, "answer": "Manille", "latency_ms": 102.68259048461914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1792}, "4": {"k": 4, "answer": "Manille", "latency_ms": 125.4887580871582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2373}, "5": {"k": 5, "answer": "Manille", "latency_ms": 147.48549461364746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2944}, "6": {"k": 6, "answer": "Manille", "latency_ms": 157.8214168548584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3530}}}
{"qid": "-7311149600272472027", "question": "what percentage of the population is naturally blonde", "gold_answers": ["2 %"], "shots": {"0": {"k": 0, "answer": "2%", "latency_ms": 96.3740348815918, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "5%", "latency_ms": 76.6761302947998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "5%", "latency_ms": 101.79615020751953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1239}, "3": {"k": 3, "answer": "2% to 16%", "latency_ms": 141.65568351745605, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1845}, "4": {"k": 4, "answer": "2% to 16%", "latency_ms": 150.81000328063965, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2458}, "5": {"k": 5, "answer": "5%", "latency_ms": 169.8288917541504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3087}, "6": {"k": 6, "answer": "5%", "latency_ms": 184.13138389587402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3775}}}
{"qid": "7110705857253017304", "question": "when was the last time miss texas won miss america", "gold_answers": ["1975"], "shots": {"0": {"k": 0, "answer": "2018", "latency_ms": 118.03984642028809, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1975", "latency_ms": 262.7835273742676, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 610}, "2": {"k": 2, "answer": "1975", "latency_ms": 357.1593761444092, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1185}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 162.34230995178223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1763}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 301.7082214355469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2388}, "5": {"k": 5, "answer": "1975", "latency_ms": 183.89248847961426, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3008}, "6": {"k": 6, "answer": "1975", "latency_ms": 331.9413661956787, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3593}}}
{"qid": "-3653614166480549899", "question": "who sang first line of we are the world", "gold_answers": ["Billy Joel", "Stevie Wonder", "Lionel Richie", "James Ingram", "Tina Turner", "Paul Simon", "Kenny Rogers"], "shots": {"0": {"k": 0, "answer": "Michael Jackson", "latency_ms": 92.16833114624023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 193.08948516845703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 582}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 97.59235382080078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1199}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 243.56412887573242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1806}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 152.22454071044922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2421}, "5": {"k": 5, "answer": "Paul Young", "latency_ms": 201.19833946228027, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3068}, "6": {"k": 6, "answer": "Paul Young", "latency_ms": 258.2077980041504, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3662}}}
{"qid": "-6284503594240226071", "question": "who formed and first came to the colony of maryland", "gold_answers": ["the English"], "shots": {"0": {"k": 0, "answer": "Lord Baltimore", "latency_ms": 104.01010513305664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "George Calvert", "latency_ms": 117.22731590270996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "George Calvert", "latency_ms": 99.55263137817383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1281}, "3": {"k": 3, "answer": "George Calvert", "latency_ms": 140.00535011291504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1915}, "4": {"k": 4, "answer": "George Calvert", "latency_ms": 226.97186470031738, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2629}, "5": {"k": 5, "answer": "Cecil Calvert", "latency_ms": 200.80852508544922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3323}, "6": {"k": 6, "answer": "Cecil Calvert", "latency_ms": 144.8347568511963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3953}}}
{"qid": "6948592513354589991", "question": "who has played in the most masters tournaments", "gold_answers": ["Gary Player"], "shots": {"0": {"k": 0, "answer": "Tiger Woods", "latency_ms": 240.5703067779541, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Tiger Woods", "latency_ms": 77.26097106933594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 619}, "2": {"k": 2, "answer": "Tiger Woods", "latency_ms": 285.0801944732666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1252}, "3": {"k": 3, "answer": "Tiger Woods", "latency_ms": 129.72450256347656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1877}, "4": {"k": 4, "answer": "Tiger Woods", "latency_ms": 98.03891181945801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2531}, "5": {"k": 5, "answer": "Tiger Woods", "latency_ms": 189.11314010620117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3115}, "6": {"k": 6, "answer": "Tiger Woods", "latency_ms": 168.257474899292, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3719}}}
{"qid": "3311962143974666464", "question": "how much venom can a king cobra produce", "gold_answers": ["200 to 500 mg", "420 mg", "up to 7 ml"], "shots": {"0": {"k": 0, "answer": "100 milligrams", "latency_ms": 178.20501327514648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1.31 mg/kg (intravenous) to 1.93 mg/kg (subcutaneous)", "latency_ms": 572.7274417877197, "em": 0.0, "f1": 0.18181818181818182, "containment": 0.0, "context_chars": 618}, "2": {"k": 2, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 434.53478813171387, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1240}, "3": {"k": 3, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 446.11620903015137, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1889}, "4": {"k": 4, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 564.6629333496094, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2579}, "5": {"k": 5, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 335.57605743408203, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3228}, "6": {"k": 6, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 552.9696941375732, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3897}}}
{"qid": "4829683075369336935", "question": "last team to win fa cup outside top flight", "gold_answers": ["Arsenal", "West Ham United"], "shots": {"0": {"k": 0, "answer": "Barnsley 2003", "latency_ms": 147.6602554321289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "West Ham United", "latency_ms": 75.97899436950684, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 586}, "2": {"k": 2, "answer": "West Ham United", "latency_ms": 83.40764045715332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1225}, "3": {"k": 3, "answer": "West Ham United", "latency_ms": 94.96784210205078, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1805}, "4": {"k": 4, "answer": "West Ham United", "latency_ms": 194.64826583862305, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2397}, "5": {"k": 5, "answer": "West Ham United", "latency_ms": 109.87997055053711, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2916}, "6": {"k": 6, "answer": "West Ham United", "latency_ms": 167.3564910888672, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3533}}}
{"qid": "947113025953164512", "question": "where did you go to drink during prohibition", "gold_answers": ["Speakeasies"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 356.4128875732422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 76.27630233764648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 619}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 92.67878532409668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1271}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 121.34313583374023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1913}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 133.8491439819336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2565}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 251.7244815826416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3259}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 259.71412658691406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3883}}}
{"qid": "2162587275527723755", "question": "where does the edinburgh fringe festival take place", "gold_answers": ["in Edinburgh , Scotland"], "shots": {"0": {"k": 0, "answer": "Edinburgh, Scotland", "latency_ms": 120.0556755065918, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Edinburgh, Scotland", "latency_ms": 271.81005477905273, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 694}, "2": {"k": 2, "answer": "Edinburgh, Scotland", "latency_ms": 421.6480255126953, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1372}, "3": {"k": 3, "answer": "Edinburgh, Scotland", "latency_ms": 169.36302185058594, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2068}, "4": {"k": 4, "answer": "Edinburgh, Scotland", "latency_ms": 142.75503158569336, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2704}, "5": {"k": 5, "answer": "Edinburgh, Scotland", "latency_ms": 385.8201503753662, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3333}, "6": {"k": 6, "answer": "Edinburgh, Scotland", "latency_ms": 247.06673622131348, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3984}}}
{"qid": "1875157261182375834", "question": "who won the wwe championship elimination chamber 2018", "gold_answers": ["Roman Reigns"], "shots": {"0": {"k": 0, "answer": "Braun Strowman", "latency_ms": 201.3843059539795, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 212.24713325500488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 675}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 336.72380447387695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": "Roman Reigns", "latency_ms": 322.45421409606934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1981}, "4": {"k": 4, "answer": "Bray Wyatt", "latency_ms": 253.80492210388184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2609}, "5": {"k": 5, "answer": "Bray Wyatt", "latency_ms": 305.4697513580322, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3226}, "6": {"k": 6, "answer": "Bray Wyatt", "latency_ms": 334.21945571899414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3897}}}
{"qid": "8926902974073457645", "question": "where does safe haven take place in the book", "gold_answers": ["Southport , North Carolina"], "shots": {"0": {"k": 0, "answer": "Florida", "latency_ms": 95.48115730285645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Southport, North Carolina", "latency_ms": 144.27423477172852, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 640}, "2": {"k": 2, "answer": "Southport, North Carolina", "latency_ms": 122.08056449890137, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1243}, "3": {"k": 3, "answer": "Southport, North Carolina", "latency_ms": 410.10165214538574, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1848}, "4": {"k": 4, "answer": "Southport, North Carolina", "latency_ms": 162.3835563659668, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2488}, "5": {"k": 5, "answer": "Southport, North Carolina", "latency_ms": 291.92399978637695, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3125}, "6": {"k": 6, "answer": "Southport, North Carolina", "latency_ms": 237.86616325378418, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3759}}}
{"qid": "8445703091969850957", "question": "who came up with the term baby boomer", "gold_answers": ["The Washington Post"], "shots": {"0": {"k": 0, "answer": "David Owen", "latency_ms": 69.84281539916992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Landon Jones", "latency_ms": 74.93925094604492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "Sylvia F. Porter", "latency_ms": 186.39492988586426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1249}, "3": {"k": 3, "answer": "Sylvia F. Porter", "latency_ms": 170.94993591308594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1877}, "4": {"k": 4, "answer": "Sylvia F. Porter", "latency_ms": 216.46809577941895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2527}, "5": {"k": 5, "answer": "Sylvia F. Porter", "latency_ms": 152.909517288208, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3171}, "6": {"k": 6, "answer": "Sylvia F. Porter", "latency_ms": 350.09264945983887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3846}}}
{"qid": "-2961860751699827965", "question": "who was elected president of the united states in 1928", "gold_answers": ["Herbert Hoover"], "shots": {"0": {"k": 0, "answer": "Herbert Hoover", "latency_ms": 235.70632934570312, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Herbert Hoover", "latency_ms": 160.3248119354248, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 714}, "2": {"k": 2, "answer": "Herbert Hoover", "latency_ms": 143.41139793395996, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1414}, "3": {"k": 3, "answer": "Herbert Hoover", "latency_ms": 162.7941131591797, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2151}, "4": {"k": 4, "answer": "Herbert Hoover", "latency_ms": 362.80250549316406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2897}, "5": {"k": 5, "answer": "Herbert Hoover", "latency_ms": 261.28554344177246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3534}, "6": {"k": 6, "answer": "Herbert Hoover", "latency_ms": 373.59023094177246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4176}}}
{"qid": "-3997272399784107764", "question": "dogs name in the grinch who stole christmas", "gold_answers": ["Max"], "shots": {"0": {"k": 0, "answer": "Max", "latency_ms": 94.23041343688965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Max", "latency_ms": 54.7642707824707, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 619}, "2": {"k": 2, "answer": "Max", "latency_ms": 58.585405349731445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1247}, "3": {"k": 3, "answer": "Max", "latency_ms": 104.23111915588379, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1848}, "4": {"k": 4, "answer": "Max", "latency_ms": 83.27221870422363, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2452}, "5": {"k": 5, "answer": "Max", "latency_ms": 119.09842491149902, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3097}, "6": {"k": 6, "answer": "Max", "latency_ms": 155.0464630126953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3796}}}
{"qid": "119543419987074002", "question": "who played anne in anne with an e", "gold_answers": ["Amybeth McNulty"], "shots": {"0": {"k": 0, "answer": "Sasha Lane", "latency_ms": 273.09155464172363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Geneviève Bujold", "latency_ms": 489.0637397766113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 188.44223022460938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1315}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 147.8099822998047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1972}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 165.34709930419922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2576}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 182.04355239868164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3198}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 223.30451011657715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3790}}}
{"qid": "4362450141225960365", "question": "what was going on in the world in 1900", "gold_answers": ["Boxer Rebellion", "U.S. population exceeds 75 million", "Galveston hurricane"], "shots": {"0": {"k": 0, "answer": "End of the 19th century, industrialization, imperialism, and the rise of new technologies.", "latency_ms": 757.2798728942871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Women beginning to serve in limited military roles in some countries", "latency_ms": 171.29087448120117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Paris 1900 chess tournament, Exposition Universelle", "latency_ms": 347.38779067993164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1308}, "3": {"k": 3, "answer": "Paris 1900 chess tournament, World War I not yet started, World War II not yet started", "latency_ms": 525.7081985473633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1918}, "4": {"k": 4, "answer": "World War I, Paris 1900 chess tournament, women beginning military roles, U.S. Navy transitioning to steel battleships", "latency_ms": 736.4697456359863, "em": 0.0, "f1": 0.0909090909090909, "containment": 0.0, "context_chars": 2579}, "5": {"k": 5, "answer": "World War I, Boxer Rebellion, Spanish–American War, Exposition Universelle", "latency_ms": 585.0653648376465, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 3271}, "6": {"k": 6, "answer": "World War I, Spanish–American War, Boxer Rebellion, Exposition Universelle, rise of steel hulls in navy, women beginning limited military roles", "latency_ms": 559.6516132354736, "em": 0.0, "f1": 0.18181818181818182, "containment": 1.0, "context_chars": 3863}}}
{"qid": "-599106694350296477", "question": "theme of the song roar by katy perry", "gold_answers": ["standing up for oneself", "self - empowerment"], "shots": {"0": {"k": 0, "answer": "Self-empowerment and confidence", "latency_ms": 197.95727729797363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "empowerment", "latency_ms": 127.70962715148926, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 670}, "2": {"k": 2, "answer": "standing up for oneself and self-empowerment", "latency_ms": 720.8294868469238, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1283}, "3": {"k": 3, "answer": "self-empowerment", "latency_ms": 135.44797897338867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1905}, "4": {"k": 4, "answer": "self-empowerment", "latency_ms": 279.7203063964844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2587}, "5": {"k": 5, "answer": "self-empowerment", "latency_ms": 390.29383659362793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3280}, "6": {"k": 6, "answer": "self-empowerment", "latency_ms": 272.7351188659668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3951}}}
{"qid": "6341281025332348318", "question": "who picks the players in the nfl draft", "gold_answers": ["each team"], "shots": {"0": {"k": 0, "answer": "NFL teams", "latency_ms": 51.94282531738281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "NFL franchises", "latency_ms": 56.2746524810791, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 627}, "2": {"k": 2, "answer": "NFL franchises", "latency_ms": 69.43035125732422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1267}, "3": {"k": 3, "answer": "NFL franchises", "latency_ms": 96.80867195129395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1901}, "4": {"k": 4, "answer": "NFL franchises", "latency_ms": 155.96532821655273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2513}, "5": {"k": 5, "answer": "NFL franchises", "latency_ms": 177.51002311706543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3091}, "6": {"k": 6, "answer": "NFL franchises", "latency_ms": 141.11876487731934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3722}}}
{"qid": "1656636013570720778", "question": "who discovered gold in the sierra nevada of california", "gold_answers": ["James W. Marshall"], "shots": {"0": {"k": 0, "answer": "James W. Marshall", "latency_ms": 270.9212303161621, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "James Marshall", "latency_ms": 316.1036968231201, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 626}, "2": {"k": 2, "answer": "James Marshall", "latency_ms": 73.4560489654541, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1282}, "3": {"k": 3, "answer": "James Marshall", "latency_ms": 134.25898551940918, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1928}, "4": {"k": 4, "answer": "James Marshall", "latency_ms": 312.91723251342773, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2546}, "5": {"k": 5, "answer": "James Marshall", "latency_ms": 275.388240814209, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3220}, "6": {"k": 6, "answer": "James W. Marshall", "latency_ms": 157.81641006469727, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3781}}}
{"qid": "6797662554091549033", "question": "which mirror is used in vehicles for rear view", "gold_answers": ["rear - view mirror"], "shots": {"0": {"k": 0, "answer": "Convex mirror", "latency_ms": 150.11310577392578, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "rear-view mirror", "latency_ms": 267.122745513916, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "rear-view mirror", "latency_ms": 99.75910186767578, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1238}, "3": {"k": 3, "answer": "rear-view mirror", "latency_ms": 175.36020278930664, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1859}, "4": {"k": 4, "answer": "rear-view mirror", "latency_ms": 126.59001350402832, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2505}, "5": {"k": 5, "answer": "rear-view mirror", "latency_ms": 194.79608535766602, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3136}, "6": {"k": 6, "answer": "rear-view mirror", "latency_ms": 174.11017417907715, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3800}}}
{"qid": "7788386234127027369", "question": "how many medals did austria win in the 2011 alpine skiing world championships", "gold_answers": ["8"], "shots": {"0": {"k": 0, "answer": "1 medal", "latency_ms": 147.186279296875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 88.09828758239746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 183.22324752807617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1242}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 195.0700283050537, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1873}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 164.9324893951416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2489}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 257.0366859436035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3139}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 182.21282958984375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3745}}}
{"qid": "647605647914971565", "question": "who wrote lyrics for phantom of the opera", "gold_answers": ["Richard Stilgoe", "Charles Hart and Richard Stilgoe", "Charles Hart"], "shots": {"0": {"k": 0, "answer": "Charles Hart", "latency_ms": 95.54815292358398, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Charles Hart", "latency_ms": 203.95970344543457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 608}, "2": {"k": 2, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 378.8270950317383, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1227}, "3": {"k": 3, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 252.56848335266113, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1863}, "4": {"k": 4, "answer": "Charles Hart", "latency_ms": 150.90060234069824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2497}, "5": {"k": 5, "answer": "Charles Hart", "latency_ms": 190.65213203430176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3161}, "6": {"k": 6, "answer": "Charles Hart", "latency_ms": 275.53749084472656, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3836}}}
{"qid": "-7980433566693172177", "question": "what culture region is germany a part of", "gold_answers": ["Central Germany"], "shots": {"0": {"k": 0, "answer": "Central Europe", "latency_ms": 55.477142333984375, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Northern Europe", "latency_ms": 151.9784927368164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 643}, "2": {"k": 2, "answer": "Central Germany", "latency_ms": 64.42737579345703, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1291}, "3": {"k": 3, "answer": "Central Germany", "latency_ms": 168.5044765472412, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1937}, "4": {"k": 4, "answer": "Central Germany (cultural area)", "latency_ms": 158.79583358764648, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2648}, "5": {"k": 5, "answer": "Central Germany (cultural area)", "latency_ms": 179.18038368225098, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3406}, "6": {"k": 6, "answer": "Northern Germany", "latency_ms": 218.54662895202637, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 4098}}}
{"qid": "6268706985209815767", "question": "mount and blade with fire and sword time period", "gold_answers": ["1648 - 51"], "shots": {"0": {"k": 0, "answer": "13th century", "latency_ms": 196.75230979919434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "post-medieval era", "latency_ms": 360.17918586730957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "1648–1651", "latency_ms": 575.0360488891602, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1301}, "3": {"k": 3, "answer": "1648–1651", "latency_ms": 206.96616172790527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1978}, "4": {"k": 4, "answer": "1648–1651", "latency_ms": 273.529052734375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2648}, "5": {"k": 5, "answer": "1648–1651", "latency_ms": 437.7636909484863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3409}, "6": {"k": 6, "answer": "later, more modern period than earlier titles", "latency_ms": 570.6167221069336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4087}}}
