{"qid": "175824256524792922", "question": "who plays harry's mom in harry potter", "gold_answers": ["Geraldine Margaret Agnew - Somerville"], "shots": {"0": {"k": 0, "answer": "Rita Skeeter", "latency_ms": 336.87901496887207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 334.0795040130615, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 607}, "2": {"k": 2, "answer": "Molly Weasley", "latency_ms": 196.5177059173584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1248}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 250.0452995300293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1894}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 196.87604904174805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2511}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 315.47093391418457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3129}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 125.33020973205566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3760}}}
{"qid": "-8223571798173928629", "question": "who sang you'll be a woman soon in pulp fiction", "gold_answers": ["Urge Overkill", "rock band Urge Overkill"], "shots": {"0": {"k": 0, "answer": "Minnie Riperton", "latency_ms": 169.6608066558838, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Urge Overkill", "latency_ms": 118.27874183654785, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 653}, "2": {"k": 2, "answer": "Urge Overkill", "latency_ms": 254.9881935119629, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1277}, "3": {"k": 3, "answer": "Urge Overkill", "latency_ms": 232.85460472106934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1946}, "4": {"k": 4, "answer": "Urge Overkill", "latency_ms": 345.5824851989746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2583}, "5": {"k": 5, "answer": "Urge Overkill", "latency_ms": 318.96257400512695, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3176}, "6": {"k": 6, "answer": "Urge Overkill", "latency_ms": 341.3057327270508, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3781}}}
{"qid": "-8095180930653230478", "question": "where does a roadrunner live in the desert", "gold_answers": ["arid lowland", "mountainous shrubland"], "shots": {"0": {"k": 0, "answer": "Southwestern United States and Mexico", "latency_ms": 500.4246234893799, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 232.81431198120117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 684}, "2": {"k": 2, "answer": "deserts of the southwestern United States, Mexico, and Central America", "latency_ms": 204.11300659179688, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1288}, "3": {"k": 3, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 373.9781379699707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1951}, "4": {"k": 4, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 519.106388092041, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2601}, "5": {"k": 5, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 379.2290687561035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3274}, "6": {"k": 6, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 630.1634311676025, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3870}}}
{"qid": "-5489426796364143729", "question": "who was the first president of the constituent assembly", "gold_answers": ["Dr. Rajendra Prasad"], "shots": {"0": {"k": 0, "answer": "Dr. Rajendra Prasad", "latency_ms": 99.67446327209473, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Karl Seitz", "latency_ms": 132.7047348022461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 683}, "2": {"k": 2, "answer": "Dr. Rajendra Prasad", "latency_ms": 246.78468704223633, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1428}, "3": {"k": 3, "answer": "Dr. Rajendra Prasad", "latency_ms": 517.296552658081, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2127}, "4": {"k": 4, "answer": "Dr. Sachchidananda Sinha", "latency_ms": 467.3588275909424, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2827}, "5": {"k": 5, "answer": "Dr. Sachchidananda Sinha", "latency_ms": 538.5968685150146, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3522}, "6": {"k": 6, "answer": "Dr. Sachchidananda Sinha", "latency_ms": 422.8789806365967, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 4236}}}
{"qid": "7835345385704208729", "question": "when does the new episode of scorpion come on", "gold_answers": ["January 15 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 269.87338066101074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 82.94439315795898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 94.17176246643066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1312}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 224.47776794433594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2002}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 181.0750961303711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2567}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 199.3546485900879, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3185}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 150.15888214111328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3803}}}
{"qid": "907238223200158003", "question": "when did first fast and furious come out", "gold_answers": ["2001"], "shots": {"0": {"k": 0, "answer": "2001", "latency_ms": 146.0871696472168, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "2001", "latency_ms": 201.29799842834473, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 640}, "2": {"k": 2, "answer": "2001", "latency_ms": 214.53404426574707, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1249}, "3": {"k": 3, "answer": "2001", "latency_ms": 177.61778831481934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1881}, "4": {"k": 4, "answer": "2001", "latency_ms": 324.05924797058105, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2514}, "5": {"k": 5, "answer": "2001", "latency_ms": 251.47533416748047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3112}, "6": {"k": 6, "answer": "2001", "latency_ms": 191.82205200195312, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3738}}}
{"qid": "-6782664238831495856", "question": "when does season 5 of the blacklist resume", "gold_answers": ["January 31 , 2018"], "shots": {"0": {"k": 0, "answer": "2018", "latency_ms": 142.00305938720703, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wednesday at 8:00 PM", "latency_ms": 503.4482479095459, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 317.3036575317383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1361}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 101.86004638671875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 266.4821147918701, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2653}, "5": {"k": 5, "answer": "September 27, 2017", "latency_ms": 283.00952911376953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3286}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 420.28117179870605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4033}}}
{"qid": "3492797766982308854", "question": "when do mr schuester and emma get together", "gold_answers": ["the fourth season"], "shots": {"0": {"k": 0, "answer": "Season 4, Episode 10 \"The First Time\"", "latency_ms": 280.90548515319824, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 226.3772487640381, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 614}, "2": {"k": 2, "answer": "the episode \"Yes/No\"", "latency_ms": 232.84173011779785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1186}, "3": {"k": 3, "answer": "after the episode \"Yes/No\"", "latency_ms": 187.8662109375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1775}, "4": {"k": 4, "answer": "\"Hell-O\"", "latency_ms": 296.6783046722412, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2338}, "5": {"k": 5, "answer": "\"Hell-O\"", "latency_ms": 126.81365013122559, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2957}, "6": {"k": 6, "answer": "\"Will and Emma try to start a relationship, but after he finds out that she's a virgin, they take a break.\"", "latency_ms": 918.4858798980713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3580}}}
{"qid": "-2654989306076235475", "question": "when does the next episode of izombie air", "gold_answers": ["May 14 , 2018", "May 7 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 215.46649932861328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 176.41019821166992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 596}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 121.87647819519043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1192}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 199.09071922302246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1856}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 127.6707649230957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2524}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 496.4785575866699, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3136}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 425.9624481201172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3782}}}
{"qid": "3066393401707107613", "question": "where was the u.s.s maine when it exploded in 1898", "gold_answers": ["Havana Harbor"], "shots": {"0": {"k": 0, "answer": "Havana, Cuba", "latency_ms": 82.12065696716309, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Havana harbor", "latency_ms": 273.23222160339355, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "Havana, Cuba", "latency_ms": 112.9002571105957, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1218}, "3": {"k": 3, "answer": "Havana Harbor", "latency_ms": 189.6345615386963, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1790}, "4": {"k": 4, "answer": "Havana Harbor", "latency_ms": 205.26742935180664, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2364}, "5": {"k": 5, "answer": "Havana Harbor", "latency_ms": 419.6298122406006, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2991}, "6": {"k": 6, "answer": "Havana Harbor", "latency_ms": 351.26543045043945, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3625}}}
{"qid": "6972056626325669866", "question": "in the honour of which god is anant chaturdashi celebrated", "gold_answers": ["Ganesh"], "shots": {"0": {"k": 0, "answer": "Shiva", "latency_ms": 47.531843185424805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 106.71377182006836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 249.32074546813965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1272}, "3": {"k": 3, "answer": "Anant Chaturdashi is not specifically celebrated in honour of a single god. It is a festival observed by both Jains and Hindus, with significance in the Jain calendar, particularly as the last day of Paryushana. While it is associated with Jain traditions, it is not primarily dedicated to a specific deity like Vaikuntha Chaturdashi is to Lord Vishnu or Shiva. Therefore, the correct answer based on the context is:\n\nI cannot answer.", "latency_ms": 2975.0304222106934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1935}, "4": {"k": 4, "answer": "Anant Chaturdashi is not specifically celebrated in honour of a particular god. It is a festival observed by both Jains and Hindus, with significance in the Jain calendar as the last day of Paryushana. While it is associated with spiritual practices like Kshamavani (forgiveness), it does not primarily honour a specific deity. \n\nI cannot answer.", "latency_ms": 2087.6901149749756, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2576}, "5": {"k": 5, "answer": "Anant Chaturdashi is not celebrated in honour of a specific god. It is a festival observed by both Jains and Hindus, with significance in the Jain calendar, particularly as the last day of Paryushana. While it is associated with Anant, a name linked to Lord Vishnu in some traditions, the context does not clearly state that it is celebrated in honour of a specific deity. The primary focus is on Jain observances, not a specific god's honour.\n\nI cannot answer.", "latency_ms": 3426.697254180908, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3149}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 324.3379592895508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3740}}}
{"qid": "6043389452532567502", "question": "which central american nations border the pacific ocean and the caribbean sea", "gold_answers": ["Costa Rica", "Nicaragua"], "shots": {"0": {"k": 0, "answer": "Belize, Guatemala, Honduras, Nicaragua, Costa Rica", "latency_ms": 437.8342628479004, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Guatemala, Honduras, Nicaragua, Costa Rica", "latency_ms": 176.13530158996582, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 658}, "2": {"k": 2, "answer": "Belize, Guatemala, Honduras, Nicaragua, Costa Rica", "latency_ms": 277.45556831359863, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1285}, "3": {"k": 3, "answer": "Belize, Guatemala, Honduras, El Salvador, Nicaragua, Costa Rica", "latency_ms": 812.6139640808105, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1944}, "4": {"k": 4, "answer": "Belize, Guatemala, Honduras, El Salvador, Nicaragua, Costa Rica, Panama", "latency_ms": 477.6170253753662, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 2591}, "5": {"k": 5, "answer": "Belize, Guatemala, Honduras, El Salvador, Nicaragua, Costa Rica, Panama", "latency_ms": 772.4020481109619, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 3186}, "6": {"k": 6, "answer": "Belize, Guatemala, Honduras, El Salvador, Nicaragua, Costa Rica, Panama", "latency_ms": 532.8285694122314, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 3883}}}
{"qid": "2541316470469570888", "question": "what kind of sentence contains an independent clause and a dependent clause", "gold_answers": ["A complex sentence", "complex sentence"], "shots": {"0": {"k": 0, "answer": "complex sentence", "latency_ms": 177.2592067718506, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "complex sentence", "latency_ms": 98.38247299194336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 704}, "2": {"k": 2, "answer": "complex sentence", "latency_ms": 329.1914463043213, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1360}, "3": {"k": 3, "answer": "complex sentence", "latency_ms": 318.41087341308594, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2042}, "4": {"k": 4, "answer": "complex sentence", "latency_ms": 80.08050918579102, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2701}, "5": {"k": 5, "answer": "complex sentence", "latency_ms": 159.26575660705566, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3365}, "6": {"k": 6, "answer": "complex sentence", "latency_ms": 142.37570762634277, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4053}}}
{"qid": "3239205946903665891", "question": "what was the book wealth of nations about", "gold_answers": ["what builds nations ' wealth"], "shots": {"0": {"k": 0, "answer": "economic systems and trade principles", "latency_ms": 140.68293571472168, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "The division of labour, productivity, and free markets", "latency_ms": 330.0950527191162, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "The division of labour, productivity, and free markets.", "latency_ms": 544.6920394897461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1265}, "3": {"k": 3, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of the wealth of nations, including topics like the division of labour, productivity, and free markets.", "latency_ms": 1135.45560836792, "em": 0.0, "f1": 0.14814814814814817, "containment": 0.0, "context_chars": 1912}, "4": {"k": 4, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of the wealth of nations, including topics like the division of labour, productivity, and free markets.", "latency_ms": 899.5375633239746, "em": 0.0, "f1": 0.14814814814814817, "containment": 0.0, "context_chars": 2541}, "5": {"k": 5, "answer": "The Wealth of Nations was about the nature and causes of the wealth of nations, including topics like the division of labour, productivity, and free markets.", "latency_ms": 758.4664821624756, "em": 0.0, "f1": 0.15384615384615385, "containment": 0.0, "context_chars": 3218}, "6": {"k": 6, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of the wealth of nations, including topics such as the division of labour, productivity, and free markets.", "latency_ms": 1015.2487754821777, "em": 0.0, "f1": 0.14285714285714285, "containment": 0.0, "context_chars": 3881}}}
{"qid": "-4659426898177460954", "question": "when will the flash season 4 be released", "gold_answers": ["October 10 , 2017"], "shots": {"0": {"k": 0, "answer": "2024", "latency_ms": 79.14543151855469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "August 28, 2018", "latency_ms": 508.07976722717285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 613}, "2": {"k": 2, "answer": "August 28, 2018", "latency_ms": 369.60673332214355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1205}, "3": {"k": 3, "answer": "August 28, 2018", "latency_ms": 342.81063079833984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1844}, "4": {"k": 4, "answer": "August 28, 2018", "latency_ms": 256.8321228027344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2491}, "5": {"k": 5, "answer": "August 28, 2018", "latency_ms": 455.4281234741211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3106}, "6": {"k": 6, "answer": "August 28, 2018", "latency_ms": 569.0248012542725, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3763}}}
{"qid": "-1204722541964024977", "question": "what is australia's location in the world and region", "gold_answers": ["Oceania"], "shots": {"0": {"k": 0, "answer": "Southern Hemisphere, Eastern Hemisphere, Oceania region", "latency_ms": 479.01105880737305, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Oceania", "latency_ms": 93.18113327026367, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 742}, "2": {"k": 2, "answer": "Oceania", "latency_ms": 160.475492477417, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1408}, "3": {"k": 3, "answer": "Oceania", "latency_ms": 120.24140357971191, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2036}, "4": {"k": 4, "answer": "Oceania", "latency_ms": 162.54758834838867, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2664}, "5": {"k": 5, "answer": "Oceania", "latency_ms": 169.64030265808105, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3339}, "6": {"k": 6, "answer": "Southern Hemisphere, Eastern Hemisphere, Oceania", "latency_ms": 232.84602165222168, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3998}}}
{"qid": "-6291722830448463311", "question": "who does luke skywalker fall in love with", "gold_answers": ["Mara Jade"], "shots": {"0": {"k": 0, "answer": "Padmé Amidala", "latency_ms": 162.42027282714844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Leia", "latency_ms": 237.55359649658203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Leia", "latency_ms": 66.44129753112793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1190}, "3": {"k": 3, "answer": "Leia", "latency_ms": 233.73794555664062, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1809}, "4": {"k": 4, "answer": "Leia", "latency_ms": 300.2798557281494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2435}, "5": {"k": 5, "answer": "Sabi Sebatynee", "latency_ms": 286.73744201660156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3055}, "6": {"k": 6, "answer": "Mara Jade", "latency_ms": 165.1930809020996, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3620}}}
{"qid": "-5631283176059213935", "question": "who was the original host of jokers wild", "gold_answers": ["Jack Barry"], "shots": {"0": {"k": 0, "answer": "Ralph Edwards", "latency_ms": 69.96941566467285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 150.9263515472412, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Barry Cryer", "latency_ms": 142.48371124267578, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1167}, "3": {"k": 3, "answer": "Barry Cryer", "latency_ms": 187.5479221343994, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1717}, "4": {"k": 4, "answer": "Barry Cryer", "latency_ms": 321.06781005859375, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2312}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 231.278657913208, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2905}, "6": {"k": 6, "answer": "Allen Ludden", "latency_ms": 174.88622665405273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3488}}}
{"qid": "5360223163814883105", "question": "which organelle is the site of atp production and storage", "gold_answers": ["mitochondrion"], "shots": {"0": {"k": 0, "answer": "mitochondria", "latency_ms": 85.45136451721191, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "mitochondria", "latency_ms": 107.62643814086914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 690}, "2": {"k": 2, "answer": "mitochondria", "latency_ms": 136.50774955749512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1347}, "3": {"k": 3, "answer": "mitochondria", "latency_ms": 231.61864280700684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1998}, "4": {"k": 4, "answer": "mitochondria", "latency_ms": 183.5958957672119, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2657}, "5": {"k": 5, "answer": "mitochondrion", "latency_ms": 129.14490699768066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3366}, "6": {"k": 6, "answer": "mitochondrion", "latency_ms": 230.39793968200684, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4139}}}
{"qid": "-718217992489656032", "question": "five tools of security management to overcome computer crime", "gold_answers": ["post-evaluation", "operative planning", "implementation", "Pre-evaluation", "strategic planning"], "shots": {"0": {"k": 0, "answer": "Risk assessment, access control, encryption, intrusion detection, employee training", "latency_ms": 383.4655284881592, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 223.11091423034668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 753}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 147.06063270568848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1462}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 287.66393661499023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2155}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 277.8477668762207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2794}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 315.5241012573242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3477}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 225.36158561706543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4207}}}
{"qid": "5584540254904933863", "question": "who sang what are we doing in love", "gold_answers": ["Dottie West", "Kenny Rogers"], "shots": {"0": {"k": 0, "answer": "The Supremes", "latency_ms": 171.97108268737793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 263.1845474243164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 427.7188777923584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1263}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 247.81131744384766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1847}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 284.3766212463379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2465}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 289.7224426269531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3066}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 311.77616119384766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3669}}}
{"qid": "97369777493072047", "question": "at what age are you no longer a toddler", "gold_answers": ["36 months"], "shots": {"0": {"k": 0, "answer": "3 years old", "latency_ms": 290.60864448547363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "36 months", "latency_ms": 188.74335289001465, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 601}, "2": {"k": 2, "answer": "36 months", "latency_ms": 196.8235969543457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1220}, "3": {"k": 3, "answer": "36 months", "latency_ms": 104.34794425964355, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1765}, "4": {"k": 4, "answer": "36 months", "latency_ms": 156.66818618774414, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2456}, "5": {"k": 5, "answer": "36 months", "latency_ms": 143.82338523864746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3064}, "6": {"k": 6, "answer": "36 months", "latency_ms": 112.14280128479004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3657}}}
{"qid": "3537093784371401720", "question": "how many episodes are there in ordeal by innocence", "gold_answers": ["three", "three part"], "shots": {"0": {"k": 0, "answer": "100", "latency_ms": 98.81377220153809, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "three-part", "latency_ms": 72.32666015625, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 652}, "2": {"k": 2, "answer": "three-part", "latency_ms": 244.06194686889648, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1268}, "3": {"k": 3, "answer": "three-part", "latency_ms": 170.75824737548828, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1892}, "4": {"k": 4, "answer": "three-part", "latency_ms": 267.2255039215088, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 2532}, "5": {"k": 5, "answer": "three-part BBC drama", "latency_ms": 147.21202850341797, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3179}, "6": {"k": 6, "answer": "three-part BBC drama", "latency_ms": 276.8404483795166, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3783}}}
{"qid": "-2126094349681352450", "question": "who played the hobbits in the lord of the rings", "gold_answers": ["Ian Holm", "Dominic Monaghan", "Elijah Wood as Frodo Baggins", "Sean Astin", "Elijah Wood", "Billy Boyd"], "shots": {"0": {"k": 0, "answer": "Elijah Wood, Vivien Leigh, Ian McKellen, Orlando Bloom, Sean Astin, Dominic Monaghan, and Billy Boyd", "latency_ms": 795.1083183288574, "em": 0.0, "f1": 0.2352941176470588, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 208.3415985107422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 666}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 236.830472946167, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1260}, "3": {"k": 3, "answer": "Frodo Baggins, Samwise Gamgee, Peregrin Took, Meriadoc Brandybuck", "latency_ms": 813.2460117340088, "em": 0.0, "f1": 0.3076923076923077, "containment": 0.0, "context_chars": 1926}, "4": {"k": 4, "answer": "Frodo Baggins, Samwise Gamgee, Peregrin Took, Meriadoc Brandybuck", "latency_ms": 778.0790328979492, "em": 0.0, "f1": 0.3076923076923077, "containment": 0.0, "context_chars": 2512}, "5": {"k": 5, "answer": "Frodo Baggins, Samwise Gamgee, Peregrin Took, Meriadoc Brandybuck", "latency_ms": 567.1098232269287, "em": 0.0, "f1": 0.3076923076923077, "containment": 0.0, "context_chars": 3118}, "6": {"k": 6, "answer": "Elijah Wood", "latency_ms": 135.1325511932373, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3747}}}
{"qid": "-6982991192528081677", "question": "what does fancy dress mean in the uk", "gold_answers": ["costumes", "A costume", "guests dress up in costumes"], "shots": {"0": {"k": 0, "answer": "Wearing costumes for a party or event, often for fun or celebration.", "latency_ms": 411.9102954864502, "em": 0.0, "f1": 0.16666666666666669, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "costume", "latency_ms": 58.46381187438965, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 569}, "2": {"k": 2, "answer": "In the UK, fancy dress refers to a costume worn for a celebration or event, often as part of a festival or parade, and is distinct from a daily lifestyle choice.", "latency_ms": 1486.5829944610596, "em": 0.0, "f1": 0.12903225806451613, "containment": 1.0, "context_chars": 1181}, "3": {"k": 3, "answer": "a way of life", "latency_ms": 310.46366691589355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1750}, "4": {"k": 4, "answer": "in the UK, fancy dress refers to a tradition rooted in masked balls of the 18th century, where people wear costumes as part of social events, and it has evolved into a cultural practice including costume parties and celebrations.", "latency_ms": 1507.1003437042236, "em": 0.0, "f1": 0.15, "containment": 1.0, "context_chars": 2363}, "5": {"k": 5, "answer": "in the UK, fancy dress means a way of life, not just a costume.", "latency_ms": 1032.153606414795, "em": 0.0, "f1": 0.25000000000000006, "containment": 1.0, "context_chars": 3033}, "6": {"k": 6, "answer": "in the UK, fancy dress means a way of life, not just a costume.", "latency_ms": 664.4430160522461, "em": 0.0, "f1": 0.25000000000000006, "containment": 1.0, "context_chars": 3705}}}
{"qid": "-6751220433242447969", "question": "how many seasons of the bastard executioner are there", "gold_answers": ["one season", "one"], "shots": {"0": {"k": 0, "answer": "3", "latency_ms": 88.17124366760254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1", "latency_ms": 248.91996383666992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 604}, "2": {"k": 2, "answer": "1", "latency_ms": 57.8770637512207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1204}, "3": {"k": 3, "answer": "1 season", "latency_ms": 99.47943687438965, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1902}, "4": {"k": 4, "answer": "1", "latency_ms": 139.47510719299316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2527}, "5": {"k": 5, "answer": "1", "latency_ms": 305.8280944824219, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3150}, "6": {"k": 6, "answer": "1 season", "latency_ms": 313.72547149658203, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3776}}}
{"qid": "-7607921794902030568", "question": "why did jean valjean take care of cosette", "gold_answers": ["he had promised"], "shots": {"0": {"k": 0, "answer": "To save her from being exploited and to fulfill his moral duty.", "latency_ms": 330.52706718444824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "To rescue her from being sold and to provide her with a better life.", "latency_ms": 374.31955337524414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 604}, "2": {"k": 2, "answer": "To protect her and provide her with a better life.", "latency_ms": 545.9146499633789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1242}, "3": {"k": 3, "answer": "To protect her from the Thénardiers' abuse and to give her a better life.", "latency_ms": 519.2511081695557, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1854}, "4": {"k": 4, "answer": "To protect her from the Thénardiers' abuse and to give her a better life.", "latency_ms": 1085.0889682769775, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2467}, "5": {"k": 5, "answer": "To save her from the Thénardiers' abuse and to provide her with a better life.", "latency_ms": 924.9141216278076, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3117}, "6": {"k": 6, "answer": "To protect her and provide her with a better life after the Thénardiers mistreated her.", "latency_ms": 616.6119575500488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3717}}}
{"qid": "-5341729935918652247", "question": "who does the democratic republic of congo trade with", "gold_answers": ["Belgium", "World Bank", "China", "Zambia", "African Development Bank", "SADC", "South Africa", "AU", "WTO", "France", "Kenya", "Zimbabwe", "Group of 77", "IMF"], "shots": {"0": {"k": 0, "answer": "China, Belgium, France, Zambia, Angola, South Africa, Brazil, Russia, United States, Nigeria, Tanzania, Rwanda, Zambia, Uganda, Kenya, Ethiopia, Sudan, Namibia, Botswana, Zimbabwe, Zambia, Malawi, Mozambique, South Africa, Tanzania, Uganda, Kenya, Ethiopia, Sudan, Namibia, Botswana, Zimbabwe, Zambia, Malawi, Mozambique", "latency_ms": 1701.4503479003906, "em": 0.0, "f1": 0.1, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Russia", "latency_ms": 156.4633846282959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 699}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 170.96185684204102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1393}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 175.46892166137695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2112}, "4": {"k": 4, "answer": "Kenya", "latency_ms": 153.1081199645996, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2802}, "5": {"k": 5, "answer": "Kenya", "latency_ms": 280.29417991638184, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3529}, "6": {"k": 6, "answer": "Kenya", "latency_ms": 303.9524555206299, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4223}}}
{"qid": "5518515944351776633", "question": "what was the purpose of the bantu education act", "gold_answers": ["enforcing racially separated educational facilities"], "shots": {"0": {"k": 0, "answer": "To separate Black South Africans from the white population and maintain racial hierarchy.", "latency_ms": 414.5505428314209, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "to consolidate Bantu education and uniformly implement discriminatory educational practices", "latency_ms": 465.1339054107666, "em": 0.0, "f1": 0.13333333333333333, "containment": 0.0, "context_chars": 714}, "2": {"k": 2, "answer": "to consolidate Bantu education so that discriminatory educational practices could be uniformly implemented across South Africa", "latency_ms": 742.4097061157227, "em": 0.0, "f1": 0.09523809523809523, "containment": 0.0, "context_chars": 1413}, "3": {"k": 3, "answer": "to consolidate Bantu education so that discriminatory educational practices could be uniformly implemented across South Africa", "latency_ms": 825.8254528045654, "em": 0.0, "f1": 0.09523809523809523, "containment": 0.0, "context_chars": 2090}, "4": {"k": 4, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 598.0408191680908, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2704}, "5": {"k": 5, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 339.81776237487793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3382}, "6": {"k": 6, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 368.4995174407959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4022}}}
{"qid": "-4271099756149693907", "question": "what type of snake is kaa in jungle book", "gold_answers": ["Indian rock python"], "shots": {"0": {"k": 0, "answer": "Python", "latency_ms": 55.49788475036621, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kaa is a snake.", "latency_ms": 319.3681240081787, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "Kaa is a giant python.", "latency_ms": 237.84899711608887, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1174}, "3": {"k": 3, "answer": "Kaa is a python.", "latency_ms": 252.63524055480957, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1759}, "4": {"k": 4, "answer": "Kaa is a python.", "latency_ms": 393.46814155578613, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2384}, "5": {"k": 5, "answer": "Kaa is a python.", "latency_ms": 300.3723621368408, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3007}, "6": {"k": 6, "answer": "Kaa is a python.", "latency_ms": 312.50762939453125, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3616}}}
{"qid": "-6527462410412242988", "question": "what is the snow on mars made of", "gold_answers": ["water ice", "Frozen carbon dioxide"], "shots": {"0": {"k": 0, "answer": "water ice and carbon dioxide ice", "latency_ms": 369.08888816833496, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "frost or snow mixed generously with dust", "latency_ms": 132.31682777404785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "snow mixed generously with dust", "latency_ms": 143.00870895385742, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1229}, "3": {"k": 3, "answer": "frost or snow mixed generously with dust", "latency_ms": 149.32942390441895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1874}, "4": {"k": 4, "answer": "snow mixed generously with dust", "latency_ms": 122.31969833374023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2522}, "5": {"k": 5, "answer": "snow mixed with dust", "latency_ms": 136.3685131072998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3140}, "6": {"k": 6, "answer": "water ice mixed with dust", "latency_ms": 341.23754501342773, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3727}}}
{"qid": "-2155345050664800035", "question": "when does elena turn into a vampire in the tv series", "gold_answers": ["season four", "fourth season"], "shots": {"0": {"k": 0, "answer": "Season 1, Episode 4", "latency_ms": 107.9256534576416, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "when Elena opens her eyes in the hospital", "latency_ms": 235.7783317565918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "when Elena opens her eyes in the hospital in the episode \"The Departed\"", "latency_ms": 588.109016418457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1222}, "3": {"k": 3, "answer": "transition occurs in the episode where she opens her eyes in the hospital, marking the pivotal point in season 3.", "latency_ms": 805.0925731658936, "em": 0.0, "f1": 0.10526315789473684, "containment": 0.0, "context_chars": 1786}, "4": {"k": 4, "answer": "transition occurs in the episode \"The Departed\" after the series begins, initially planned for end of second season but delayed. Final answer: during the series, after the second season.", "latency_ms": 1186.5408420562744, "em": 0.0, "f1": 0.07692307692307693, "containment": 0.0, "context_chars": 2348}, "5": {"k": 5, "answer": "Elena turns into a vampire in season 3 of The Vampire Diaries.", "latency_ms": 387.2518539428711, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 2913}, "6": {"k": 6, "answer": "Elena turns into a vampire in season 3 of The Vampire Diaries.", "latency_ms": 337.86606788635254, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 3477}}}
{"qid": "5491319565508469860", "question": "who gets first pick of fa cup games", "gold_answers": ["the BBC", "BBC"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 166.28623008728027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 144.9601650238037, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 578}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 91.2785530090332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1168}, "3": {"k": 3, "answer": "team named first on the date specified", "latency_ms": 398.4408378601074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1749}, "4": {"k": 4, "answer": "team named first on the date specified", "latency_ms": 334.4426155090332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2396}, "5": {"k": 5, "answer": "team named first on the date specified", "latency_ms": 488.56043815612793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2983}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 135.84423065185547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3592}}}
{"qid": "-816889472900831237", "question": "what part of dc is howard university in", "gold_answers": ["northwest", "northwest Washington", "in northwest Washington"], "shots": {"0": {"k": 0, "answer": "Ward 8", "latency_ms": 274.98817443847656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Northwest Washington, DC", "latency_ms": 259.613037109375, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 670}, "2": {"k": 2, "answer": "Northwest Washington, DC", "latency_ms": 271.7154026031494, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1346}, "3": {"k": 3, "answer": "Northwest Washington, DC", "latency_ms": 402.6145935058594, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2040}, "4": {"k": 4, "answer": "Northwest Washington, DC", "latency_ms": 253.53050231933594, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2831}, "5": {"k": 5, "answer": "Northwest Washington, DC", "latency_ms": 415.36855697631836, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3517}, "6": {"k": 6, "answer": "Northwest Washington, DC", "latency_ms": 230.6210994720459, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 4166}}}
{"qid": "479254158383415496", "question": "what is the first covenant in the old testament", "gold_answers": ["the Noahic Covenant"], "shots": {"0": {"k": 0, "answer": "Covenant of Noah", "latency_ms": 315.565824508667, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Covenant of Noah", "latency_ms": 255.4607391357422, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Covenant of Noah", "latency_ms": 142.63033866882324, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1234}, "3": {"k": 3, "answer": "Mosaic covenant", "latency_ms": 82.42249488830566, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1845}, "4": {"k": 4, "answer": "Mosaic covenant", "latency_ms": 94.12002563476562, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2483}, "5": {"k": 5, "answer": "Mosaic Covenant", "latency_ms": 110.49771308898926, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3140}, "6": {"k": 6, "answer": "Mosaic covenant", "latency_ms": 201.31230354309082, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3810}}}
{"qid": "-8025421046364238920", "question": "who sings the theme song for living single", "gold_answers": ["Queen Latifah"], "shots": {"0": {"k": 0, "answer": "Lil' Kim", "latency_ms": 126.47366523742676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wings", "latency_ms": 173.92849922180176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 216.80569648742676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1188}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 110.29386520385742, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1768}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 267.73619651794434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2371}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 192.08431243896484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2968}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 272.40538597106934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3539}}}
{"qid": "-2347826883564742884", "question": "who said life is like a bowl of cherries", "gold_answers": ["Lew Brown"], "shots": {"0": {"k": 0, "answer": "W. C. Fields", "latency_ms": 410.1889133453369, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Rudy Vallée", "latency_ms": 77.30627059936523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 590}, "2": {"k": 2, "answer": "Rudy Vallée", "latency_ms": 93.00088882446289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1180}, "3": {"k": 3, "answer": "Anna King", "latency_ms": 101.409912109375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1785}, "4": {"k": 4, "answer": "Rudy Vallée and Anna King", "latency_ms": 300.89497566223145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2383}, "5": {"k": 5, "answer": "Rudy Vallée and Anna King", "latency_ms": 268.0182456970215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3018}, "6": {"k": 6, "answer": "Rudy Vallée, Anna King, Mary Engelbreit, Erma Bombeck", "latency_ms": 581.275463104248, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3669}}}
{"qid": "2467509477831469982", "question": "where is fort myers located at in florida", "gold_answers": ["Lee County", "the Southwest"], "shots": {"0": {"k": 0, "answer": "Southwest Florida coast", "latency_ms": 159.46102142333984, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lee County, Florida", "latency_ms": 135.94365119934082, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 642}, "2": {"k": 2, "answer": "Lee County, Florida", "latency_ms": 168.84255409240723, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1265}, "3": {"k": 3, "answer": "Lee County, Florida", "latency_ms": 188.92192840576172, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1882}, "4": {"k": 4, "answer": "Lee County, Florida", "latency_ms": 113.41285705566406, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2529}, "5": {"k": 5, "answer": "Lee County, Florida", "latency_ms": 224.5950698852539, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3149}, "6": {"k": 6, "answer": "Fort Myers, Florida", "latency_ms": 187.0567798614502, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3746}}}
{"qid": "4440911260005876349", "question": "state the position of india in terms of population size", "gold_answers": ["second"], "shots": {"0": {"k": 0, "answer": "2nd", "latency_ms": 113.70015144348145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 84.9452018737793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 247.99132347106934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1273}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 130.70130348205566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1981}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 233.69836807250977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2572}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 128.3555030822754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3255}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 250.69546699523926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3922}}}
{"qid": "3172949410241605868", "question": "who became a leader of the indian national congress", "gold_answers": ["Mahatma Gandhi"], "shots": {"0": {"k": 0, "answer": "Mahatma Gandhi", "latency_ms": 142.05002784729004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Mahatma Gandhi", "latency_ms": 179.92711067199707, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 694}, "2": {"k": 2, "answer": "Mahatma Gandhi", "latency_ms": 125.23722648620605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1402}, "3": {"k": 3, "answer": "Mahatma Gandhi", "latency_ms": 365.7712936401367, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2124}, "4": {"k": 4, "answer": "A.O. Hume", "latency_ms": 322.47018814086914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2818}, "5": {"k": 5, "answer": "Sonia Gandhi", "latency_ms": 160.80689430236816, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3492}, "6": {"k": 6, "answer": "Mahatma Gandhi", "latency_ms": 184.94033813476562, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4202}}}
{"qid": "-5136645336730213305", "question": "winner of worst cooks in america season 5", "gold_answers": ["Amber Brauner"], "shots": {"0": {"k": 0, "answer": "Terry Jones", "latency_ms": 96.33898735046387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 231.5526008605957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 185.07647514343262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1211}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 132.49707221984863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1857}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 226.54461860656738, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2448}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 309.7724914550781, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3034}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 178.9236068725586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3645}}}
{"qid": "159123015669900402", "question": "what is the filename extension used for all java source files", "gold_answers": [". java"], "shots": {"0": {"k": 0, "answer": ".java", "latency_ms": 40.93647003173828, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": ".java", "latency_ms": 43.427228927612305, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": ".java", "latency_ms": 133.32271575927734, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1275}, "3": {"k": 3, "answer": ".java", "latency_ms": 58.4263801574707, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1934}, "4": {"k": 4, "answer": ".java", "latency_ms": 90.09957313537598, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2549}, "5": {"k": 5, "answer": ".java", "latency_ms": 238.5413646697998, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3152}, "6": {"k": 6, "answer": ".java", "latency_ms": 339.58888053894043, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3772}}}
{"qid": "6860341019198485637", "question": "who cracked the enigma code in world war 2", "gold_answers": ["Turing"], "shots": {"0": {"k": 0, "answer": "Alan Turing", "latency_ms": 242.3102855682373, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Poland, then the British", "latency_ms": 136.60717010498047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 683}, "2": {"k": 2, "answer": "Polish Cipher Bureau", "latency_ms": 203.93753051757812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1300}, "3": {"k": 3, "answer": "Poland, then Britain", "latency_ms": 231.12726211547852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1995}, "4": {"k": 4, "answer": "Poland, Britain, and France", "latency_ms": 363.62743377685547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2696}, "5": {"k": 5, "answer": "Poland, Britain, and France", "latency_ms": 340.9135341644287, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3423}, "6": {"k": 6, "answer": "Poland, Britain, France", "latency_ms": 209.60187911987305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4102}}}
{"qid": "7990281571644070368", "question": "what is the job of the whip in congress", "gold_answers": ["to ensure party discipline"], "shots": {"0": {"k": 0, "answer": "To ensure party members vote according to party leadership.", "latency_ms": 442.38924980163574, "em": 0.0, "f1": 0.4615384615384615, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "to ensure party members vote according to party line and manage party votes in Congress", "latency_ms": 222.66340255737305, "em": 0.0, "f1": 0.31578947368421056, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "Ensure party members vote according to party line and maintain attendance at votes.", "latency_ms": 635.4100704193115, "em": 0.0, "f1": 0.3529411764705882, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Ensure party members vote along party lines and manage vote counts through communication and incentives.", "latency_ms": 768.3775424957275, "em": 0.0, "f1": 0.2105263157894737, "containment": 0.0, "context_chars": 1952}, "4": {"k": 4, "answer": "Ensure members adhere to party policies and vote as directed.", "latency_ms": 743.5433864593506, "em": 0.0, "f1": 0.4285714285714285, "containment": 0.0, "context_chars": 2586}, "5": {"k": 5, "answer": "Ensure members adhere to party policies and vote as directed.", "latency_ms": 479.1100025177002, "em": 0.0, "f1": 0.4285714285714285, "containment": 0.0, "context_chars": 3310}, "6": {"k": 6, "answer": "Ensure members adhere to party policies and vote as directed.", "latency_ms": 538.4612083435059, "em": 0.0, "f1": 0.4285714285714285, "containment": 0.0, "context_chars": 4003}}}
{"qid": "-3089533754017582743", "question": "when was the last time unc did not make the ncaa tournament", "gold_answers": ["2003"], "shots": {"0": {"k": 0, "answer": "2020", "latency_ms": 149.35302734375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2011", "latency_ms": 130.76353073120117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 579}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 235.7954978942871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1157}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 187.25347518920898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1776}, "4": {"k": 4, "answer": "1973", "latency_ms": 212.1865749359131, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2331}, "5": {"k": 5, "answer": "1973", "latency_ms": 273.5719680786133, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3013}, "6": {"k": 6, "answer": "1973", "latency_ms": 214.24102783203125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3677}}}
{"qid": "-3530165048900528552", "question": "which foreign currency option is the​ right but not the​ obligation to buy foreign​ currency", "gold_answers": ["foreign exchange option"], "shots": {"0": {"k": 0, "answer": "Call option", "latency_ms": 81.56061172485352, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "FX option", "latency_ms": 60.36710739135742, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 676}, "2": {"k": 2, "answer": "FX option", "latency_ms": 101.18746757507324, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1364}, "3": {"k": 3, "answer": "FX option", "latency_ms": 98.95682334899902, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2054}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 279.8581123352051, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2709}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 327.0230293273926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3368}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 129.27889823913574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3985}}}
{"qid": "2795932191977151266", "question": "how many house of representative do we have in nigeria", "gold_answers": ["360", "360 members"], "shots": {"0": {"k": 0, "answer": "360", "latency_ms": 169.71421241760254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "360", "latency_ms": 199.11742210388184, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 721}, "2": {"k": 2, "answer": "360", "latency_ms": 102.97989845275879, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1380}, "3": {"k": 3, "answer": "360", "latency_ms": 160.76254844665527, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2030}, "4": {"k": 4, "answer": "360", "latency_ms": 104.30145263671875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2724}, "5": {"k": 5, "answer": "360", "latency_ms": 211.33112907409668, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3379}, "6": {"k": 6, "answer": "360", "latency_ms": 164.3204689025879, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4027}}}
{"qid": "4714895284721516061", "question": "who was the actor who played grizzly adams", "gold_answers": ["`` Dan '' Haggerty"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 142.99941062927246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dan Haggerty", "latency_ms": 199.13864135742188, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "Dan Haggerty", "latency_ms": 231.24933242797852, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1285}, "3": {"k": 3, "answer": "Dan Haggerty", "latency_ms": 251.80935859680176, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1953}, "4": {"k": 4, "answer": "Dan Haggerty", "latency_ms": 239.95280265808105, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2599}, "5": {"k": 5, "answer": "Dan Haggerty", "latency_ms": 387.27688789367676, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3240}, "6": {"k": 6, "answer": "Dan Haggerty", "latency_ms": 238.17777633666992, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3952}}}
{"qid": "-6101054679785992133", "question": "who sings blame it on the bossa nova", "gold_answers": ["Eydie Gormé"], "shots": {"0": {"k": 0, "answer": "Bobby Vee", "latency_ms": 436.16175651550293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Gormé", "latency_ms": 143.8601016998291, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 599}, "2": {"k": 2, "answer": "Eydie Gormé", "latency_ms": 280.5490493774414, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1181}, "3": {"k": 3, "answer": "Eydie Gormé", "latency_ms": 309.6630573272705, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1757}, "4": {"k": 4, "answer": "Eydie Gormé", "latency_ms": 185.29319763183594, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2334}, "5": {"k": 5, "answer": "Eydie Gormé", "latency_ms": 272.23849296569824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2899}, "6": {"k": 6, "answer": "Eydie Gormé", "latency_ms": 225.8915901184082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3505}}}
{"qid": "1882117704293632224", "question": "who is the supreme court judge was a former chief justice of kerala high court", "gold_answers": ["Konakuppakatil Gopinathan Balakrishnan"], "shots": {"0": {"k": 0, "answer": "Justice D. K. Jain", "latency_ms": 256.0853958129883, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 191.01524353027344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 613}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 134.97090339660645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 109.98845100402832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1838}, "4": {"k": 4, "answer": "M. Jagannadha Rao", "latency_ms": 434.3075752258301, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2406}, "5": {"k": 5, "answer": "M. Jagannadha Rao", "latency_ms": 338.62829208374023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3081}, "6": {"k": 6, "answer": "M. Jagannadha Rao", "latency_ms": 419.1122055053711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3658}}}
{"qid": "2524852627963543698", "question": "who has won more grand slam titles in tennis", "gold_answers": ["Roger Federer"], "shots": {"0": {"k": 0, "answer": "Novak Djokovic", "latency_ms": 107.96070098876953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 192.37613677978516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 216.6581153869629, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 163.98000717163086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "Serena Williams", "latency_ms": 142.66633987426758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2489}, "5": {"k": 5, "answer": "Serena Williams", "latency_ms": 177.99639701843262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3128}, "6": {"k": 6, "answer": "Serena Williams", "latency_ms": 113.9075756072998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3728}}}
{"qid": "-6230694167244306035", "question": "how many times has the saints won the super bowl", "gold_answers": ["1"], "shots": {"0": {"k": 0, "answer": "5", "latency_ms": 64.33916091918945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1", "latency_ms": 56.108713150024414, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 608}, "2": {"k": 2, "answer": "1", "latency_ms": 82.16547966003418, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1228}, "3": {"k": 3, "answer": "1", "latency_ms": 211.9615077972412, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1827}, "4": {"k": 4, "answer": "1", "latency_ms": 86.78913116455078, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2434}, "5": {"k": 5, "answer": "1", "latency_ms": 290.35353660583496, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3017}, "6": {"k": 6, "answer": "1", "latency_ms": 303.59482765197754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3624}}}
{"qid": "4186485631454362041", "question": "when was the last time the los angeles lakers won a championship", "gold_answers": ["2010", "in 2010"], "shots": {"0": {"k": 0, "answer": "2020", "latency_ms": 82.05699920654297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2010", "latency_ms": 87.2344970703125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 653}, "2": {"k": 2, "answer": "2010", "latency_ms": 130.65338134765625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1267}, "3": {"k": 3, "answer": "2010", "latency_ms": 279.54745292663574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1883}, "4": {"k": 4, "answer": "2010", "latency_ms": 108.53052139282227, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2534}, "5": {"k": 5, "answer": "2010", "latency_ms": 189.38422203063965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3130}, "6": {"k": 6, "answer": "2010", "latency_ms": 202.30579376220703, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3751}}}
{"qid": "6332740420650663139", "question": "how many gold medals did australia win in the 2000 olympics", "gold_answers": ["16"], "shots": {"0": {"k": 0, "answer": "32", "latency_ms": 202.41069793701172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 300.9769916534424, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 627}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 157.7126979827881, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1233}, "3": {"k": 3, "answer": "1 gold medal", "latency_ms": 185.00995635986328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1881}, "4": {"k": 4, "answer": "1 gold medal", "latency_ms": 135.9872817993164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2519}, "5": {"k": 5, "answer": "1", "latency_ms": 130.3555965423584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3180}, "6": {"k": 6, "answer": "5", "latency_ms": 202.84748077392578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3831}}}
{"qid": "-2418032234376469176", "question": "where is the eagle creek fire located in oregon", "gold_answers": ["Cascade Locks", "Columbia River Gorge"], "shots": {"0": {"k": 0, "answer": "Wallowa County, Oregon", "latency_ms": 213.56821060180664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 187.82758712768555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 628}, "2": {"k": 2, "answer": "Columbia River Gorge", "latency_ms": 308.72607231140137, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1254}, "3": {"k": 3, "answer": "Columbia River Gorge", "latency_ms": 197.48544692993164, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1896}, "4": {"k": 4, "answer": "Columbia River Gorge, Oregon", "latency_ms": 362.22362518310547, "em": 0.0, "f1": 0.8571428571428571, "containment": 1.0, "context_chars": 2513}, "5": {"k": 5, "answer": "Columbia River Gorge, Oregon", "latency_ms": 245.91565132141113, "em": 0.0, "f1": 0.8571428571428571, "containment": 1.0, "context_chars": 3144}, "6": {"k": 6, "answer": "Columbia River Gorge, Oregon", "latency_ms": 322.16572761535645, "em": 0.0, "f1": 0.8571428571428571, "containment": 1.0, "context_chars": 3795}}}
{"qid": "-2622854017829301835", "question": "different ways to spell corey for a boy", "gold_answers": ["Kori", "Cory", "Kory", "Coire", "Corrie", "Correy", "Corie", "Corey", "Khouri"], "shots": {"0": {"k": 0, "answer": "Corey, Corry, Kory, Cory, Koorie", "latency_ms": 862.8294467926025, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Corey, Corry, Corye, Korry", "latency_ms": 769.2146301269531, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 619}, "2": {"k": 2, "answer": "Corey, Corey, Corey", "latency_ms": 284.79528427124023, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1255}, "3": {"k": 3, "answer": "Corey, Corey, Corey", "latency_ms": 203.6733627319336, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1857}, "4": {"k": 4, "answer": "Corey, Corey, Corey", "latency_ms": 240.90147018432617, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2430}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 302.23989486694336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3038}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 257.9519748687744, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3682}}}
{"qid": "3480908309420822259", "question": "who sang the song should have been a cowboy", "gold_answers": ["Toby Keith"], "shots": {"0": {"k": 0, "answer": "Linda Ronstadt", "latency_ms": 80.68561553955078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Randy Houser", "latency_ms": 82.87620544433594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 157.7780246734619, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1206}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 264.71734046936035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1852}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 210.91580390930176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2450}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 309.4358444213867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3084}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 238.42716217041016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3751}}}
{"qid": "-4198273171611748168", "question": "how many counties does the state of georgia have", "gold_answers": ["159", "159 counties"], "shots": {"0": {"k": 0, "answer": "159", "latency_ms": 101.42230987548828, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "159", "latency_ms": 85.8757495880127, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "159", "latency_ms": 157.84072875976562, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1333}, "3": {"k": 3, "answer": "159", "latency_ms": 121.01578712463379, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1974}, "4": {"k": 4, "answer": "159", "latency_ms": 236.19842529296875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2617}, "5": {"k": 5, "answer": "159", "latency_ms": 164.42465782165527, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3252}, "6": {"k": 6, "answer": "159", "latency_ms": 211.5943431854248, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3867}}}
{"qid": "2077711785168359308", "question": "who sang picking up pebbles and throwing them into the sea", "gold_answers": ["Matt Flinders"], "shots": {"0": {"k": 0, "answer": "Tom Jones", "latency_ms": 115.37504196166992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 192.57712364196777, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 189.08071517944336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1174}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 205.38997650146484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1758}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 187.71624565124512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2349}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 181.72264099121094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2979}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 159.5134735107422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3514}}}
{"qid": "9062886943819260445", "question": "who made the most free throws in nba history", "gold_answers": ["Karl Malone"], "shots": {"0": {"k": 0, "answer": "Stephen Curry", "latency_ms": 51.7725944519043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 80.83844184875488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 626}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 152.83584594726562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1219}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 316.62797927856445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1821}, "4": {"k": 4, "answer": "Dirk Nowitzki", "latency_ms": 397.4578380584717, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2415}, "5": {"k": 5, "answer": "Dirk Nowitzki", "latency_ms": 290.346622467041, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3155}, "6": {"k": 6, "answer": "Dirk Nowitzki", "latency_ms": 280.37571907043457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3741}}}
{"qid": "1278533364941960992", "question": "which is the ring finger for male in india", "gold_answers": ["the left ring finger", "left ring finger"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 76.61128044128418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "left hand ring finger", "latency_ms": 184.41081047058105, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 552}, "2": {"k": 2, "answer": "right hand", "latency_ms": 84.97476577758789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1137}, "3": {"k": 3, "answer": "right hand", "latency_ms": 73.8213062286377, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1691}, "4": {"k": 4, "answer": "right hand", "latency_ms": 130.6145191192627, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2293}, "5": {"k": 5, "answer": "right hand", "latency_ms": 187.98232078552246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2857}, "6": {"k": 6, "answer": "right hand", "latency_ms": 159.46602821350098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3471}}}
{"qid": "-8788207958970648395", "question": "what disney cartoon character's middle name is fauntleroy", "gold_answers": ["Donald Fauntleroy Duck", "Donald Duck"], "shots": {"0": {"k": 0, "answer": "Pinocchio", "latency_ms": 192.58356094360352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Donald Duck", "latency_ms": 145.7061767578125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 647}, "2": {"k": 2, "answer": "Donald Duck", "latency_ms": 134.17625427246094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1282}, "3": {"k": 3, "answer": "Donald Duck", "latency_ms": 199.25761222839355, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1934}, "4": {"k": 4, "answer": "Donald Duck", "latency_ms": 295.36914825439453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2561}, "5": {"k": 5, "answer": "Donald Duck", "latency_ms": 312.4377727508545, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3199}, "6": {"k": 6, "answer": "Donald Duck", "latency_ms": 329.14018630981445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3868}}}
{"qid": "-4323000711881657078", "question": "who played king theoden in lord of the rings", "gold_answers": ["Bernard Hill"], "shots": {"0": {"k": 0, "answer": "Waited I cannot answer.", "latency_ms": 608.5915565490723, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 303.6930561065674, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 242.91038513183594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1238}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 243.3605194091797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1860}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 113.38639259338379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2466}, "5": {"k": 5, "answer": "Sean Astin", "latency_ms": 146.88372611999512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3067}, "6": {"k": 6, "answer": "Sean Astin", "latency_ms": 176.94807052612305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3680}}}
{"qid": "-2664158830468622213", "question": "dark parables return of the salt princess release date", "gold_answers": ["September 2017"], "shots": {"0": {"k": 0, "answer": "2023", "latency_ms": 133.6688995361328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 149.61671829223633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 616}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 146.32654190063477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1191}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 288.1503105163574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1792}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 234.78293418884277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2426}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 463.09423446655273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3068}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 373.49629402160645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3656}}}
{"qid": "9078092620239351027", "question": "where was the movie 500 days of summer filmed", "gold_answers": ["Los Angeles"], "shots": {"0": {"k": 0, "answer": "Los Angeles, California", "latency_ms": 299.1821765899658, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 115.98372459411621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 650}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 138.95750045776367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1305}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 322.5839138031006, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1996}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 261.39378547668457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2635}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 172.96957969665527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3260}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 234.96198654174805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3875}}}
{"qid": "4747795168824294053", "question": "who played emma in h2o just add water", "gold_answers": ["Claire Rhiannon Holt"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 135.4224681854248, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 184.99398231506348, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 100.20208358764648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1252}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 178.9085865020752, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 138.63086700439453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2564}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 209.7923755645752, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3135}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 343.7480926513672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3752}}}
{"qid": "-1368633715963532113", "question": "where can carbon be found in the biosphere", "gold_answers": ["other living organisms", "plants", "soil", "The terrestrial biosphere"], "shots": {"0": {"k": 0, "answer": "atmosphere, oceans, soil, living organisms", "latency_ms": 208.5247039794922, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "plants, animals, dead plant matter", "latency_ms": 269.80137825012207, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 621}, "2": {"k": 2, "answer": "in plants and other living organisms, soils, and as dead plant matter", "latency_ms": 532.9549312591553, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1271}, "3": {"k": 3, "answer": "in plants, soils, animals, and dead plant matter", "latency_ms": 475.5876064300537, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 1956}, "4": {"k": 4, "answer": "in plants, soils, and organic matter of land-living organisms", "latency_ms": 548.2728481292725, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 2663}, "5": {"k": 5, "answer": "in plants, soils, animals, and dead plant matter", "latency_ms": 216.98760986328125, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 3396}, "6": {"k": 6, "answer": "in plants, soils, and dead organic matter", "latency_ms": 465.73352813720703, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 4116}}}
{"qid": "-1173252802989599019", "question": "who is the winner of bigg boss kannada season", "gold_answers": ["rapper Chandan Shetty", "Chandan Shetty"], "shots": {"0": {"k": 0, "answer": "2023: Nandini Reddy", "latency_ms": 344.11168098449707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 312.0887279510498, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 220.18027305603027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1187}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 207.197904586792, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1761}, "4": {"k": 4, "answer": "Chandan Shetty", "latency_ms": 171.6480255126953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2373}, "5": {"k": 5, "answer": "Chandan Shetty", "latency_ms": 314.5625591278076, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2968}, "6": {"k": 6, "answer": "Chandan Shetty", "latency_ms": 254.64534759521484, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3611}}}
{"qid": "-8950878510839541193", "question": "where was the outlaw of josey wales filmed", "gold_answers": ["Utah", "Arizona", "Kanab , Utah", "Lake Powell", "Wyoming", "Oroville , California"], "shots": {"0": {"k": 0, "answer": "Tennessee", "latency_ms": 114.2570972442627, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 190.20652770996094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 89.45894241333008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1247}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 243.11280250549316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1866}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 158.6451530456543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2499}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 124.34530258178711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3126}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 128.3700466156006, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3744}}}
{"qid": "445108246725869963", "question": "who plays the dad in the new beauty and the beast", "gold_answers": ["Kevin Kline"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 157.484769821167, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 142.60220527648926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 116.26577377319336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1172}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 108.21652412414551, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1831}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 272.62043952941895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2457}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 365.41032791137695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3100}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 233.07394981384277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3777}}}
{"qid": "-5745040113272333526", "question": "where does the saskatchewan river start and end", "gold_answers": ["the Hudson Bay", "central Saskatchewan", "Saskatchewan River Forks , Saskatchewan", "Saskatchewan Glacier , Alberta"], "shots": {"0": {"k": 0, "answer": "Starts in the Rocky Mountains, ends in the North Dakota-Montana border.", "latency_ms": 340.6405448913574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Starts at North Saskatchewan River in Canadian Rockies, ends at Hudson Bay", "latency_ms": 572.6447105407715, "em": 0.0, "f1": 0.2857142857142857, "containment": 0.0, "context_chars": 653}, "2": {"k": 2, "answer": "Starts in central Saskatchewan, east of the Rocky Mountains; ends in Lake Winnipeg.", "latency_ms": 724.7376441955566, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 1383}, "3": {"k": 3, "answer": "Starts: confluence of North and South Saskatchewan rivers in central Saskatchewan; Ends: Lake Winnipeg", "latency_ms": 386.8539333343506, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 2093}, "4": {"k": 4, "answer": "Starts at Saskatchewan River Forks (confluence of North and South Saskatchewan Rivers in central Saskatchewan); ends in Lake Winnipeg.", "latency_ms": 962.6970291137695, "em": 0.0, "f1": 0.34782608695652173, "containment": 1.0, "context_chars": 2737}, "5": {"k": 5, "answer": "Starts at Saskatchewan River Forks (confluence of North and South Saskatchewan Rivers in central Saskatchewan); ends in Lake Winnipeg.", "latency_ms": 914.1767024993896, "em": 0.0, "f1": 0.34782608695652173, "containment": 1.0, "context_chars": 3342}, "6": {"k": 6, "answer": "Starts at Saskatchewan River Forks (confluence of North and South Saskatchewan Rivers in central Saskatchewan); ends in Lake Winnipeg.", "latency_ms": 779.5584201812744, "em": 0.0, "f1": 0.34782608695652173, "containment": 1.0, "context_chars": 4010}}}
{"qid": "2370076891269438265", "question": "where are the spanish steps located in italy", "gold_answers": ["Rome"], "shots": {"0": {"k": 0, "answer": "Rome", "latency_ms": 51.21159553527832, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 120.24641036987305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "Rome, Italy", "latency_ms": 94.70391273498535, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1262}, "3": {"k": 3, "answer": "Rome, Italy", "latency_ms": 147.48597145080566, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1916}, "4": {"k": 4, "answer": "Rome, Italy", "latency_ms": 186.38896942138672, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2547}, "5": {"k": 5, "answer": "Rome, Italy", "latency_ms": 228.39999198913574, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3180}, "6": {"k": 6, "answer": "Rome, Italy", "latency_ms": 193.67408752441406, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3778}}}
{"qid": "-1701212376306091221", "question": "what does the m number mean on a pint glass", "gold_answers": ["imperial fluid ounces", "the manufacturing company or site"], "shots": {"0": {"k": 0, "answer": "M number refers to the glass's capacity in milliliters.", "latency_ms": 263.7755870819092, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "manufacturing company or site", "latency_ms": 121.0181713104248, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "the manufacturing company or site", "latency_ms": 111.21249198913574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1285}, "3": {"k": 3, "answer": "the manufacturing company or site", "latency_ms": 111.22775077819824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1903}, "4": {"k": 4, "answer": "the manufacturing company or site", "latency_ms": 359.44247245788574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2507}, "5": {"k": 5, "answer": "the manufacturing company or site", "latency_ms": 212.51916885375977, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3117}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 186.5706443786621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3724}}}
{"qid": "3631867991857999976", "question": "other than water what else has hydrogen bonds", "gold_answers": ["inorganic molecules such as water"], "shots": {"0": {"k": 0, "answer": "ammonia, alcohols, carboxylic acids", "latency_ms": 449.68485832214355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "ammonia, alcohols, acids", "latency_ms": 234.01808738708496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 473.7250804901123, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1267}, "3": {"k": 3, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 494.89712715148926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1940}, "4": {"k": 4, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 432.97672271728516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2625}, "5": {"k": 5, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 627.8400421142578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3216}, "6": {"k": 6, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 256.06822967529297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3820}}}
{"qid": "-1665528830781573810", "question": "when boy meets girl waiting for a star to fall", "gold_answers": ["1988"], "shots": {"0": {"k": 0, "answer": "1988", "latency_ms": 76.25007629394531, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1989", "latency_ms": 162.88232803344727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "1988", "latency_ms": 213.7320041656494, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1173}, "3": {"k": 3, "answer": "1988", "latency_ms": 109.78126525878906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1830}, "4": {"k": 4, "answer": "1988", "latency_ms": 237.90645599365234, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2433}, "5": {"k": 5, "answer": "1988", "latency_ms": 263.98515701293945, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3094}, "6": {"k": 6, "answer": "1988", "latency_ms": 327.52227783203125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3718}}}
{"qid": "4289442278067074619", "question": "trick taking card game name derived from spanish for man", "gold_answers": ["Skat"], "shots": {"0": {"k": 0, "answer": "Truco", "latency_ms": 155.12561798095703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ombre", "latency_ms": 64.52298164367676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "Piquet", "latency_ms": 182.47675895690918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1234}, "3": {"k": 3, "answer": "Euchre", "latency_ms": 100.80265998840332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1835}, "4": {"k": 4, "answer": "Euchre", "latency_ms": 246.17767333984375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2424}, "5": {"k": 5, "answer": "Euchre", "latency_ms": 148.86975288391113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3063}, "6": {"k": 6, "answer": "Ombre", "latency_ms": 237.72764205932617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3628}}}
{"qid": "-7311149600272472027", "question": "what percentage of the population is naturally blonde", "gold_answers": ["2 %"], "shots": {"0": {"k": 0, "answer": "2%", "latency_ms": 306.66589736938477, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "5%", "latency_ms": 138.59868049621582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "5%", "latency_ms": 138.4286880493164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1317}, "3": {"k": 3, "answer": "2% to 16%", "latency_ms": 319.0727233886719, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1927}, "4": {"k": 4, "answer": "2% to 16%", "latency_ms": 451.446533203125, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2556}, "5": {"k": 5, "answer": "2% to 16%", "latency_ms": 264.7066116333008, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3261}, "6": {"k": 6, "answer": "2% to 16%", "latency_ms": 474.3528366088867, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3867}}}
{"qid": "7110705857253017304", "question": "when was the last time miss texas won miss america", "gold_answers": ["1975"], "shots": {"0": {"k": 0, "answer": "2018", "latency_ms": 342.76676177978516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1989", "latency_ms": 205.64723014831543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 573}, "2": {"k": 2, "answer": "1975", "latency_ms": 465.79790115356445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1178}, "3": {"k": 3, "answer": "1975", "latency_ms": 113.53611946105957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1756}, "4": {"k": 4, "answer": "1975", "latency_ms": 114.5329475402832, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2368}, "5": {"k": 5, "answer": "1975", "latency_ms": 149.9786376953125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2949}, "6": {"k": 6, "answer": "1975", "latency_ms": 170.65000534057617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3539}}}
{"qid": "-3653614166480549899", "question": "who sang first line of we are the world", "gold_answers": ["Billy Joel", "Stevie Wonder", "Lionel Richie", "James Ingram", "Tina Turner", "Paul Simon", "Kenny Rogers"], "shots": {"0": {"k": 0, "answer": "Michael Jackson", "latency_ms": 71.57111167907715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 252.65145301818848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 605}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 155.7772159576416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1175}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 133.5892677307129, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1769}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 271.98004722595215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2356}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 199.5096206665039, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2941}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 228.96170616149902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3537}}}
{"qid": "-6284503594240226071", "question": "who formed and first came to the colony of maryland", "gold_answers": ["the English"], "shots": {"0": {"k": 0, "answer": "Lord Baltimore", "latency_ms": 137.6054286956787, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lord Baltimore", "latency_ms": 97.22447395324707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "English Lord Baltimore", "latency_ms": 145.20263671875, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1302}, "3": {"k": 3, "answer": "Cecil Calvert", "latency_ms": 297.5590229034424, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1992}, "4": {"k": 4, "answer": "Cecil Calvert and Leonard Calvert", "latency_ms": 304.0313720703125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2603}, "5": {"k": 5, "answer": "Cecilius Calvert", "latency_ms": 460.0098133087158, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3254}, "6": {"k": 6, "answer": "Cecilius Calvert", "latency_ms": 343.07360649108887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3916}}}
{"qid": "6948592513354589991", "question": "who has played in the most masters tournaments", "gold_answers": ["Gary Player"], "shots": {"0": {"k": 0, "answer": "Tiger Woods", "latency_ms": 186.5980625152588, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 260.2429389953613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 648}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 92.70787239074707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1298}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 392.29750633239746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1972}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 215.97623825073242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2632}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 133.70156288146973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3302}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 174.6070384979248, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3997}}}
{"qid": "3311962143974666464", "question": "how much venom can a king cobra produce", "gold_answers": ["200 to 500 mg", "420 mg", "up to 7 ml"], "shots": {"0": {"k": 0, "answer": "100 milliliters", "latency_ms": 122.0705509185791, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1.31 mg/kg (intravenous) to 1.93 mg/kg (subcutaneous)", "latency_ms": 639.1255855560303, "em": 0.0, "f1": 0.18181818181818182, "containment": 0.0, "context_chars": 618}, "2": {"k": 2, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 367.56348609924316, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1240}, "3": {"k": 3, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 540.5855178833008, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1878}, "4": {"k": 4, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 667.874813079834, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2466}, "5": {"k": 5, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 555.0615787506104, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3105}, "6": {"k": 6, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 580.6941986083984, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3765}}}
{"qid": "4829683075369336935", "question": "last team to win fa cup outside top flight", "gold_answers": ["Arsenal", "West Ham United"], "shots": {"0": {"k": 0, "answer": "Barnsley 2003", "latency_ms": 260.41698455810547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "West Bromwich (1931)", "latency_ms": 181.13422393798828, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 578}, "2": {"k": 2, "answer": "West Ham United", "latency_ms": 171.25177383422852, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1166}, "3": {"k": 3, "answer": "West Ham United", "latency_ms": 151.32713317871094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1758}, "4": {"k": 4, "answer": "West Ham United", "latency_ms": 286.11063957214355, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2356}, "5": {"k": 5, "answer": "West Ham United", "latency_ms": 296.8623638153076, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2944}, "6": {"k": 6, "answer": "West Ham United", "latency_ms": 320.44005393981934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3536}}}
{"qid": "947113025953164512", "question": "where did you go to drink during prohibition", "gold_answers": ["Speakeasies"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 179.55756187438965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 84.87820625305176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 674}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 130.26809692382812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1395}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 191.70522689819336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2113}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 232.4681282043457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2806}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 193.53556632995605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3481}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 231.3978672027588, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4165}}}
{"qid": "2162587275527723755", "question": "where does the edinburgh fringe festival take place", "gold_answers": ["in Edinburgh , Scotland"], "shots": {"0": {"k": 0, "answer": "Edinburgh, Scotland", "latency_ms": 212.36467361450195, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Edinburgh, Scotland", "latency_ms": 238.4335994720459, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 694}, "2": {"k": 2, "answer": "Edinburgh, Scotland", "latency_ms": 421.97346687316895, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1379}, "3": {"k": 3, "answer": "Edinburgh, Scotland", "latency_ms": 271.04878425598145, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2052}, "4": {"k": 4, "answer": "Edinburgh, Scotland", "latency_ms": 388.05651664733887, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2728}, "5": {"k": 5, "answer": "Edinburgh, Scotland", "latency_ms": 257.584810256958, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3406}, "6": {"k": 6, "answer": "Edinburgh, Scotland", "latency_ms": 301.8174171447754, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 4109}}}
{"qid": "1875157261182375834", "question": "who won the wwe championship elimination chamber 2018", "gold_answers": ["Roman Reigns"], "shots": {"0": {"k": 0, "answer": "Braun Strowman", "latency_ms": 309.4673156738281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 80.88469505310059, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 163.1922721862793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1354}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 105.91006278991699, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2028}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 270.5087661743164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2688}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 284.41929817199707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3374}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 225.1589298248291, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4068}}}
{"qid": "8926902974073457645", "question": "where does safe haven take place in the book", "gold_answers": ["Southport , North Carolina"], "shots": {"0": {"k": 0, "answer": "Florida", "latency_ms": 66.03169441223145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Boston", "latency_ms": 71.8080997467041, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 601}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 119.11559104919434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1206}, "3": {"k": 3, "answer": "Southport, North Carolina", "latency_ms": 180.60922622680664, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1848}, "4": {"k": 4, "answer": "Southport, North Carolina", "latency_ms": 165.64512252807617, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2440}, "5": {"k": 5, "answer": "Southport, North Carolina", "latency_ms": 366.93620681762695, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3091}, "6": {"k": 6, "answer": "Southport, North Carolina", "latency_ms": 239.4113540649414, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3725}}}
{"qid": "8445703091969850957", "question": "who came up with the term baby boomer", "gold_answers": ["The Washington Post"], "shots": {"0": {"k": 0, "answer": "David A. Halpern", "latency_ms": 139.75000381469727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 84.43069458007812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 103.89995574951172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1299}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 272.6268768310547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1908}, "4": {"k": 4, "answer": "Sylvia F. Porter", "latency_ms": 381.13975524902344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2521}, "5": {"k": 5, "answer": "Daily Press", "latency_ms": 103.37209701538086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3180}, "6": {"k": 6, "answer": "Daily Press", "latency_ms": 140.09428024291992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3861}}}
{"qid": "-2961860751699827965", "question": "who was elected president of the united states in 1928", "gold_answers": ["Herbert Hoover"], "shots": {"0": {"k": 0, "answer": "Herbert Hoover", "latency_ms": 112.67447471618652, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Franklin D. Roosevelt", "latency_ms": 266.2489414215088, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 669}, "2": {"k": 2, "answer": "Herbert Hoover", "latency_ms": 84.05208587646484, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1306}, "3": {"k": 3, "answer": "Herbert Hoover", "latency_ms": 93.9035415649414, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2012}, "4": {"k": 4, "answer": "Herbert Hoover", "latency_ms": 376.89995765686035, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2728}, "5": {"k": 5, "answer": "Herbert Hoover", "latency_ms": 226.78732872009277, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3465}, "6": {"k": 6, "answer": "Herbert Hoover", "latency_ms": 183.24947357177734, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4170}}}
{"qid": "-3997272399784107764", "question": "dogs name in the grinch who stole christmas", "gold_answers": ["Max"], "shots": {"0": {"k": 0, "answer": "Max", "latency_ms": 40.29583930969238, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Max", "latency_ms": 134.02795791625977, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 643}, "2": {"k": 2, "answer": "Max", "latency_ms": 58.02750587463379, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1285}, "3": {"k": 3, "answer": "Max", "latency_ms": 71.23303413391113, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1967}, "4": {"k": 4, "answer": "Max", "latency_ms": 120.87845802307129, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2615}, "5": {"k": 5, "answer": "Max", "latency_ms": 259.2296600341797, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3269}, "6": {"k": 6, "answer": "Max", "latency_ms": 99.01309013366699, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3871}}}
{"qid": "119543419987074002", "question": "who played anne in anne with an e", "gold_answers": ["Amybeth McNulty"], "shots": {"0": {"k": 0, "answer": "Sasha Lane", "latency_ms": 176.93662643432617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 158.0369472503662, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 146.6045379638672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1212}, "3": {"k": 3, "answer": "Mary Miles Minter", "latency_ms": 125.31423568725586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1816}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 112.88595199584961, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2412}, "5": {"k": 5, "answer": "Mary Miles Minter", "latency_ms": 155.897855758667, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3019}, "6": {"k": 6, "answer": "Helen Edmundson", "latency_ms": 143.27621459960938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3676}}}
{"qid": "4362450141225960365", "question": "what was going on in the world in 1900", "gold_answers": ["Boxer Rebellion", "U.S. population exceeds 75 million", "Galveston hurricane"], "shots": {"0": {"k": 0, "answer": "Industrialization, imperialism, and the rise of global powers", "latency_ms": 547.9493141174316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Women beginning limited military roles in some countries", "latency_ms": 205.38663864135742, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "Women beginning to serve in limited military roles; no liberal democracies with universal suffrage; 19 constitutional monarchies", "latency_ms": 747.6236820220947, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1250}, "3": {"k": 3, "answer": "Women beginning to serve in limited military roles; no liberal democracies with universal suffrage; 19 constitutional monarchies", "latency_ms": 619.2314624786377, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1855}, "4": {"k": 4, "answer": "Women in some countries began serving in limited military roles; no liberal democracies with universal suffrage existed globally; Majestic-class battleships served in foreign navies and the British fleet.", "latency_ms": 1048.8042831420898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2502}, "5": {"k": 5, "answer": "Women beginning to serve in limited military roles; no liberal democracies with universal suffrage; 19 constitutional monarchies; childbirth in hospitals rare; Majestic-class battleships active; World War I not yet begun.", "latency_ms": 980.5655479431152, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3133}, "6": {"k": 6, "answer": "Women beginning to serve in limited military roles; no liberal democracies with universal suffrage; Majestic-class battleships serving in foreign navies; childbirth in hospitals increasing; hospital births in U.S. at 5%", "latency_ms": 1011.2154483795166, "em": 0.0, "f1": 0.05714285714285714, "containment": 0.0, "context_chars": 3699}}}
{"qid": "-599106694350296477", "question": "theme of the song roar by katy perry", "gold_answers": ["standing up for oneself", "self - empowerment"], "shots": {"0": {"k": 0, "answer": "Self-empowerment and confidence", "latency_ms": 180.4487705230713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "self-empowerment", "latency_ms": 202.64506340026855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "self-empowerment", "latency_ms": 162.43219375610352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "self-empowerment", "latency_ms": 259.17840003967285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1865}, "4": {"k": 4, "answer": "self-empowerment", "latency_ms": 480.4577827453613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2540}, "5": {"k": 5, "answer": "self-empowerment", "latency_ms": 196.16365432739258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3165}, "6": {"k": 6, "answer": "self-empowerment", "latency_ms": 264.83607292175293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3772}}}
{"qid": "6341281025332348318", "question": "who picks the players in the nfl draft", "gold_answers": ["each team"], "shots": {"0": {"k": 0, "answer": "NFL teams", "latency_ms": 90.96002578735352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "NFL franchises", "latency_ms": 58.28976631164551, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 576}, "2": {"k": 2, "answer": "NFL franchises", "latency_ms": 116.90211296081543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1175}, "3": {"k": 3, "answer": "NFL franchises", "latency_ms": 103.91020774841309, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1822}, "4": {"k": 4, "answer": "NFL franchises", "latency_ms": 212.48173713684082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2440}, "5": {"k": 5, "answer": "NFL franchises", "latency_ms": 178.95960807800293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3074}, "6": {"k": 6, "answer": "NFL franchises", "latency_ms": 184.36861038208008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3714}}}
{"qid": "1656636013570720778", "question": "who discovered gold in the sierra nevada of california", "gold_answers": ["James W. Marshall"], "shots": {"0": {"k": 0, "answer": "James W. Marshall", "latency_ms": 165.54832458496094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 290.8928394317627, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "James W. Marshall", "latency_ms": 206.15649223327637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1300}, "3": {"k": 3, "answer": "James W. Marshall", "latency_ms": 384.64879989624023, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1962}, "4": {"k": 4, "answer": "Sutter", "latency_ms": 147.810697555542, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2552}, "5": {"k": 5, "answer": "Sutter", "latency_ms": 170.13144493103027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3138}, "6": {"k": 6, "answer": "Sutter", "latency_ms": 281.419038772583, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3789}}}
{"qid": "6797662554091549033", "question": "which mirror is used in vehicles for rear view", "gold_answers": ["rear - view mirror"], "shots": {"0": {"k": 0, "answer": "Convex mirror", "latency_ms": 96.63081169128418, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "rear-view mirror", "latency_ms": 149.73211288452148, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "rear-view mirror", "latency_ms": 113.74092102050781, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1233}, "3": {"k": 3, "answer": "rear-view mirror", "latency_ms": 164.51334953308105, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1897}, "4": {"k": 4, "answer": "rear-view mirror", "latency_ms": 198.5609531402588, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2528}, "5": {"k": 5, "answer": "rear-view mirror", "latency_ms": 129.51254844665527, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3126}, "6": {"k": 6, "answer": "rear-view mirror", "latency_ms": 155.1647186279297, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3768}}}
{"qid": "7788386234127027369", "question": "how many medals did austria win in the 2011 alpine skiing world championships", "gold_answers": ["8"], "shots": {"0": {"k": 0, "answer": "10 medals", "latency_ms": 79.53953742980957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 112.78963088989258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 579}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 126.2044906616211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1243}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 270.69759368896484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1859}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 186.58971786499023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2513}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 164.52622413635254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3163}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 223.25825691223145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3850}}}
{"qid": "647605647914971565", "question": "who wrote lyrics for phantom of the opera", "gold_answers": ["Richard Stilgoe", "Charles Hart and Richard Stilgoe", "Charles Hart"], "shots": {"0": {"k": 0, "answer": "Charles Hart", "latency_ms": 79.46443557739258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Charles Hart and Richard Stilgoe, and additional lyrics by Mike Batt", "latency_ms": 682.1959018707275, "em": 0.0, "f1": 0.625, "containment": 1.0, "context_chars": 634}, "2": {"k": 2, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 419.2016124725342, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1253}, "3": {"k": 3, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 229.91394996643066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1844}, "4": {"k": 4, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 322.22437858581543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2508}, "5": {"k": 5, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 210.94894409179688, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3165}, "6": {"k": 6, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 644.2606449127197, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3840}}}
{"qid": "-7980433566693172177", "question": "what culture region is germany a part of", "gold_answers": ["Central Germany"], "shots": {"0": {"k": 0, "answer": "Central Europe", "latency_ms": 363.0938529968262, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Central Germany", "latency_ms": 87.76164054870605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 629}, "2": {"k": 2, "answer": "Central Germany (cultural area)", "latency_ms": 332.6139450073242, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1277}, "3": {"k": 3, "answer": "Central Germany (cultural area)", "latency_ms": 175.74405670166016, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1923}, "4": {"k": 4, "answer": "Central Germany", "latency_ms": 195.4514980316162, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2568}, "5": {"k": 5, "answer": "Central Germany (cultural area)", "latency_ms": 416.11647605895996, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3222}, "6": {"k": 6, "answer": "Central Germany", "latency_ms": 139.2216682434082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3933}}}
{"qid": "6268706985209815767", "question": "mount and blade with fire and sword time period", "gold_answers": ["1648 - 51"], "shots": {"0": {"k": 0, "answer": "11th to 13th century", "latency_ms": 449.17964935302734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "post-medieval era", "latency_ms": 141.63804054260254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "post-medieval era", "latency_ms": 313.55977058410645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1312}, "3": {"k": 3, "answer": "post-medieval era", "latency_ms": 371.3717460632324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1954}, "4": {"k": 4, "answer": "1648–1651", "latency_ms": 229.8448085784912, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2631}, "5": {"k": 5, "answer": "1648–1651", "latency_ms": 324.4609832763672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3248}, "6": {"k": 6, "answer": "1648–1651", "latency_ms": 262.2411251068115, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3845}}}
