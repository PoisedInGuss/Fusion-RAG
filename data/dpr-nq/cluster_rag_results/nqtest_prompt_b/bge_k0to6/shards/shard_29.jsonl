{"qid": "3012568508229512715", "question": "when does like cage season 2 come out", "gold_answers": ["June 22 , 2018"], "shots": {"0": {"k": 0, "answer": "2024", "latency_ms": 317.4402713775635, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "June 21, 2018", "latency_ms": 373.1224536895752, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "June 22, 2018", "latency_ms": 526.9348621368408, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1266}, "3": {"k": 3, "answer": "June 22, 2018", "latency_ms": 503.3571720123291, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1948}, "4": {"k": 4, "answer": "June 22, 2018", "latency_ms": 406.91423416137695, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2582}, "5": {"k": 5, "answer": "June 22, 2018", "latency_ms": 515.5918598175049, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3233}, "6": {"k": 6, "answer": "June 22, 2018", "latency_ms": 591.8107032775879, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3935}}}
{"qid": "839669511198247322", "question": "types of skiing in the winter olympics 2018", "gold_answers": ["Super-G", "Slalom -- ( SC )", "Downhill", "Giant slalom", "Slalom"], "shots": {"0": {"k": 0, "answer": "Alpine, Cross-Country, Biathlon, Freestyle, Snowboard", "latency_ms": 582.9076766967773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Alpine skiing", "latency_ms": 132.8125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "Alpine skiing, mixed team event", "latency_ms": 710.6528282165527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Alpine skiing, mixed team event", "latency_ms": 299.69048500061035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1937}, "4": {"k": 4, "answer": "Alpine skiing, cross-country skiing", "latency_ms": 181.25009536743164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2604}, "5": {"k": 5, "answer": "Alpine skiing, cross-country skiing, ski jumping", "latency_ms": 377.06971168518066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3246}, "6": {"k": 6, "answer": "Alpine, cross-country, ski jumping, freestyle", "latency_ms": 541.1415100097656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3901}}}
{"qid": "-5881961149303861412", "question": "who played gino in a place to call home", "gold_answers": ["Aldo Mignone"], "shots": {"0": {"k": 0, "answer": "Derek Luke", "latency_ms": 216.53294563293457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 273.8480567932129, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 139.15729522705078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1311}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 193.9690113067627, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1949}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 248.31151962280273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2572}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 342.27538108825684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3248}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 361.4048957824707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3944}}}
{"qid": "-9077495306870424234", "question": "who did the 49ers beat in the super bowls", "gold_answers": ["San Diego Chargers", "Miami Dolphins", "Denver Broncos", "Cincinnati Bengals"], "shots": {"0": {"k": 0, "answer": "Pittsburgh Steelers", "latency_ms": 121.01173400878906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "New York Giants", "latency_ms": 82.35740661621094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 552}, "2": {"k": 2, "answer": "New York Giants in Super Bowl XIX", "latency_ms": 266.9222354888916, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1199}, "3": {"k": 3, "answer": "Baltimore Ravens", "latency_ms": 267.8701877593994, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1882}, "4": {"k": 4, "answer": "New York Giants in Super Bowl XIX", "latency_ms": 512.3698711395264, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2523}, "5": {"k": 5, "answer": "Miami Dolphins", "latency_ms": 136.2740993499756, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3172}, "6": {"k": 6, "answer": "Miami Dolphins", "latency_ms": 151.84617042541504, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3776}}}
{"qid": "-5623229691925051533", "question": "when was the south asian association for regional co-operation (saarc) formed", "gold_answers": ["December 1985", "8 December 1985"], "shots": {"0": {"k": 0, "answer": "1985", "latency_ms": 317.08478927612305, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "8 December 1985", "latency_ms": 504.6536922454834, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 727}, "2": {"k": 2, "answer": "8 December 1985", "latency_ms": 486.33837699890137, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1447}, "3": {"k": 3, "answer": "8 December 1985", "latency_ms": 138.86761665344238, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2144}, "4": {"k": 4, "answer": "8 December 1985", "latency_ms": 406.34775161743164, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2861}, "5": {"k": 5, "answer": "8 December 1985", "latency_ms": 164.78443145751953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3557}, "6": {"k": 6, "answer": "8 December 1985", "latency_ms": 493.5312271118164, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4315}}}
{"qid": "8707879945343660137", "question": "what hangs from the trees in the bayou", "gold_answers": ["Spanish moss"], "shots": {"0": {"k": 0, "answer": "cypress knees", "latency_ms": 166.5492057800293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "moss-covered cypress trees", "latency_ms": 251.21688842773438, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 635}, "2": {"k": 2, "answer": "moss-covered cypress trees", "latency_ms": 384.28497314453125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1343}, "3": {"k": 3, "answer": "moss-covered cypress trees", "latency_ms": 385.2837085723877, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1987}, "4": {"k": 4, "answer": "moss-covered cypress trees", "latency_ms": 191.3294792175293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2635}, "5": {"k": 5, "answer": "cypress knees", "latency_ms": 182.04832077026367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3264}, "6": {"k": 6, "answer": "cypress knees", "latency_ms": 302.095890045166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3949}}}
{"qid": "-731256923033768782", "question": "new york heart association functional classification of angina", "gold_answers": ["II"], "shots": {"0": {"k": 0, "answer": "NYHA Class I-IV", "latency_ms": 108.85238647460938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "NYHA Functional Classification", "latency_ms": 174.77130889892578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 730}, "2": {"k": 2, "answer": "NYHA Functional Classification", "latency_ms": 226.28092765808105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1494}, "3": {"k": 3, "answer": "NYHA Functional Classification", "latency_ms": 194.68092918395996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2250}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 281.8775177001953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2959}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 359.1043949127197, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3658}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 257.8887939453125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4437}}}
{"qid": "-5350413673278085389", "question": "who wrote the song would i lie to you", "gold_answers": ["Mike Leeson", "Peter Vale"], "shots": {"0": {"k": 0, "answer": "Tommy Roe", "latency_ms": 135.95032691955566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Mike Leeson and Peter Vale", "latency_ms": 149.45030212402344, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 583}, "2": {"k": 2, "answer": "Mike Leeson and Peter Vale", "latency_ms": 333.24170112609863, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1219}, "3": {"k": 3, "answer": "Mike Leeson and Peter Vale", "latency_ms": 357.2819232940674, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1856}, "4": {"k": 4, "answer": "Mike Leeson and Peter Vale", "latency_ms": 311.511754989624, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2455}, "5": {"k": 5, "answer": "Mike Leeson and Peter Vale", "latency_ms": 227.1289825439453, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3060}, "6": {"k": 6, "answer": "Mike Leeson and Peter Vale", "latency_ms": 360.4259490966797, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3712}}}
{"qid": "-6190173184813176768", "question": "number of films in lord of the rings", "gold_answers": ["three"], "shots": {"0": {"k": 0, "answer": "3", "latency_ms": 48.148155212402344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "3", "latency_ms": 49.89123344421387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 641}, "2": {"k": 2, "answer": "3", "latency_ms": 103.8522720336914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "3", "latency_ms": 72.06511497497559, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1882}, "4": {"k": 4, "answer": "3", "latency_ms": 184.9231719970703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2643}, "5": {"k": 5, "answer": "3", "latency_ms": 90.22212028503418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3299}, "6": {"k": 6, "answer": "3", "latency_ms": 192.94476509094238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3927}}}
{"qid": "8239463385147663167", "question": "is jaffa cakes a biscuit or a cake", "gold_answers": ["cakes", "Cake", "biscuit - sized cakes"], "shots": {"0": {"k": 0, "answer": "biscuit", "latency_ms": 94.78092193603516, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "biscuit-sized cake", "latency_ms": 104.8884391784668, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 604}, "2": {"k": 2, "answer": "biscuit-sized cake", "latency_ms": 196.35510444641113, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1184}, "3": {"k": 3, "answer": "a cake", "latency_ms": 123.15583229064941, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1765}, "4": {"k": 4, "answer": "cake", "latency_ms": 255.56087493896484, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2460}, "5": {"k": 5, "answer": "cake", "latency_ms": 156.04734420776367, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3101}, "6": {"k": 6, "answer": "cake", "latency_ms": 191.67852401733398, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3681}}}
{"qid": "756385103791298508", "question": "who does the voice of stewie family guy", "gold_answers": ["Seth MacFarlane"], "shots": {"0": {"k": 0, "answer": "Dan Castellaneta", "latency_ms": 220.0174331665039, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "John Swartzwelder", "latency_ms": 98.24562072753906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 675}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 215.60144424438477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1399}, "3": {"k": 3, "answer": "Rachael MacFarlane", "latency_ms": 242.49958992004395, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2065}, "4": {"k": 4, "answer": "Seth MacFarlane", "latency_ms": 122.64275550842285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2738}, "5": {"k": 5, "answer": "Seth MacFarlane", "latency_ms": 396.75068855285645, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3407}, "6": {"k": 6, "answer": "Seth MacFarlane", "latency_ms": 313.48133087158203, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4062}}}
{"qid": "7355457342276623885", "question": "what drug does the mom use in requiem for a dream", "gold_answers": ["weight - loss amphetamines", "amphetamines"], "shots": {"0": {"k": 0, "answer": "cocaine", "latency_ms": 117.47264862060547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "amphetamines", "latency_ms": 153.55825424194336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 609}, "2": {"k": 2, "answer": "amphetamines", "latency_ms": 104.48002815246582, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1254}, "3": {"k": 3, "answer": "amphetamines", "latency_ms": 117.36273765563965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1831}, "4": {"k": 4, "answer": "amphetamines", "latency_ms": 116.40810966491699, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2445}, "5": {"k": 5, "answer": "amphetamines", "latency_ms": 165.37117958068848, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3085}, "6": {"k": 6, "answer": "amphetamines", "latency_ms": 232.94973373413086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3708}}}
{"qid": "-251770629620567718", "question": "how long was dark side of the moon number 1", "gold_answers": ["42 : 49", "a week"], "shots": {"0": {"k": 0, "answer": "19 minutes 38 seconds", "latency_ms": 137.3579502105713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "14 years", "latency_ms": 71.80666923522949, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 601}, "2": {"k": 2, "answer": "14 years", "latency_ms": 111.50074005126953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1214}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 116.38283729553223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1800}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 193.56131553649902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2412}, "5": {"k": 5, "answer": "1 week", "latency_ms": 152.3611545562744, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3023}, "6": {"k": 6, "answer": "1 week", "latency_ms": 204.5454978942871, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3656}}}
{"qid": "3227997019205769806", "question": "where is the snooker world open currently held", "gold_answers": ["Hainan International Exhibition Center", "Haikou"], "shots": {"0": {"k": 0, "answer": "London, UK", "latency_ms": 224.35998916625977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 219.21396255493164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 98.1283187866211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1286}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 207.96895027160645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1933}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 250.8385181427002, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2562}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 214.09201622009277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3223}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 252.8371810913086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3836}}}
{"qid": "8499576329103738172", "question": "who is the actor that plays jt on the young and the restless", "gold_answers": ["Thaddeus Rowe Luckinbill"], "shots": {"0": {"k": 0, "answer": "Joshua \"JT\" Williams", "latency_ms": 453.7544250488281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Thad Luckinbill", "latency_ms": 288.19966316223145, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 623}, "2": {"k": 2, "answer": "Thad Luckinbill", "latency_ms": 130.6746006011963, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Thad Luckinbill", "latency_ms": 186.62285804748535, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1898}, "4": {"k": 4, "answer": "Thad Luckinbill", "latency_ms": 213.37437629699707, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2485}, "5": {"k": 5, "answer": "Thad Luckinbill", "latency_ms": 345.5688953399658, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3103}, "6": {"k": 6, "answer": "Thad Luckinbill", "latency_ms": 349.78175163269043, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3708}}}
{"qid": "4230593664190705298", "question": "when was the first commercial cell phone released", "gold_answers": ["1983", "1973"], "shots": {"0": {"k": 0, "answer": "1973", "latency_ms": 334.05613899230957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "early 1990s", "latency_ms": 114.12358283996582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 666}, "2": {"k": 2, "answer": "1990s", "latency_ms": 186.73944473266602, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1359}, "3": {"k": 3, "answer": "1973", "latency_ms": 229.40850257873535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2001}, "4": {"k": 4, "answer": "1983", "latency_ms": 192.78335571289062, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2690}, "5": {"k": 5, "answer": "1983", "latency_ms": 160.20679473876953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3320}, "6": {"k": 6, "answer": "1983", "latency_ms": 326.6470432281494, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3903}}}
{"qid": "2241516377343726249", "question": "who sings the theme song for degrassi the next generation", "gold_answers": ["The Degrassi Junior Strings", "Jakalope"], "shots": {"0": {"k": 0, "answer": "Lil' Kim", "latency_ms": 155.0149917602539, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dave Ogilvie, Anthony Valcic, Jakalope, Damhnait Doyle, in-show band", "latency_ms": 596.5211391448975, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 645}, "2": {"k": 2, "answer": "Dave Ogilvie, Anthony Valcic, Jakalope, Damhnait Doyle, Alexz Johnson", "latency_ms": 631.9787502288818, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "Dave Ogilvie, Anthony Valcic, Damhnait Doyle, Alexz Johnson, Jakalope", "latency_ms": 567.7886009216309, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 1947}, "4": {"k": 4, "answer": "Dave Ogilvie, Anthony Valcic, Damhnait Doyle, Alexz Johnson, Jim McGrath, Shobha", "latency_ms": 719.0477848052979, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2598}, "5": {"k": 5, "answer": "Jakalope", "latency_ms": 210.43157577514648, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3209}, "6": {"k": 6, "answer": "Jakalope", "latency_ms": 347.7170467376709, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3935}}}
{"qid": "8179100082271836759", "question": "when did the first ice age come out", "gold_answers": ["March 15 , 2002", "2002"], "shots": {"0": {"k": 0, "answer": "2.6 million years ago", "latency_ms": 282.2606563568115, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 224.83062744140625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 627}, "2": {"k": 2, "answer": "360 million years ago", "latency_ms": 316.6537284851074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1276}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 191.47706031799316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1919}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 339.4758701324463, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2576}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 202.12793350219727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3192}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 162.5654697418213, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3871}}}
{"qid": "-1946900884888632776", "question": "what is a e8 in the marine corps", "gold_answers": ["Master sergeant", "First sergeant"], "shots": {"0": {"k": 0, "answer": "E-8 in the Marine Corps is a rank of Colonel.", "latency_ms": 412.45508193969727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 133.03756713867188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 732}, "2": {"k": 2, "answer": "E-8 in the Marine Corps is a senior enlisted rank, typically a Sergeant Major, providing technical leadership and serving as the senior enlisted member in a unit.", "latency_ms": 1857.593059539795, "em": 0.0, "f1": 0.08333333333333334, "containment": 0.0, "context_chars": 1425}, "3": {"k": 3, "answer": "E-8 is the rank of Sergeant Major in the Marine Corps.", "latency_ms": 890.8650875091553, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 2087}, "4": {"k": 4, "answer": "E-8 is the rank level of Sergeant Major in the United States Marine Corps.", "latency_ms": 389.96100425720215, "em": 0.0, "f1": 0.14285714285714285, "containment": 0.0, "context_chars": 2737}, "5": {"k": 5, "answer": "E-8 in the U.S. Marine Corps is a senior enlisted rank, typically held by a Sergeant Major, who provides technical leadership and serves as the senior enlisted Marine in a unit, assisting the commanding officer with discipline, administration, and morale.", "latency_ms": 2051.4869689941406, "em": 0.0, "f1": 0.05555555555555555, "containment": 0.0, "context_chars": 3440}, "6": {"k": 6, "answer": "E-8 is a pay grade with two ranks: Sergeant Major and First Sergeant, serving as senior enlisted Marines providing leadership in discipline, administration, and morale.", "latency_ms": 1176.5165328979492, "em": 0.0, "f1": 0.15384615384615385, "containment": 1.0, "context_chars": 4176}}}
{"qid": "8538836642175821278", "question": "who is given credit for the gnu initiative", "gold_answers": ["Richard Stallman"], "shots": {"0": {"k": 0, "answer": "Richard Stallman", "latency_ms": 79.47778701782227, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Richard Stallman", "latency_ms": 174.1158962249756, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 685}, "2": {"k": 2, "answer": "Richard Stallman", "latency_ms": 112.77651786804199, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1332}, "3": {"k": 3, "answer": "Richard Stallman", "latency_ms": 172.14035987854004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1981}, "4": {"k": 4, "answer": "Richard Stallman", "latency_ms": 231.7211627960205, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2594}, "5": {"k": 5, "answer": "Richard Stallman", "latency_ms": 336.66229248046875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3284}, "6": {"k": 6, "answer": "Richard Stallman", "latency_ms": 342.4978256225586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3933}}}
{"qid": "-170392909557030937", "question": "who won the 10m air pistol gold medal at commonwealth shooting championship in brisbane australia", "gold_answers": ["Shahzar Rizvi"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 274.6927738189697, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 92.75674819946289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 179.53133583068848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1218}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 110.01849174499512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1828}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 150.39348602294922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2433}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 179.28504943847656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3014}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 183.2897663116455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3641}}}
{"qid": "-7137073055537137289", "question": "who is the lead singer of collective soul", "gold_answers": ["Ed Roland"], "shots": {"0": {"k": 0, "answer": "Chris Layton", "latency_ms": 231.23908042907715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ed Roland", "latency_ms": 192.28315353393555, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 693}, "2": {"k": 2, "answer": "Ed Roland", "latency_ms": 128.4167766571045, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1369}, "3": {"k": 3, "answer": "Ed Roland", "latency_ms": 98.56581687927246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1997}, "4": {"k": 4, "answer": "Ed Roland", "latency_ms": 208.1918716430664, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2622}, "5": {"k": 5, "answer": "Ed Roland", "latency_ms": 151.9155502319336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3207}, "6": {"k": 6, "answer": "Ed Roland", "latency_ms": 258.791446685791, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3813}}}
{"qid": "7198381700174721586", "question": "who does dwyane wade play for right now", "gold_answers": ["the Miami Heat", "Miami Heat"], "shots": {"0": {"k": 0, "answer": "Miami Heat", "latency_ms": 145.4174518585205, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Miami Heat", "latency_ms": 206.12096786499023, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 619}, "2": {"k": 2, "answer": "Miami Heat", "latency_ms": 101.01819038391113, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1310}, "3": {"k": 3, "answer": "Miami Heat", "latency_ms": 88.19246292114258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1908}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 226.8044948577881, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2505}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 187.5591278076172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3072}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 258.61215591430664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3682}}}
{"qid": "-4036207256798544363", "question": "what is the value of the currency in paraguay", "gold_answers": ["126 PYG to 1 USD"], "shots": {"0": {"k": 0, "answer": "Guarani", "latency_ms": 83.9242935180664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "guarani (PYG)", "latency_ms": 237.8976345062256, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 656}, "2": {"k": 2, "answer": "guarani (PYG)", "latency_ms": 341.9506549835205, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1225}, "3": {"k": 3, "answer": "PYG", "latency_ms": 157.89294242858887, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 1823}, "4": {"k": 4, "answer": "guaran√≠ (PYG)", "latency_ms": 374.8626708984375, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2392}, "5": {"k": 5, "answer": "PYG", "latency_ms": 117.4459457397461, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 3030}, "6": {"k": 6, "answer": "PYG", "latency_ms": 156.19897842407227, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 3667}}}
{"qid": "222526478564019881", "question": "a drug that might be used in surgery for its amnesic properties is", "gold_answers": ["benzodiazepines", "Benzodiazepines"], "shots": {"0": {"k": 0, "answer": "ketamine", "latency_ms": 152.62579917907715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "benzodiazepines", "latency_ms": 380.30171394348145, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 678}, "2": {"k": 2, "answer": "benzodiazepines", "latency_ms": 129.05120849609375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1397}, "3": {"k": 3, "answer": "benzodiazepines", "latency_ms": 129.82892990112305, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2036}, "4": {"k": 4, "answer": "benzodiazepines", "latency_ms": 301.5856742858887, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2771}, "5": {"k": 5, "answer": "benzodiazepines", "latency_ms": 174.53670501708984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3496}, "6": {"k": 6, "answer": "benzodiazepines", "latency_ms": 283.8480472564697, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4207}}}
{"qid": "-9216522514171777028", "question": "a single period of precession of earth's axis is completed in about", "gold_answers": ["26,000 years", "approximately 26,000 years"], "shots": {"0": {"k": 0, "answer": "26,000 years", "latency_ms": 301.5012741088867, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "26,000 years", "latency_ms": 346.77815437316895, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 626}, "2": {"k": 2, "answer": "26,000 years", "latency_ms": 391.92700386047363, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1249}, "3": {"k": 3, "answer": "26,000 years", "latency_ms": 294.5899963378906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1840}, "4": {"k": 4, "answer": "26,000 years", "latency_ms": 344.60926055908203, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2468}, "5": {"k": 5, "answer": "26,000 years", "latency_ms": 240.67306518554688, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3095}, "6": {"k": 6, "answer": "26,000 years", "latency_ms": 165.8167839050293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3720}}}
{"qid": "8838750614698973059", "question": "who did johnny manziel play college football for", "gold_answers": ["Texas A&M University", "Texas A&M Aggies", "Texas A&M"], "shots": {"0": {"k": 0, "answer": "Texas A&M", "latency_ms": 89.02239799499512, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Texas A&M University", "latency_ms": 262.418270111084, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 618}, "2": {"k": 2, "answer": "Texas A&M University", "latency_ms": 173.07591438293457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1239}, "3": {"k": 3, "answer": "Texas A&M University", "latency_ms": 164.44754600524902, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1894}, "4": {"k": 4, "answer": "Texas A&M University", "latency_ms": 260.3793144226074, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2518}, "5": {"k": 5, "answer": "Texas A&M University", "latency_ms": 160.8595848083496, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3165}, "6": {"k": 6, "answer": "Texas A&M University", "latency_ms": 164.58773612976074, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3823}}}
{"qid": "7579233308152571691", "question": "natural resources can be divided into what two categories", "gold_answers": ["Biotic", "Abiotic"], "shots": {"0": {"k": 0, "answer": "Renewable and non-renewable", "latency_ms": 156.56256675720215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "renewable and non-renewable", "latency_ms": 475.85320472717285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 697}, "2": {"k": 2, "answer": "renewable and non-renewable", "latency_ms": 258.20469856262207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1403}, "3": {"k": 3, "answer": "renewable and non-renewable", "latency_ms": 379.08291816711426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2201}, "4": {"k": 4, "answer": "renewable and non-renewable", "latency_ms": 358.64925384521484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2932}, "5": {"k": 5, "answer": "renewable and non-renewable", "latency_ms": 263.34095001220703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3579}, "6": {"k": 6, "answer": "renewable and non-renewable", "latency_ms": 246.3853359222412, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4356}}}
{"qid": "1106103350744797710", "question": "what is the white stork a symbol for in western culture", "gold_answers": ["bringing babies to new parents"], "shots": {"0": {"k": 0, "answer": "Good luck and prosperity", "latency_ms": 119.06814575195312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "parental devotion", "latency_ms": 176.832914352417, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 640}, "2": {"k": 2, "answer": "parental devotion", "latency_ms": 148.34332466125488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1230}, "3": {"k": 3, "answer": "purity and maternal devotion", "latency_ms": 230.1616668701172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1879}, "4": {"k": 4, "answer": "parental devotion, purity, mercy, and child-bringing", "latency_ms": 548.403263092041, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2522}, "5": {"k": 5, "answer": "child-bringing myth, purity, parental devotion, national bird, cultural presence", "latency_ms": 582.6067924499512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3189}, "6": {"k": 6, "answer": "child-birth, purity, parental devotion, mercy, self-conceit", "latency_ms": 360.522985458374, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3795}}}
{"qid": "-8361853086037854880", "question": "india south africa test series 2018 highest runs", "gold_answers": ["AB de Villiers", "286"], "shots": {"0": {"k": 0, "answer": "Rohit Sharma", "latency_ms": 79.55384254455566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 119.30060386657715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 584}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 96.62413597106934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1203}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 126.37042999267578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1834}, "4": {"k": 4, "answer": "558", "latency_ms": 113.01565170288086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2447}, "5": {"k": 5, "answer": "558", "latency_ms": 117.6915168762207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3084}, "6": {"k": 6, "answer": "558", "latency_ms": 281.2371253967285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3681}}}
{"qid": "-5202207990213031495", "question": "the oligodynamic effect is a phenomenon that describes", "gold_answers": ["a biocidal effect of metals"], "shots": {"0": {"k": 0, "answer": "killing of microorganisms by low concentrations of metals", "latency_ms": 238.88087272644043, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "biocidal effect of metals in low concentrations", "latency_ms": 154.9825668334961, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 715}, "2": {"k": 2, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 381.65974617004395, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 1412}, "3": {"k": 3, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 698.8868713378906, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 2074}, "4": {"k": 4, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 367.8908348083496, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 2740}, "5": {"k": 5, "answer": "the biocidal effect of metals, especially heavy metals, in low concentrations", "latency_ms": 687.9088878631592, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3478}, "6": {"k": 6, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 877.4082660675049, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 4146}}}
{"qid": "6391370451004919214", "question": "who was last person to be executed in us", "gold_answers": ["Ruben Cardenas Ramirez"], "shots": {"0": {"k": 0, "answer": "2006", "latency_ms": 235.90326309204102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Rainey Bethea", "latency_ms": 182.09075927734375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 634}, "2": {"k": 2, "answer": "Rainey Bethea", "latency_ms": 375.016450881958, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1263}, "3": {"k": 3, "answer": "Rainey Bethea", "latency_ms": 198.38333129882812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1874}, "4": {"k": 4, "answer": "Rainey Bethea", "latency_ms": 230.79586029052734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2532}, "5": {"k": 5, "answer": "Rainey Bethea", "latency_ms": 307.80816078186035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3156}, "6": {"k": 6, "answer": "Rainey Bethea", "latency_ms": 244.26865577697754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3776}}}
{"qid": "-8069963735453146166", "question": "when do you declare honors in contract bridge", "gold_answers": ["any time after the auction"], "shots": {"0": {"k": 0, "answer": "After the opening lead and before the first trick is played.", "latency_ms": 193.25900077819824, "em": 0.0, "f1": 0.15384615384615383, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 143.48387718200684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 614}, "2": {"k": 2, "answer": "at the conclusion of play", "latency_ms": 345.4480171203613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1166}, "3": {"k": 3, "answer": "at the conclusion of play", "latency_ms": 124.33719635009766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1736}, "4": {"k": 4, "answer": "at the conclusion of play", "latency_ms": 223.79446029663086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2342}, "5": {"k": 5, "answer": "at the conclusion of play", "latency_ms": 359.5285415649414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2924}, "6": {"k": 6, "answer": "at the conclusion of play", "latency_ms": 297.49059677124023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3618}}}
{"qid": "-2154502691082554319", "question": "which country is the last member of saarc", "gold_answers": ["Afghanistan"], "shots": {"0": {"k": 0, "answer": "Sri Lanka", "latency_ms": 115.35358428955078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Afghanistan", "latency_ms": 259.859561920166, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 699}, "2": {"k": 2, "answer": "Afghanistan", "latency_ms": 231.48226737976074, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1416}, "3": {"k": 3, "answer": "Afghanistan", "latency_ms": 144.53697204589844, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2145}, "4": {"k": 4, "answer": "Afghanistan", "latency_ms": 293.23530197143555, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2851}, "5": {"k": 5, "answer": "Afghanistan", "latency_ms": 305.7365417480469, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3547}, "6": {"k": 6, "answer": "Afghanistan", "latency_ms": 172.23405838012695, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4271}}}
{"qid": "-8498955431733322253", "question": "describe the three phases of the normal blood clotting process", "gold_answers": ["blood coagulation", "vasoconstriction ,"], "shots": {"0": {"k": 0, "answer": "Vascular spasm, platelet plug formation, coagulation", "latency_ms": 180.46855926513672, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "vasoconstriction, platelet plug, fibrin clot", "latency_ms": 371.8700408935547, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 684}, "2": {"k": 2, "answer": "vasoconstriction, platelet plug formation, coagulation", "latency_ms": 447.60799407958984, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": "vasoconstriction, platelet plug formation, coagulation", "latency_ms": 673.8102436065674, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 1994}, "4": {"k": 4, "answer": "vasoconstriction, platelet plug formation, coagulation", "latency_ms": 894.5212364196777, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 2663}, "5": {"k": 5, "answer": "Vasoconstriction, platelet plug formation, coagulation", "latency_ms": 502.31242179870605, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 3321}, "6": {"k": 6, "answer": "Vasoconstriction, platelet plug formation, blood coagulation", "latency_ms": 289.0291213989258, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 4042}}}
{"qid": "7392498509104714323", "question": "who played sonny in lemony snicket's a series of unfortunate events", "gold_answers": ["Presley Smith"], "shots": {"0": {"k": 0, "answer": "Jesse McCartney", "latency_ms": 251.9841194152832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 312.3817443847656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 624}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 202.20184326171875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1324}, "3": {"k": 3, "answer": "Presley Smith", "latency_ms": 113.66438865661621, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2016}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 326.0071277618408, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2700}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 264.4383907318115, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3306}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 320.7721710205078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4000}}}
{"qid": "2911493370352980057", "question": "when was united nations convention on the rights of the child created", "gold_answers": ["20 November 1989"], "shots": {"0": {"k": 0, "answer": "1989", "latency_ms": 244.1883087158203, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "20 November 1989", "latency_ms": 206.3605785369873, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 664}, "2": {"k": 2, "answer": "20 November 1989", "latency_ms": 190.26565551757812, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1324}, "3": {"k": 3, "answer": "20 November 1989", "latency_ms": 300.872802734375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2012}, "4": {"k": 4, "answer": "20 November 1989", "latency_ms": 480.73363304138184, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2594}, "5": {"k": 5, "answer": "20 November 1989", "latency_ms": 413.4230613708496, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3212}, "6": {"k": 6, "answer": "20 November 1989", "latency_ms": 234.5128059387207, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3865}}}
{"qid": "2495228512593253768", "question": "when did we decide to leave the eu", "gold_answers": ["23 June 2016"], "shots": {"0": {"k": 0, "answer": "2016", "latency_ms": 101.66645050048828, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "29 March 2017", "latency_ms": 295.3212261199951, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 623}, "2": {"k": 2, "answer": "23 June 2016", "latency_ms": 368.55149269104004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1294}, "3": {"k": 3, "answer": "23 June 2016", "latency_ms": 274.3039131164551, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1923}, "4": {"k": 4, "answer": "23 June 2016", "latency_ms": 368.410587310791, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2504}, "5": {"k": 5, "answer": "23 June 2016", "latency_ms": 579.7510147094727, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3099}, "6": {"k": 6, "answer": "23 June 2016", "latency_ms": 349.43532943725586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3773}}}
{"qid": "95636137517606056", "question": "tallest building in the world of all time", "gold_answers": ["Burj Khalifa"], "shots": {"0": {"k": 0, "answer": "Burj Khalifa", "latency_ms": 111.33503913879395, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Burj Khalifa", "latency_ms": 336.86089515686035, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 639}, "2": {"k": 2, "answer": "Burj Khalifa", "latency_ms": 164.9937629699707, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1320}, "3": {"k": 3, "answer": "Burj Khalifa", "latency_ms": 183.36033821105957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2001}, "4": {"k": 4, "answer": "Burj Khalifa", "latency_ms": 324.9211311340332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2688}, "5": {"k": 5, "answer": "Burj Khalifa", "latency_ms": 136.61932945251465, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3291}, "6": {"k": 6, "answer": "Burj Khalifa", "latency_ms": 426.24783515930176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3982}}}
{"qid": "-4020097033891103749", "question": "who plays chummy's mother in call the midwife", "gold_answers": ["Cheryl Campbell"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 103.30963134765625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 122.18046188354492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 664}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 145.07246017456055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1319}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 144.08016204833984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1940}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 111.98902130126953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2578}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 351.7475128173828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3162}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 226.95612907409668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3742}}}
{"qid": "-7690316790133138931", "question": "who created separation of powers and checks and balances", "gold_answers": ["Montesquieu in the Enlightenment"], "shots": {"0": {"k": 0, "answer": "Montesquieu", "latency_ms": 130.02490997314453, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Montesquieu", "latency_ms": 119.75336074829102, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 647}, "2": {"k": 2, "answer": "Montesquieu", "latency_ms": 96.6796875, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1348}, "3": {"k": 3, "answer": "Montesquieu", "latency_ms": 134.8423957824707, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1992}, "4": {"k": 4, "answer": "Montesquieu", "latency_ms": 182.1274757385254, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2714}, "5": {"k": 5, "answer": "Montesquieu", "latency_ms": 350.53110122680664, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3352}, "6": {"k": 6, "answer": "Montesquieu", "latency_ms": 349.7295379638672, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 4054}}}
{"qid": "914815550191007566", "question": "who was the nfl first draft pick 2017", "gold_answers": ["Myles Garrett"], "shots": {"0": {"k": 0, "answer": "Kyler Murray", "latency_ms": 265.5832767486572, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 158.26916694641113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 678}, "2": {"k": 2, "answer": "12th overall pick: Deshaun Watson", "latency_ms": 450.06704330444336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1319}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 271.6207504272461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1948}, "4": {"k": 4, "answer": "No first draft pick mentioned in context.", "latency_ms": 249.4971752166748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2585}, "5": {"k": 5, "answer": "No answer provided in context.", "latency_ms": 427.40845680236816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3213}, "6": {"k": 6, "answer": "No answer provided in context.", "latency_ms": 189.32271003723145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3758}}}
{"qid": "972568347460262100", "question": "when was nepal declared a secular state in bs", "gold_answers": ["January 15 , 2007"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 292.4826145172119, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 94.27428245544434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 714}, "2": {"k": 2, "answer": "2008", "latency_ms": 333.67347717285156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1412}, "3": {"k": 3, "answer": "2008", "latency_ms": 157.52243995666504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2124}, "4": {"k": 4, "answer": "2008", "latency_ms": 347.8426933288574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2877}, "5": {"k": 5, "answer": "2008", "latency_ms": 282.76515007019043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3522}, "6": {"k": 6, "answer": "2008", "latency_ms": 262.5386714935303, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4163}}}
{"qid": "-4845827823184041115", "question": "who took control of jamestown and made new rules that made everyone work", "gold_answers": ["John Smith"], "shots": {"0": {"k": 0, "answer": "Captain John Smith", "latency_ms": 244.63653564453125, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "John Smith", "latency_ms": 182.74497985839844, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 623}, "2": {"k": 2, "answer": "John Smith", "latency_ms": 157.07731246948242, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1261}, "3": {"k": 3, "answer": "John Smith", "latency_ms": 134.6874237060547, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1923}, "4": {"k": 4, "answer": "John Smith", "latency_ms": 91.05610847473145, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2569}, "5": {"k": 5, "answer": "John Smith", "latency_ms": 168.91932487487793, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3241}, "6": {"k": 6, "answer": "John Smith", "latency_ms": 104.9802303314209, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3966}}}
{"qid": "7592863385884727299", "question": "the site of secretion of h+ and cl-", "gold_answers": ["Parietal cells"], "shots": {"0": {"k": 0, "answer": "parietal cells", "latency_ms": 165.07720947265625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "parietal cells", "latency_ms": 435.0762367248535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 625}, "2": {"k": 2, "answer": "parietal cells", "latency_ms": 175.87041854858398, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1212}, "3": {"k": 3, "answer": "parietal cells in the stomach fundus region", "latency_ms": 427.77562141418457, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1838}, "4": {"k": 4, "answer": "parietal cells of the gastric mucosa", "latency_ms": 332.9658508300781, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2495}, "5": {"k": 5, "answer": "parietal cells in the stomach fundus region", "latency_ms": 274.6155261993408, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3126}, "6": {"k": 6, "answer": "parietal cells in the stomach fundus region", "latency_ms": 437.56675720214844, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3870}}}
{"qid": "-217684522847197793", "question": "when does body temperature tend to be lowest", "gold_answers": ["11 p.m. to 3 a.m."], "shots": {"0": {"k": 0, "answer": "between 2 and 4 a.m.", "latency_ms": 212.30554580688477, "em": 0.0, "f1": 0.20000000000000004, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "4 a.m.", "latency_ms": 191.75148010253906, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "4 a.m.", "latency_ms": 306.6554069519043, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1301}, "3": {"k": 3, "answer": "4 a.m.", "latency_ms": 205.9488296508789, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1975}, "4": {"k": 4, "answer": "4 a.m.", "latency_ms": 181.67495727539062, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2610}, "5": {"k": 5, "answer": "4 a.m.", "latency_ms": 211.52234077453613, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3264}, "6": {"k": 6, "answer": "4 a.m.", "latency_ms": 187.64328956604004, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3895}}}
{"qid": "3926678456788602613", "question": "what us state forms the western boundary of montana", "gold_answers": ["Idaho"], "shots": {"0": {"k": 0, "answer": "Idaho", "latency_ms": 53.14445495605469, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 87.65101432800293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 673}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 229.353666305542, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1332}, "3": {"k": 3, "answer": "Idaho", "latency_ms": 182.02996253967285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1926}, "4": {"k": 4, "answer": "Idaho", "latency_ms": 212.09335327148438, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2599}, "5": {"k": 5, "answer": "Idaho", "latency_ms": 156.4791202545166, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3235}, "6": {"k": 6, "answer": "Idaho", "latency_ms": 249.32146072387695, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3903}}}
{"qid": "-444951262186580688", "question": "what is the pirates of the caribbean in order", "gold_answers": ["At World 's End", "Dead Men Tell No Tales", "Dead Man 's Chest", "On Stranger Tides"], "shots": {"0": {"k": 0, "answer": "1. The Curse of the Black Pearl  \n2. Dead Man's Chest  \n3. At World's End  \n4. On Stranger Tides  \n5. Dead Men Tell No Tales", "latency_ms": 837.3260498046875, "em": 0.0, "f1": 0.3571428571428571, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 81.04777336120605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 223.9229679107666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1271}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 213.98043632507324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1835}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 427.02269554138184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2520}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 132.57551193237305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3151}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 251.4951229095459, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3775}}}
{"qid": "-822810697742332337", "question": "if a piece of music is perceived to have changed key then we say the piece has", "gold_answers": ["transposed", "transposition"], "shots": {"0": {"k": 0, "answer": "modulated", "latency_ms": 113.93427848815918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "changed key", "latency_ms": 58.07948112487793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 674}, "2": {"k": 2, "answer": "changed key", "latency_ms": 63.37618827819824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1429}, "3": {"k": 3, "answer": "changed key", "latency_ms": 83.54544639587402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2096}, "4": {"k": 4, "answer": "modulated", "latency_ms": 235.2607250213623, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2741}, "5": {"k": 5, "answer": "modulated", "latency_ms": 104.12144660949707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3351}, "6": {"k": 6, "answer": "modulated", "latency_ms": 160.85076332092285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3963}}}
{"qid": "-4663296615598634791", "question": "who was allowed to vote in the roman republic", "gold_answers": ["unknown", "citizens"], "shots": {"0": {"k": 0, "answer": "Roman citizens of certain wealth and status", "latency_ms": 198.10986518859863, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Roman citizens", "latency_ms": 58.16340446472168, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 672}, "2": {"k": 2, "answer": "Full Roman citizens (excluded women, slaves, and originally those living outside of Rome)", "latency_ms": 561.1910820007324, "em": 0.0, "f1": 0.14285714285714288, "containment": 1.0, "context_chars": 1311}, "3": {"k": 3, "answer": "Adult male citizens", "latency_ms": 159.3911647796631, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1991}, "4": {"k": 4, "answer": "Adult male Roman citizens", "latency_ms": 226.21679306030273, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2685}, "5": {"k": 5, "answer": "Adult male Roman citizens", "latency_ms": 232.65528678894043, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3347}, "6": {"k": 6, "answer": "Adult male Roman citizens", "latency_ms": 154.68859672546387, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 4058}}}
{"qid": "3695813920971622504", "question": "when does the next warrior book come out", "gold_answers": ["April 10 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 114.44616317749023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 104.5982837677002, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 635}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 234.57694053649902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1503}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 371.26684188842773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2163}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 317.26813316345215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2807}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 246.48547172546387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3421}, "6": {"k": 6, "answer": "9 April 2019", "latency_ms": 493.57080459594727, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 4119}}}
{"qid": "8533079479108336590", "question": "blood alcohol concentration means the parts of alcohol in the blood in relation to what", "gold_answers": ["volume of blood", "ethanol"], "shots": {"0": {"k": 0, "answer": "blood volume", "latency_ms": 245.64552307128906, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "blood volume", "latency_ms": 104.26878929138184, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "volume of blood", "latency_ms": 250.61726570129395, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1302}, "3": {"k": 3, "answer": "volume of blood", "latency_ms": 168.24889183044434, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1899}, "4": {"k": 4, "answer": "volume of blood", "latency_ms": 157.4993133544922, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2514}, "5": {"k": 5, "answer": "volume of blood", "latency_ms": 158.2202911376953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3172}, "6": {"k": 6, "answer": "volume of blood", "latency_ms": 198.6064910888672, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3889}}}
{"qid": "-1531676396763282931", "question": "which greek god flew too close to the sun", "gold_answers": ["Icarus"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 97.61238098144531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 97.39351272583008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Pha√´thon", "latency_ms": 166.57066345214844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1200}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 133.69035720825195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1736}, "4": {"k": 4, "answer": "Phaethon", "latency_ms": 113.57355117797852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2351}, "5": {"k": 5, "answer": "Phaethon", "latency_ms": 126.6016960144043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2921}, "6": {"k": 6, "answer": "Phaethon", "latency_ms": 302.04319953918457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3539}}}
{"qid": "-3682350378604306871", "question": "when did the song rapper's delight come out", "gold_answers": ["September 16 , 1979", "1979"], "shots": {"0": {"k": 0, "answer": "1979", "latency_ms": 197.95846939086914, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 16, 1979", "latency_ms": 620.5823421478271, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 615}, "2": {"k": 2, "answer": "September 16, 1979", "latency_ms": 427.20675468444824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1195}, "3": {"k": 3, "answer": "1979", "latency_ms": 234.93266105651855, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1851}, "4": {"k": 4, "answer": "1979", "latency_ms": 217.30732917785645, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2464}, "5": {"k": 5, "answer": "1979", "latency_ms": 249.62401390075684, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3123}, "6": {"k": 6, "answer": "1979", "latency_ms": 260.52379608154297, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3752}}}
{"qid": "-3646291394492263644", "question": "when does the stock market open in hong kong", "gold_answers": ["09 : 15"], "shots": {"0": {"k": 0, "answer": "9:30 AM Hong Kong Time", "latency_ms": 125.65779685974121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 89.34307098388672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "09:00 am", "latency_ms": 223.26016426086426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1242}, "3": {"k": 3, "answer": "09:00 am", "latency_ms": 349.32541847229004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1900}, "4": {"k": 4, "answer": "09:00 am", "latency_ms": 222.95403480529785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2548}, "5": {"k": 5, "answer": "09:00 am", "latency_ms": 340.817928314209, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3177}, "6": {"k": 6, "answer": "9:00 am", "latency_ms": 290.738582611084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3849}}}
{"qid": "9187994979510737197", "question": "who's the model on let's make a deal", "gold_answers": ["Tiffany Adams Coyne"], "shots": {"0": {"k": 0, "answer": "Monty Hall", "latency_ms": 70.47438621520996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Tiffany Coyne", "latency_ms": 232.70893096923828, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 574}, "2": {"k": 2, "answer": "Tiffany Coyne", "latency_ms": 223.48809242248535, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1202}, "3": {"k": 3, "answer": "Tiffany Coyne", "latency_ms": 266.6008472442627, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1812}, "4": {"k": 4, "answer": "Tiffany Coyne", "latency_ms": 290.13657569885254, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2398}, "5": {"k": 5, "answer": "Tiffany Coyne", "latency_ms": 319.7770118713379, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3025}, "6": {"k": 6, "answer": "Tiffany Coyne", "latency_ms": 347.42259979248047, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3639}}}
{"qid": "5078709962400196312", "question": "what mlb teams did deion sanders play for", "gold_answers": ["San Francisco Giants", "Cincinnati Reds", "Atlanta Braves", "New York Yankees"], "shots": {"0": {"k": 0, "answer": "Atlanta Braves, New York Mets", "latency_ms": 408.5712432861328, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Atlanta Braves", "latency_ms": 60.12296676635742, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 609}, "2": {"k": 2, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 499.07469749450684, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1250}, "3": {"k": 3, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 633.5430145263672, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1856}, "4": {"k": 4, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 322.03173637390137, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 2499}, "5": {"k": 5, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 632.7483654022217, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3075}, "6": {"k": 6, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 354.36367988586426, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3684}}}
{"qid": "-4910305402988079742", "question": "how much money did the film titanic make", "gold_answers": ["$2.18 billion", "$2.187 billion"], "shots": {"0": {"k": 0, "answer": "$2.2 billion", "latency_ms": 152.94909477233887, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "$343.4 million", "latency_ms": 283.9975357055664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 624}, "2": {"k": 2, "answer": "$1.84 billion", "latency_ms": 284.8360538482666, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1256}, "3": {"k": 3, "answer": "$1.84 billion", "latency_ms": 170.2744960784912, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1914}, "4": {"k": 4, "answer": "$1.84 billion", "latency_ms": 326.9033432006836, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2507}, "5": {"k": 5, "answer": "$1,843,201,268", "latency_ms": 784.8773002624512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3176}, "6": {"k": 6, "answer": "$2.187 billion", "latency_ms": 385.8072757720947, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3827}}}
{"qid": "1636956235349274030", "question": "when does it's a wonderful life comes on", "gold_answers": ["Christmas season"], "shots": {"0": {"k": 0, "answer": "December 25", "latency_ms": 75.21700859069824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 121.08397483825684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 189.55349922180176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1249}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 111.13858222961426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1909}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 138.93604278564453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2535}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 265.7897472381592, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3177}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 176.20182037353516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3837}}}
{"qid": "5608421991404219229", "question": "where does the last name hansen come from", "gold_answers": ["the Faroe Islands", "Norway", "Denmark", "Scandinavian"], "shots": {"0": {"k": 0, "answer": "Denmark/Norway", "latency_ms": 198.4241008758545, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Denmark", "latency_ms": 136.54685020446777, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 686}, "2": {"k": 2, "answer": "Scandinavia", "latency_ms": 212.91446685791016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1342}, "3": {"k": 3, "answer": "Scandinavian patronymic surname, meaning \"son of Hans\"", "latency_ms": 518.9568996429443, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 1993}, "4": {"k": 4, "answer": "Scandinavian patronymic surname, meaning \"son of Hans\"", "latency_ms": 690.9112930297852, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 2621}, "5": {"k": 5, "answer": "Scandinavian patronymic surname, meaning \"son of Hans\"", "latency_ms": 269.22607421875, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 3233}, "6": {"k": 6, "answer": "Scandinavia", "latency_ms": 166.15796089172363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3858}}}
{"qid": "2577140274301537828", "question": "what is the symbol for christmas in brazil", "gold_answers": ["Nativity scene", "Christmas Tree"], "shots": {"0": {"k": 0, "answer": "papai noel", "latency_ms": 88.84167671203613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Puritina", "latency_ms": 154.39081192016602, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 652}, "2": {"k": 2, "answer": "Christmas Tree and Nativity scene", "latency_ms": 118.36862564086914, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1281}, "3": {"k": 3, "answer": "Christmas Tree and Nativity scene", "latency_ms": 495.34106254577637, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1949}, "4": {"k": 4, "answer": "Christmas tree and Nativity scene", "latency_ms": 370.2383041381836, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2595}, "5": {"k": 5, "answer": "Christmas tree and Nativity scene", "latency_ms": 162.06598281860352, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3270}, "6": {"k": 6, "answer": "Christmas tree and Nativity scene", "latency_ms": 243.7450885772705, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3908}}}
{"qid": "-6762654750812139940", "question": "who sings she's like the wind lyrics", "gold_answers": ["Wendy Fraser", "Patrick Swayze"], "shots": {"0": {"k": 0, "answer": "Linda Ronstadt", "latency_ms": 355.7770252227783, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 119.87042427062988, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 624}, "2": {"k": 2, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 357.0361137390137, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1249}, "3": {"k": 3, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 323.48132133483887, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1881}, "4": {"k": 4, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 333.1584930419922, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2475}, "5": {"k": 5, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 493.6234951019287, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3098}, "6": {"k": 6, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 481.41026496887207, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3723}}}
{"qid": "-3640066593401557276", "question": "where does the this is us family live", "gold_answers": ["Pittsburgh"], "shots": {"0": {"k": 0, "answer": "Rhode Island", "latency_ms": 261.27147674560547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Los Angeles, New Jersey", "latency_ms": 344.33531761169434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "Los Angeles, Pittsburgh, New Jersey", "latency_ms": 413.7599468231201, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1299}, "3": {"k": 3, "answer": "Los Angeles, Pittsburgh, New Jersey", "latency_ms": 275.6161689758301, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1910}, "4": {"k": 4, "answer": "Los Angeles, New Jersey, Pittsburgh", "latency_ms": 360.07094383239746, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 2558}, "5": {"k": 5, "answer": "Los Angeles, Pittsburgh, New Jersey", "latency_ms": 528.17702293396, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3223}, "6": {"k": 6, "answer": "Los Angeles, Pittsburgh, New Jersey", "latency_ms": 503.08752059936523, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3885}}}
{"qid": "-4546405080702371934", "question": "when was the term prime minister first used", "gold_answers": ["18th century", "1624"], "shots": {"0": {"k": 0, "answer": "1827", "latency_ms": 316.5469169616699, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 122.4818229675293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "1901", "latency_ms": 180.8493137359619, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1319}, "3": {"k": 3, "answer": "1869", "latency_ms": 177.30069160461426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1987}, "4": {"k": 4, "answer": "18th century", "latency_ms": 220.31188011169434, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2651}, "5": {"k": 5, "answer": "1727", "latency_ms": 183.8700771331787, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3319}, "6": {"k": 6, "answer": "1727", "latency_ms": 256.3750743865967, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3987}}}
{"qid": "3653635084331615973", "question": "who won the mens single ice skating 2018", "gold_answers": ["Javier Fern√°ndez", "Shoma Uno", "Yuzuru Hanyu"], "shots": {"0": {"k": 0, "answer": "Yuzuru Hanyu", "latency_ms": 271.3308334350586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Yuzuru Hanyu", "latency_ms": 117.23732948303223, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 627}, "2": {"k": 2, "answer": "Yuzuru Hanyu", "latency_ms": 123.47817420959473, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1252}, "3": {"k": 3, "answer": "Yuzuru Hanyu", "latency_ms": 313.3707046508789, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1940}, "4": {"k": 4, "answer": "Yuzuru Hanyu", "latency_ms": 246.86050415039062, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2588}, "5": {"k": 5, "answer": "Yuzuru Hanyu", "latency_ms": 326.3564109802246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3188}, "6": {"k": 6, "answer": "Yuzuru Hanyu", "latency_ms": 328.7167549133301, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3798}}}
{"qid": "-8943432852681116531", "question": "what is cain and abel software used for", "gold_answers": ["password recovery"], "shots": {"0": {"k": 0, "answer": "Cain and Abel is a network scanning and password cracking tool used for ethical hacking and penetration testing.", "latency_ms": 752.9311180114746, "em": 0.0, "f1": 0.10526315789473684, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 165.03095626831055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 694}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 142.98486709594727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1385}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 105.96990585327148, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2023}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 314.72039222717285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2693}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 156.6324234008789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3384}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 383.41665267944336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4055}}}
{"qid": "3562729819386821776", "question": "what is the name of governor of maharashtra", "gold_answers": ["Chennamaneni Vidyasagar Rao"], "shots": {"0": {"k": 0, "answer": "Eknath Shinde", "latency_ms": 166.3045883178711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 82.94105529785156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "C. Vidyasagar Rao", "latency_ms": 158.57243537902832, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1299}, "3": {"k": 3, "answer": "C. Vidyasagar Rao", "latency_ms": 273.00429344177246, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1953}, "4": {"k": 4, "answer": "C. Vidyasagar Rao", "latency_ms": 207.7782154083252, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2593}, "5": {"k": 5, "answer": "C. Vidyasagar Rao", "latency_ms": 262.71510124206543, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3232}, "6": {"k": 6, "answer": "C. Vidyasagar Rao", "latency_ms": 384.5789432525635, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3873}}}
{"qid": "-2040762149245803825", "question": "what channel is abc on in phoenix arizona", "gold_answers": ["15"], "shots": {"0": {"k": 0, "answer": "ABC 5 on digital channel 5.1", "latency_ms": 323.319673538208, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "15", "latency_ms": 110.83650588989258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 603}, "2": {"k": 2, "answer": "15", "latency_ms": 130.99050521850586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "15", "latency_ms": 100.93474388122559, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1959}, "4": {"k": 4, "answer": "15", "latency_ms": 94.87724304199219, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2713}, "5": {"k": 5, "answer": "15", "latency_ms": 348.8774299621582, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3390}, "6": {"k": 6, "answer": "15", "latency_ms": 283.6127281188965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4022}}}
{"qid": "-1378070576053000718", "question": "who played jennifer in back to the future", "gold_answers": ["Claudia Grace Wells"], "shots": {"0": {"k": 0, "answer": "Michael J. Fox", "latency_ms": 218.0647850036621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Claudia Wells", "latency_ms": 144.78778839111328, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 597}, "2": {"k": 2, "answer": "Claudia Wells", "latency_ms": 256.0291290283203, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1207}, "3": {"k": 3, "answer": "Claudia Wells", "latency_ms": 212.68415451049805, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1829}, "4": {"k": 4, "answer": "Claudia Wells", "latency_ms": 238.3744716644287, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2477}, "5": {"k": 5, "answer": "Claudia Wells", "latency_ms": 283.22315216064453, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3062}, "6": {"k": 6, "answer": "Claudia Wells", "latency_ms": 160.2039337158203, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3667}}}
{"qid": "-8946826352712324685", "question": "where was the summer olympics held in 2012", "gold_answers": ["London", "London , United Kingdom"], "shots": {"0": {"k": 0, "answer": "London, UK", "latency_ms": 116.39404296875, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "London, United Kingdom", "latency_ms": 240.8747673034668, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 639}, "2": {"k": 2, "answer": "London, United Kingdom", "latency_ms": 185.8499050140381, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1244}, "3": {"k": 3, "answer": "London, United Kingdom", "latency_ms": 465.52348136901855, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1891}, "4": {"k": 4, "answer": "London, United Kingdom", "latency_ms": 257.52735137939453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2490}, "5": {"k": 5, "answer": "London, United Kingdom", "latency_ms": 165.12584686279297, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3105}, "6": {"k": 6, "answer": "London, United Kingdom", "latency_ms": 179.28504943847656, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3741}}}
{"qid": "-5578685052965147220", "question": "who cooks for the president of the united states", "gold_answers": ["Cristeta Comerford", "The White House Executive Chef", "White House Executive Chef"], "shots": {"0": {"k": 0, "answer": "The First Lady or designated chef", "latency_ms": 452.24785804748535, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "White House Executive Chef", "latency_ms": 233.33120346069336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 686}, "2": {"k": 2, "answer": "White House Executive Chef", "latency_ms": 144.96326446533203, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1369}, "3": {"k": 3, "answer": "White House Executive Chef", "latency_ms": 224.29919242858887, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2038}, "4": {"k": 4, "answer": "White House Executive Chef", "latency_ms": 468.23644638061523, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2656}, "5": {"k": 5, "answer": "White House Executive Chef", "latency_ms": 326.06029510498047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3325}, "6": {"k": 6, "answer": "White House Executive Chef", "latency_ms": 307.2679042816162, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3982}}}
{"qid": "-7323892214876884658", "question": "who was the first indian to be appointed as a judge in the international court of justice", "gold_answers": ["Sir Benegal Narsing Rau"], "shots": {"0": {"k": 0, "answer": "Justice M. H. Beg", "latency_ms": 194.90766525268555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 267.2708034515381, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 182.8329563140869, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1248}, "3": {"k": 3, "answer": "Benegal Narsing Rau", "latency_ms": 170.23372650146484, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 1954}, "4": {"k": 4, "answer": "Benegal Narsing Rau", "latency_ms": 147.93848991394043, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 2571}, "5": {"k": 5, "answer": "Benegal Narsing Rau", "latency_ms": 206.17914199829102, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 3200}, "6": {"k": 6, "answer": "Benegal Narsing Rau", "latency_ms": 238.9509677886963, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 3846}}}
{"qid": "-7530191989994326613", "question": "where was the film coming home for christmas filmed", "gold_answers": ["British Columbia"], "shots": {"0": {"k": 0, "answer": "Canada", "latency_ms": 54.924964904785156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "British Columbia", "latency_ms": 60.721635818481445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 656}, "2": {"k": 2, "answer": "British Columbia", "latency_ms": 96.68779373168945, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1301}, "3": {"k": 3, "answer": "Aldergrove, British Columbia", "latency_ms": 588.1478786468506, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1986}, "4": {"k": 4, "answer": "Aldergrove, British Columbia", "latency_ms": 500.72264671325684, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2592}, "5": {"k": 5, "answer": "Aldergrove, British Columbia", "latency_ms": 329.0114402770996, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3250}, "6": {"k": 6, "answer": "Aldergrove, British Columbia", "latency_ms": 410.2184772491455, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3850}}}
{"qid": "1036801998292429298", "question": "when do the new episodes of supernatural start", "gold_answers": ["May 10 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 397.9184627532959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "October 12, 2017", "latency_ms": 249.71771240234375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "October 11, 2018", "latency_ms": 181.34188652038574, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1322}, "3": {"k": 3, "answer": "October 11, 2018", "latency_ms": 312.0274543762207, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1949}, "4": {"k": 4, "answer": "October 11, 2018", "latency_ms": 403.78522872924805, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2601}, "5": {"k": 5, "answer": "October 11, 2018", "latency_ms": 503.8139820098877, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3233}, "6": {"k": 6, "answer": "October 11, 2018", "latency_ms": 257.335901260376, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3869}}}
{"qid": "-7296166998534064378", "question": "who played the middle sister on full house", "gold_answers": ["Jodie Sweetin"], "shots": {"0": {"k": 0, "answer": "Dana Delany", "latency_ms": 144.52099800109863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lori Loughlin", "latency_ms": 104.33006286621094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "Jodie Sweetin", "latency_ms": 97.6717472076416, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1282}, "3": {"k": 3, "answer": "Jodie Sweetin", "latency_ms": 190.42301177978516, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1885}, "4": {"k": 4, "answer": "Jodie Sweetin", "latency_ms": 403.8877487182617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2513}, "5": {"k": 5, "answer": "Lori Loughlin", "latency_ms": 424.70693588256836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3121}, "6": {"k": 6, "answer": "Lori Loughlin", "latency_ms": 384.2577934265137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3746}}}
{"qid": "-4173722566701480014", "question": "a 58-story skyscraper in san francisco is tilting and sinking", "gold_answers": ["Millennium Tower"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 321.2552070617676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 233.28852653503418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 614}, "2": {"k": 2, "answer": "Millennium Tower", "latency_ms": 230.1630973815918, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1266}, "3": {"k": 3, "answer": "Millennium Tower", "latency_ms": 245.23687362670898, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1867}, "4": {"k": 4, "answer": "Millennium Tower", "latency_ms": 288.8140678405762, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2553}, "5": {"k": 5, "answer": "Millennium Tower", "latency_ms": 193.9241886138916, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3248}, "6": {"k": 6, "answer": "Millennium Tower", "latency_ms": 370.2261447906494, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3905}}}
{"qid": "8741253661372783389", "question": "when was the first season of when calls the heart", "gold_answers": ["2014", "January 11 , 2014"], "shots": {"0": {"k": 0, "answer": "2014", "latency_ms": 255.86509704589844, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "2015", "latency_ms": 259.7463130950928, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 626}, "2": {"k": 2, "answer": "2014", "latency_ms": 161.66234016418457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1221}, "3": {"k": 3, "answer": "2014", "latency_ms": 207.40914344787598, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1889}, "4": {"k": 4, "answer": "January 11, 2014", "latency_ms": 816.0583972930908, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2525}, "5": {"k": 5, "answer": "January 11, 2014", "latency_ms": 570.0318813323975, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3161}, "6": {"k": 6, "answer": "January 11, 2014", "latency_ms": 643.4388160705566, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3787}}}
{"qid": "-2244798821441304446", "question": "who said one man's vulgarity is another's lyric", "gold_answers": ["Justice Harlan"], "shots": {"0": {"k": 0, "answer": "T.S. Eliot", "latency_ms": 239.68982696533203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 122.53785133361816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 616}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 142.3792839050293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1307}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 228.92403602600098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1945}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 227.0042896270752, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2559}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 231.22286796569824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3165}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 269.484281539917, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3812}}}
{"qid": "4764100575661627990", "question": "what football player is called the honey badger", "gold_answers": ["Tyrann Devine Mathieu"], "shots": {"0": {"k": 0, "answer": "Chris Berman", "latency_ms": 66.11943244934082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Tyrann Mathieu", "latency_ms": 102.49948501586914, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "Tyrann Mathieu", "latency_ms": 184.41271781921387, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Tyrann Mathieu", "latency_ms": 206.32672309875488, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1968}, "4": {"k": 4, "answer": "Tyrann Mathieu", "latency_ms": 313.0342960357666, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2618}, "5": {"k": 5, "answer": "Tyrann Mathieu", "latency_ms": 204.30660247802734, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3236}, "6": {"k": 6, "answer": "Tyrann Mathieu", "latency_ms": 351.5024185180664, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3928}}}
{"qid": "-2273381713182307888", "question": "how many episodes of the killing on netflix", "gold_answers": ["six", "6"], "shots": {"0": {"k": 0, "answer": "100 episodes", "latency_ms": 96.08578681945801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "6", "latency_ms": 87.03255653381348, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 660}, "2": {"k": 2, "answer": "six episodes", "latency_ms": 113.9836311340332, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1318}, "3": {"k": 3, "answer": "6", "latency_ms": 119.02928352355957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1926}, "4": {"k": 4, "answer": "6", "latency_ms": 110.18514633178711, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2633}, "5": {"k": 5, "answer": "6", "latency_ms": 130.6476593017578, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3306}, "6": {"k": 6, "answer": "6", "latency_ms": 141.2365436553955, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3923}}}
{"qid": "7214445857680379083", "question": "from whose perspective is the story of all quiet on the western front told", "gold_answers": ["Paul Baumer"], "shots": {"0": {"k": 0, "answer": "Paul Baumer's", "latency_ms": 76.40528678894043, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "soldiers' perspective", "latency_ms": 374.20082092285156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "Erich Maria Remarque", "latency_ms": 328.25279235839844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1261}, "3": {"k": 3, "answer": "Paul's perspective", "latency_ms": 185.97865104675293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1905}, "4": {"k": 4, "answer": "Erich Maria Remarque's perspective", "latency_ms": 316.1590099334717, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2492}, "5": {"k": 5, "answer": "Paul's perspective", "latency_ms": 254.37426567077637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3117}, "6": {"k": 6, "answer": "Paul's perspective", "latency_ms": 210.94346046447754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3670}}}
{"qid": "4905519627710399354", "question": "who was the person who escaped from alcatraz", "gold_answers": ["John Anglin", "Clarence Anglin", "Frank Morris"], "shots": {"0": {"k": 0, "answer": "Frank Morris and the Anglin brothers", "latency_ms": 168.73979568481445, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Morris and the Anglins", "latency_ms": 302.5028705596924, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Allen West", "latency_ms": 135.28704643249512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1175}, "3": {"k": 3, "answer": "Morris and the Anglins", "latency_ms": 275.51937103271484, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1858}, "4": {"k": 4, "answer": "John Paul Scott", "latency_ms": 146.96526527404785, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2497}, "5": {"k": 5, "answer": "John Paul Scott", "latency_ms": 116.80054664611816, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3121}, "6": {"k": 6, "answer": "John Paul Scott", "latency_ms": 218.91164779663086, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3717}}}
{"qid": "-5199162398326752157", "question": "where does trick or treat for unicef money go", "gold_answers": ["UNICEF 's global programing"], "shots": {"0": {"k": 0, "answer": "UNICEF programs worldwide", "latency_ms": 243.63279342651367, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "UNICEF", "latency_ms": 78.43351364135742, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 718}, "2": {"k": 2, "answer": "UNICEF delivers lifesaving therapeutic food to severely malnourished children around the world.", "latency_ms": 631.9925785064697, "em": 0.0, "f1": 0.13333333333333333, "containment": 0.0, "context_chars": 1376}, "3": {"k": 3, "answer": "lifesaving packets of therapeutic food to severely malnourished children around the world", "latency_ms": 425.5509376525879, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2106}, "4": {"k": 4, "answer": "UNICEF's global programing, including lifesaving therapeutic food for severely malnourished children.", "latency_ms": 837.7735614776611, "em": 0.0, "f1": 0.26666666666666666, "containment": 0.0, "context_chars": 2800}, "5": {"k": 5, "answer": "UNICEF's global programing, including lifesaving therapeutic food for severely malnourished children.", "latency_ms": 653.5019874572754, "em": 0.0, "f1": 0.26666666666666666, "containment": 0.0, "context_chars": 3488}, "6": {"k": 6, "answer": "UNICEF's global programing, including lifesaving therapeutic food for severely malnourished children around the world.", "latency_ms": 505.3069591522217, "em": 0.0, "f1": 0.2352941176470588, "containment": 0.0, "context_chars": 4121}}}
{"qid": "9200381329776083757", "question": "where is the inscription on the statue of liberty", "gold_answers": ["The New Colossus"], "shots": {"0": {"k": 0, "answer": "Statue's base in New York Harbor", "latency_ms": 128.3113956451416, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Inside the statue", "latency_ms": 68.66598129272461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "inside the pedestal of the Statue of Liberty", "latency_ms": 143.13817024230957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "Inside the pedestal of the Statue of Liberty", "latency_ms": 194.46945190429688, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1864}, "4": {"k": 4, "answer": "Inside the pedestal, on a plaque engraved with lines from \"The New Colossus\" by Emma Lazarus.", "latency_ms": 414.07084465026855, "em": 0.0, "f1": 0.2666666666666667, "containment": 1.0, "context_chars": 2421}, "5": {"k": 5, "answer": "Inside the pedestal, on a plaque engraved with lines from \"The New Colossus\" by Emma Lazarus.", "latency_ms": 1296.9765663146973, "em": 0.0, "f1": 0.2666666666666667, "containment": 1.0, "context_chars": 3013}, "6": {"k": 6, "answer": "Inside the pedestal, on a plaque engraved with lines from \"The New Colossus\" by Emma Lazarus.", "latency_ms": 662.5814437866211, "em": 0.0, "f1": 0.2666666666666667, "containment": 1.0, "context_chars": 3691}}}
{"qid": "9179702110514757818", "question": "where are the next two winter olympics going to be held", "gold_answers": ["Beijing", "TBD"], "shots": {"0": {"k": 0, "answer": "2026 Milano-Cortina, 2030 Beijing", "latency_ms": 513.2935047149658, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 245.4988956451416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 141.6463851928711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1236}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 319.0300464630127, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1838}, "4": {"k": 4, "answer": "Beijing 2022, Pyeongchang 2018", "latency_ms": 551.2499809265137, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2463}, "5": {"k": 5, "answer": "Beijing and Pyeongchang", "latency_ms": 456.5000534057617, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3064}, "6": {"k": 6, "answer": "Beijing (2022), then 2026 host to be selected on June 23, 2019", "latency_ms": 1222.7883338928223, "em": 0.0, "f1": 0.15384615384615385, "containment": 1.0, "context_chars": 3726}}}
{"qid": "-5410471527866417159", "question": "when did i'm coming out come out", "gold_answers": ["August 22 , 1980"], "shots": {"0": {"k": 0, "answer": "1984", "latency_ms": 261.77525520324707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "August 22, 1980", "latency_ms": 586.5170955657959, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 631}, "2": {"k": 2, "answer": "August 22, 1980", "latency_ms": 224.5335578918457, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1256}, "3": {"k": 3, "answer": "August 22, 1980", "latency_ms": 379.8794746398926, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1892}, "4": {"k": 4, "answer": "August 22, 1980", "latency_ms": 235.7769012451172, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2519}, "5": {"k": 5, "answer": "August 22, 1980", "latency_ms": 420.09472846984863, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3135}, "6": {"k": 6, "answer": "August 22, 1980", "latency_ms": 446.75707817077637, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3702}}}
{"qid": "-3621585537083644585", "question": "what type of bridge is the charles bridge", "gold_answers": ["bow bridge", "a bow bridge", "Stone"], "shots": {"0": {"k": 0, "answer": "Stone arch bridge", "latency_ms": 126.35326385498047, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "historic bridge", "latency_ms": 98.6318588256836, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 631}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 256.08038902282715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1311}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 379.69970703125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1978}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 253.89432907104492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2679}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 219.21086311340332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3335}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 303.8616180419922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3913}}}
{"qid": "-6934435686987287378", "question": "who makes the important government decisions in an autocracy", "gold_answers": ["one person"], "shots": {"0": {"k": 0, "answer": "The ruler or ruling party", "latency_ms": 263.3090019226074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "one person or a small elite", "latency_ms": 241.00852012634277, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 674}, "2": {"k": 2, "answer": "one person or a small elite", "latency_ms": 216.93658828735352, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1333}, "3": {"k": 3, "answer": "one person or a small elite", "latency_ms": 181.27179145812988, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1997}, "4": {"k": 4, "answer": "one person or a small elite", "latency_ms": 499.7141361236572, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2632}, "5": {"k": 5, "answer": "one person or a small elite", "latency_ms": 200.99925994873047, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3298}, "6": {"k": 6, "answer": "one person or a small elite", "latency_ms": 369.3056106567383, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3952}}}
{"qid": "-7488727646515698670", "question": "what was the main characteristic of post-world war ii american society", "gold_answers": ["strong economic growth"], "shots": {"0": {"k": 0, "answer": "Consumerism", "latency_ms": 169.96359825134277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Blue Social Model", "latency_ms": 324.216365814209, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "stable jobs, high school diploma sufficed, large labor unions, regulated prices, limited competition", "latency_ms": 679.7089576721191, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1368}, "3": {"k": 3, "answer": "stable jobs, high school diploma sufficed for stable career, large labor unions, low corporate competition, government regulation keeping prices down", "latency_ms": 508.87537002563477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2077}, "4": {"k": 4, "answer": "prosperity and stability", "latency_ms": 201.94387435913086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2839}, "5": {"k": 5, "answer": "prosperity, stability, and a strong work ethic", "latency_ms": 507.9345703125, "em": 0.0, "f1": 0.2222222222222222, "containment": 0.0, "context_chars": 3538}, "6": {"k": 6, "answer": "prosperity, stability, and a strong work ethic", "latency_ms": 378.09109687805176, "em": 0.0, "f1": 0.2222222222222222, "containment": 0.0, "context_chars": 4241}}}
{"qid": "5760101138584878549", "question": "which way does the earth orbit the sun", "gold_answers": ["counter clockwise direction", "counter clockwise"], "shots": {"0": {"k": 0, "answer": "Counterclockwise (as viewed from above the North Pole)", "latency_ms": 276.25060081481934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "counterclockwise", "latency_ms": 85.89768409729004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 595}, "2": {"k": 2, "answer": "counterclockwise", "latency_ms": 163.98310661315918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1147}, "3": {"k": 3, "answer": "counterclockwise", "latency_ms": 108.06727409362793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1755}, "4": {"k": 4, "answer": "counterclockwise", "latency_ms": 109.48729515075684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2362}, "5": {"k": 5, "answer": "counterclockwise", "latency_ms": 280.4996967315674, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2993}, "6": {"k": 6, "answer": "counterclockwise", "latency_ms": 228.10959815979004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3595}}}
{"qid": "-7003244893871973796", "question": "what is on top of us capitol building", "gold_answers": ["The Statue of Freedom"], "shots": {"0": {"k": 0, "answer": "Dome", "latency_ms": 237.92147636413574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "dome", "latency_ms": 109.60149765014648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 655}, "2": {"k": 2, "answer": "dome", "latency_ms": 148.62585067749023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1312}, "3": {"k": 3, "answer": "dome", "latency_ms": 252.671480178833, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1965}, "4": {"k": 4, "answer": "dome", "latency_ms": 87.45360374450684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2548}, "5": {"k": 5, "answer": "dome", "latency_ms": 183.8550567626953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3215}, "6": {"k": 6, "answer": "bronze eagle", "latency_ms": 177.34789848327637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3811}}}
{"qid": "-3587434056219625939", "question": "who did dwight howard play for last year", "gold_answers": ["Charlotte Hornets", "Atlanta Hawks"], "shots": {"0": {"k": 0, "answer": "Los Angeles Lakers", "latency_ms": 337.1107578277588, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 239.36033248901367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 588}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 207.57555961608887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1220}, "3": {"k": 3, "answer": "Washington Wizards", "latency_ms": 166.18680953979492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1859}, "4": {"k": 4, "answer": "Washington Wizards", "latency_ms": 218.22452545166016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2454}, "5": {"k": 5, "answer": "Washington Wizards", "latency_ms": 211.61198616027832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3053}, "6": {"k": 6, "answer": "Washington Wizards", "latency_ms": 274.2650508880615, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3657}}}
{"qid": "-6209397953683103736", "question": "who lasted the longest in the royal rumble", "gold_answers": ["Rey Mysterio"], "shots": {"0": {"k": 0, "answer": "Steve Austin", "latency_ms": 53.632259368896484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Greatest Royal Rumble", "latency_ms": 221.4367389678955, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "Mysterio", "latency_ms": 147.3703384399414, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1209}, "3": {"k": 3, "answer": "Mysterio", "latency_ms": 203.13549041748047, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1803}, "4": {"k": 4, "answer": "Mysterio", "latency_ms": 258.06212425231934, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2411}, "5": {"k": 5, "answer": "Mysterio", "latency_ms": 233.44779014587402, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3012}, "6": {"k": 6, "answer": "Mysterio", "latency_ms": 288.6216640472412, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3619}}}
{"qid": "-5424723408850352324", "question": "what is the largest man made lake in the us", "gold_answers": ["Lake Oahe"], "shots": {"0": {"k": 0, "answer": "Lake Mead", "latency_ms": 239.16959762573242, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lake of the Ozarks", "latency_ms": 100.80409049987793, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "Lake of the Ozarks", "latency_ms": 100.08764266967773, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "Lake of the Ozarks", "latency_ms": 316.5867328643799, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1901}, "4": {"k": 4, "answer": "Lake of the Ozarks", "latency_ms": 313.401460647583, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2566}, "5": {"k": 5, "answer": "Lake Mead", "latency_ms": 170.88818550109863, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3145}, "6": {"k": 6, "answer": "Lake Mead", "latency_ms": 281.5828323364258, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3752}}}
{"qid": "7225785319983716635", "question": "gabbar is back is a remake of which telugu movie", "gold_answers": ["Tagore"], "shots": {"0": {"k": 0, "answer": "Gabbar Is Back (2023) is a remake of the Telugu movie \"Gabbar Singh\" (2012).", "latency_ms": 1393.8357830047607, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 348.3424186706543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "Tagore", "latency_ms": 204.3294906616211, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1209}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 180.26471138000488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1860}, "4": {"k": 4, "answer": "Tagore", "latency_ms": 193.04680824279785, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2571}, "5": {"k": 5, "answer": "Tagore", "latency_ms": 171.97036743164062, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3232}, "6": {"k": 6, "answer": "Tagore", "latency_ms": 232.20515251159668, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3904}}}
{"qid": "8466196474705624263", "question": "who was running as vice president in 1984", "gold_answers": ["Ferraro", "Vice President George H.W. Bush", "Congresswoman Ferraro", "George H.W. Bush"], "shots": {"0": {"k": 0, "answer": "John Baker", "latency_ms": 199.53060150146484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Geraldine Ferraro", "latency_ms": 107.61761665344238, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 682}, "2": {"k": 2, "answer": "Geraldine Ferraro", "latency_ms": 174.63254928588867, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1375}, "3": {"k": 3, "answer": "Geraldine Ferraro", "latency_ms": 303.3452033996582, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2070}, "4": {"k": 4, "answer": "Geraldine Ferraro", "latency_ms": 166.76807403564453, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2787}, "5": {"k": 5, "answer": "Geraldine Ferraro", "latency_ms": 218.6601161956787, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3497}, "6": {"k": 6, "answer": "Geraldine Ferraro", "latency_ms": 200.8535861968994, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 4186}}}
{"qid": "7711594393122809900", "question": "where is the largest salt mine in the us", "gold_answers": ["Central New York", "Western New York"], "shots": {"0": {"k": 0, "answer": "Utah", "latency_ms": 210.63637733459473, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Hampton Corners, Town of Groveland", "latency_ms": 152.85420417785645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Hampton Corners, New York", "latency_ms": 229.9196720123291, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1346}, "3": {"k": 3, "answer": "Hampton Corners, New York", "latency_ms": 302.7994632720947, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1953}, "4": {"k": 4, "answer": "Hampton Corners, New York", "latency_ms": 275.8333683013916, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 2585}, "5": {"k": 5, "answer": "Hampton Corners, New York", "latency_ms": 222.0592498779297, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3201}, "6": {"k": 6, "answer": "Hampton Corners, New York", "latency_ms": 450.8659839630127, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3790}}}
{"qid": "-7011448454774306489", "question": "who took the first steps on the moon in 1969", "gold_answers": ["Neil Armstrong"], "shots": {"0": {"k": 0, "answer": "Neil Armstrong", "latency_ms": 104.31027412414551, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Neil Armstrong", "latency_ms": 97.53561019897461, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 565}, "2": {"k": 2, "answer": "Neil Armstrong", "latency_ms": 153.04112434387207, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1164}, "3": {"k": 3, "answer": "Neil Armstrong", "latency_ms": 212.14556694030762, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1754}, "4": {"k": 4, "answer": "Neil Armstrong", "latency_ms": 111.59229278564453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2371}, "5": {"k": 5, "answer": "Neil Armstrong", "latency_ms": 133.74710083007812, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2961}, "6": {"k": 6, "answer": "Neil Armstrong", "latency_ms": 257.2212219238281, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3623}}}
{"qid": "3125559316824073534", "question": "state with highest population growth rate in india 2011", "gold_answers": ["Dadra and Nagar Haveli", "Meghalaya"], "shots": {"0": {"k": 0, "answer": "Uttar Pradesh", "latency_ms": 173.96998405456543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Bihar", "latency_ms": 103.63578796386719, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "Meghalaya", "latency_ms": 243.62492561340332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1277}, "3": {"k": 3, "answer": "Meghalaya", "latency_ms": 275.9835720062256, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1927}, "4": {"k": 4, "answer": "Meghalaya", "latency_ms": 113.15608024597168, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2569}, "5": {"k": 5, "answer": "Meghalaya", "latency_ms": 154.96420860290527, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3223}, "6": {"k": 6, "answer": "Nagaland", "latency_ms": 231.89544677734375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3870}}}
{"qid": "400438603377436284", "question": "which city is called the manchester of north india", "gold_answers": ["Ahmedabad"], "shots": {"0": {"k": 0, "answer": "Lucknow", "latency_ms": 128.5698413848877, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 220.0312614440918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 626}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 146.75402641296387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1233}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 295.2299118041992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1842}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 173.42758178710938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2541}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 189.13006782531738, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3196}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 142.5769329071045, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3877}}}
