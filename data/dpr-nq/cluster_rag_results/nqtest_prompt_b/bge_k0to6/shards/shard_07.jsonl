{"qid": "5306300389470118812", "question": "what was the religion in persia before islam", "gold_answers": ["the Zoroastrian religion", "Zoroastrian"], "shots": {"0": {"k": 0, "answer": "Zoroastrianism", "latency_ms": 274.7511863708496, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Zoroastrianism", "latency_ms": 145.22433280944824, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 686}, "2": {"k": 2, "answer": "Zoroastrianism", "latency_ms": 365.30423164367676, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1396}, "3": {"k": 3, "answer": "Zoroastrianism", "latency_ms": 167.0083999633789, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 2068}, "4": {"k": 4, "answer": "Zoroastrianism", "latency_ms": 343.3041572570801, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 2746}, "5": {"k": 5, "answer": "Ancient Iranian religion", "latency_ms": 180.13310432434082, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3482}, "6": {"k": 6, "answer": "Zoroastrianism", "latency_ms": 357.4514389038086, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 4192}}}
{"qid": "4492088526674104172", "question": "why does kerosene oil rise up in the wick of lantern", "gold_answers": ["capillary action"], "shots": {"0": {"k": 0, "answer": "Capillary action", "latency_ms": 319.81730461120605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "capillary action", "latency_ms": 109.46464538574219, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 620}, "2": {"k": 2, "answer": "capillary action", "latency_ms": 154.44707870483398, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1208}, "3": {"k": 3, "answer": "capillary action", "latency_ms": 136.23714447021484, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1785}, "4": {"k": 4, "answer": "capillary action", "latency_ms": 201.99203491210938, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2402}, "5": {"k": 5, "answer": "capillary action", "latency_ms": 267.1520709991455, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3021}, "6": {"k": 6, "answer": "capillary action", "latency_ms": 363.2469177246094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3676}}}
{"qid": "-4373262132142058334", "question": "mention the chemical change that proinsulin undergo to be able to act as mature insulin", "gold_answers": ["post translational modification"], "shots": {"0": {"k": 0, "answer": "Proteolytic cleavage", "latency_ms": 368.59631538391113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "C-peptide cleavage", "latency_ms": 99.56836700439453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "C-peptide is cleaved", "latency_ms": 184.94009971618652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1359}, "3": {"k": 3, "answer": "C-peptide is cleaved", "latency_ms": 321.96950912475586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1967}, "4": {"k": 4, "answer": "C-peptide is cleaved by endoproteases (PC1, PC3, PC2)", "latency_ms": 757.7879428863525, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2516}, "5": {"k": 5, "answer": "C-peptide is cleaved by endoproteases (PC1, PC3, PC2)", "latency_ms": 737.5526428222656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3153}, "6": {"k": 6, "answer": "C-peptide is cleaved", "latency_ms": 152.26054191589355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3834}}}
{"qid": "-5298560410953979569", "question": "where does the sweetness of fruit come from", "gold_answers": ["Fructose , or fruit sugar", "Fructose"], "shots": {"0": {"k": 0, "answer": "Fructose", "latency_ms": 54.834842681884766, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 156.6321849822998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 654}, "2": {"k": 2, "answer": "sucrose", "latency_ms": 89.36190605163574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "sucrose and fructose", "latency_ms": 368.4401512145996, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1959}, "4": {"k": 4, "answer": "sucrose and fructose", "latency_ms": 384.56010818481445, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2636}, "5": {"k": 5, "answer": "sucrose and fructose", "latency_ms": 355.85880279541016, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3332}, "6": {"k": 6, "answer": "sugars such as sucrose, fructose, and glucose", "latency_ms": 710.8404636383057, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 4020}}}
{"qid": "4344648368832551136", "question": "an object that moves around an external axis is said to be", "gold_answers": ["orbit", "revolution or orbital revolution"], "shots": {"0": {"k": 0, "answer": "rotating", "latency_ms": 87.00966835021973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "revolving", "latency_ms": 58.73441696166992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "revolving", "latency_ms": 133.87298583984375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1186}, "3": {"k": 3, "answer": "revolving", "latency_ms": 148.84114265441895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1758}, "4": {"k": 4, "answer": "revolving", "latency_ms": 134.05942916870117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2325}, "5": {"k": 5, "answer": "revolving", "latency_ms": 172.0139980316162, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2938}, "6": {"k": 6, "answer": "revolving", "latency_ms": 115.5245304107666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3603}}}
{"qid": "-724426540432437711", "question": "what was the name of the plant on little shop of horrors", "gold_answers": ["Audrey II"], "shots": {"0": {"k": 0, "answer": "Audrey II", "latency_ms": 161.0560417175293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Audrey II", "latency_ms": 132.34901428222656, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 615}, "2": {"k": 2, "answer": "Audrey II", "latency_ms": 211.22169494628906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1243}, "3": {"k": 3, "answer": "Audrey II", "latency_ms": 200.35529136657715, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1896}, "4": {"k": 4, "answer": "Audrey II", "latency_ms": 181.78224563598633, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2594}, "5": {"k": 5, "answer": "Audrey II", "latency_ms": 112.61272430419922, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3223}, "6": {"k": 6, "answer": "Audrey II", "latency_ms": 116.72687530517578, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3852}}}
{"qid": "-5541610970112826260", "question": "who signed the declaration of independence for maryland", "gold_answers": ["Samuel Chase", "Charles Carroll of Carrollton", "Thomas Stone", "William Paca"], "shots": {"0": {"k": 0, "answer": "George Wythe", "latency_ms": 182.46078491210938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 838.994026184082, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 704}, "2": {"k": 2, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 449.91111755371094, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 1294}, "3": {"k": 3, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 504.5208930969238, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 1948}, "4": {"k": 4, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 857.3741912841797, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 2568}, "5": {"k": 5, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 716.4475917816162, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 3240}, "6": {"k": 6, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll of Carrollton", "latency_ms": 735.755443572998, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3921}}}
{"qid": "-86997217831791339", "question": "when does wonder woman come out on blue ray", "gold_answers": ["September 19 , 2017"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 180.85670471191406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 260.12706756591797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "September 19, 2017", "latency_ms": 226.84049606323242, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1194}, "3": {"k": 3, "answer": "September 19, 2017", "latency_ms": 478.99627685546875, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1848}, "4": {"k": 4, "answer": "September 19, 2017", "latency_ms": 532.9890251159668, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2521}, "5": {"k": 5, "answer": "September 19, 2017", "latency_ms": 402.9581546783447, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3161}, "6": {"k": 6, "answer": "September 19, 2017", "latency_ms": 275.64191818237305, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3825}}}
{"qid": "-8062812506672279762", "question": "what nba team did derrick rose play for", "gold_answers": ["New York Knicks", "Minnesota Timberwolves", "Chicago Bulls", "Cleveland Cavaliers"], "shots": {"0": {"k": 0, "answer": "Chicago Bulls", "latency_ms": 134.98711585998535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Chicago Bulls", "latency_ms": 149.64795112609863, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 552}, "2": {"k": 2, "answer": "Chicago Bulls, New York Knicks, Cleveland Cavaliers", "latency_ms": 332.89170265197754, "em": 0.0, "f1": 0.6, "containment": 1.0, "context_chars": 1174}, "3": {"k": 3, "answer": "Chicago Bulls, New York Knicks, Cleveland Cavaliers, Utah Jazz, Minnesota Timberwolves", "latency_ms": 952.113151550293, "em": 0.0, "f1": 0.42857142857142855, "containment": 1.0, "context_chars": 1766}, "4": {"k": 4, "answer": "Chicago Bulls, New York Knicks, Cleveland Cavaliers, Utah Jazz, Minnesota Timberwolves", "latency_ms": 484.46202278137207, "em": 0.0, "f1": 0.42857142857142855, "containment": 1.0, "context_chars": 2344}, "5": {"k": 5, "answer": "Chicago Bulls, New York Knicks, Cleveland Cavaliers, Utah Jazz, Minnesota Timberwolves", "latency_ms": 613.9650344848633, "em": 0.0, "f1": 0.42857142857142855, "containment": 1.0, "context_chars": 2919}, "6": {"k": 6, "answer": "Chicago Bulls, New York Knicks, Cleveland Cavaliers, Utah Jazz, Minnesota Timberwolves", "latency_ms": 924.7713088989258, "em": 0.0, "f1": 0.42857142857142855, "containment": 1.0, "context_chars": 3483}}}
{"qid": "7933002036740390435", "question": "where did the term liberal arts come from", "gold_answers": ["the Roman Empire"], "shots": {"0": {"k": 0, "answer": "Renaissance Europe", "latency_ms": 288.1786823272705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Roman Empire", "latency_ms": 69.15402412414551, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Marcus Tullius Cicero", "latency_ms": 367.21038818359375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1259}, "3": {"k": 3, "answer": "Marcus Tullius Cicero", "latency_ms": 381.5169334411621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1917}, "4": {"k": 4, "answer": "Marcus Tullius Cicero", "latency_ms": 251.91283226013184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2579}, "5": {"k": 5, "answer": "Marcus Tullius Cicero", "latency_ms": 209.3527317047119, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3228}, "6": {"k": 6, "answer": "Roman Empire", "latency_ms": 227.66828536987305, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3962}}}
{"qid": "-2991400700385487821", "question": "who heads the executive department of west virginia government", "gold_answers": ["the Governor of West Virginia", "the governor of West Virginia", "Jim Justice"], "shots": {"0": {"k": 0, "answer": "Governor of West Virginia", "latency_ms": 143.77260208129883, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "governor", "latency_ms": 70.59121131896973, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "governor", "latency_ms": 151.80253982543945, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1278}, "3": {"k": 3, "answer": "Governor of West Virginia", "latency_ms": 227.60319709777832, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2017}, "4": {"k": 4, "answer": "Governor of West Virginia", "latency_ms": 128.80659103393555, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2661}, "5": {"k": 5, "answer": "Governor of West Virginia", "latency_ms": 240.25702476501465, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3312}, "6": {"k": 6, "answer": "Governor of West Virginia", "latency_ms": 228.58762741088867, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3955}}}
{"qid": "-9186689755642837558", "question": "how long is the bridge between new brunswick and prince edward island", "gold_answers": ["12.9 - kilometre"], "shots": {"0": {"k": 0, "answer": "1.5 kilometers", "latency_ms": 162.92119026184082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "world's longest bridge over ice-covered", "latency_ms": 367.52939224243164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 733}, "2": {"k": 2, "answer": "12.9 kilometers", "latency_ms": 388.7348175048828, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1421}, "3": {"k": 3, "answer": "world's longest bridge over ice-covered", "latency_ms": 207.66234397888184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2090}, "4": {"k": 4, "answer": "world's longest bridge over ice-covered", "latency_ms": 336.2863063812256, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2777}, "5": {"k": 5, "answer": "world's longest bridge over ice-covered", "latency_ms": 167.20843315124512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3368}, "6": {"k": 6, "answer": "world's longest bridge over ice-covered", "latency_ms": 308.23326110839844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4029}}}
{"qid": "8029901619424338449", "question": "when did the uk and us become allies", "gold_answers": ["1940", "Since 1940"], "shots": {"0": {"k": 0, "answer": "1941", "latency_ms": 289.0751361846924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 184.09395217895508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 690}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 134.92560386657715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1380}, "3": {"k": 3, "answer": "1958", "latency_ms": 175.16493797302246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "1958", "latency_ms": 202.93807983398438, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2733}, "5": {"k": 5, "answer": "1958", "latency_ms": 241.37592315673828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3379}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 187.97588348388672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3989}}}
{"qid": "4828649525820722736", "question": "who sang the theme song for the man with the golden gun", "gold_answers": ["Lulu"], "shots": {"0": {"k": 0, "answer": "Lulu", "latency_ms": 86.87782287597656, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lulu", "latency_ms": 88.21511268615723, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 641}, "2": {"k": 2, "answer": "Lulu", "latency_ms": 91.33696556091309, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1225}, "3": {"k": 3, "answer": "Lulu", "latency_ms": 200.4389762878418, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1791}, "4": {"k": 4, "answer": "Lulu", "latency_ms": 211.29250526428223, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2446}, "5": {"k": 5, "answer": "Lulu", "latency_ms": 139.9378776550293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3096}, "6": {"k": 6, "answer": "Lulu", "latency_ms": 255.24044036865234, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3703}}}
{"qid": "-7924663090486742985", "question": "american horror story freak show girl kidnapped by clown", "gold_answers": ["Bonnie Lipton"], "shots": {"0": {"k": 0, "answer": "Lily Mason", "latency_ms": 113.4939193725586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Twisty", "latency_ms": 211.89069747924805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "Elsa", "latency_ms": 90.62719345092773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1367}, "3": {"k": 3, "answer": "Elsa", "latency_ms": 87.70060539245605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2001}, "4": {"k": 4, "answer": "Elsa", "latency_ms": 161.53812408447266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2555}, "5": {"k": 5, "answer": "Elsa", "latency_ms": 204.636812210083, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3231}, "6": {"k": 6, "answer": "Elsa", "latency_ms": 222.54395484924316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3915}}}
{"qid": "-360675773049242516", "question": "the probability of making a type i error when retaining ho at the .05 level of significance is", "gold_answers": ["5 %"], "shots": {"0": {"k": 0, "answer": "0.05", "latency_ms": 142.11225509643555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "0.05", "latency_ms": 153.08833122253418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 575}, "2": {"k": 2, "answer": "0.05", "latency_ms": 95.70693969726562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1167}, "3": {"k": 3, "answer": "0.05", "latency_ms": 107.82146453857422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1759}, "4": {"k": 4, "answer": "0.05", "latency_ms": 382.5993537902832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2325}, "5": {"k": 5, "answer": "0.05", "latency_ms": 157.4413776397705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2960}, "6": {"k": 6, "answer": "0.05", "latency_ms": 127.16388702392578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3634}}}
{"qid": "5819386267283467034", "question": "what year did the us hockey team won the olympics", "gold_answers": ["1960", "1980", "1960 and 1980"], "shots": {"0": {"k": 0, "answer": "1980", "latency_ms": 75.21677017211914, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1960, 1980", "latency_ms": 158.82039070129395, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 659}, "2": {"k": 2, "answer": "1980", "latency_ms": 125.69046020507812, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1310}, "3": {"k": 3, "answer": "1960, 1980", "latency_ms": 212.19968795776367, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1913}, "4": {"k": 4, "answer": "1960, 1980", "latency_ms": 446.67720794677734, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2510}, "5": {"k": 5, "answer": "1960, 1980", "latency_ms": 456.76112174987793, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3113}, "6": {"k": 6, "answer": "1960, 1980", "latency_ms": 505.5851936340332, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3724}}}
{"qid": "3916394566028297311", "question": "who wrote it's a long long way to pasadena", "gold_answers": ["Harry Vanda", "John Young", "David Hemmings", "George Young"], "shots": {"0": {"k": 0, "answer": "Charles Manson", "latency_ms": 54.393768310546875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "David Hemmings and Harry Vanda", "latency_ms": 248.19612503051758, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 640}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 131.12950325012207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1264}, "3": {"k": 3, "answer": "David Hemmings", "latency_ms": 160.92705726623535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1902}, "4": {"k": 4, "answer": "David Hemmings", "latency_ms": 235.26287078857422, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2514}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.1822452545166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3138}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 373.6739158630371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3739}}}
{"qid": "-2432292250757146771", "question": "who wrote the song i hate you i love you", "gold_answers": ["Angel Benitez", "Olivia O'Brien", "Garrett Nash"], "shots": {"0": {"k": 0, "answer": "Lil Wayne", "latency_ms": 102.81777381896973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 146.18659019470215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 628}, "2": {"k": 2, "answer": "Gnash", "latency_ms": 92.30589866638184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1202}, "3": {"k": 3, "answer": "Chris DeStefano, Brett James", "latency_ms": 419.3568229675293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1831}, "4": {"k": 4, "answer": "Gnash and Olivia O'Brien", "latency_ms": 242.65527725219727, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2494}, "5": {"k": 5, "answer": "The context does not specify who wrote \"I Hate U, I Love U\". I cannot answer.", "latency_ms": 483.78944396972656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3069}, "6": {"k": 6, "answer": "Gnash and Olivia O'Brien", "latency_ms": 230.47351837158203, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3643}}}
{"qid": "2393389448844632496", "question": "what is loihi and where does it originate from", "gold_answers": ["from the Hawaii hotspot"], "shots": {"0": {"k": 0, "answer": "Loihi Seamount, Hawaii", "latency_ms": 151.26371383666992, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 187.24322319030762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 607}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 258.65793228149414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1185}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 239.73655700683594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1793}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 111.8626594543457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2440}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.75969505310059, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3027}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 308.93850326538086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3592}}}
{"qid": "-6677440871585683891", "question": "who rebuilt the temple after the babylonian captivity", "gold_answers": ["Herod"], "shots": {"0": {"k": 0, "answer": "Zerubbabel", "latency_ms": 195.59121131896973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cyrus the Great", "latency_ms": 174.39699172973633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 596}, "2": {"k": 2, "answer": "Haggai and Zechariah", "latency_ms": 421.938419342041, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1201}, "3": {"k": 3, "answer": "Haggai and Zechariah", "latency_ms": 401.78823471069336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1820}, "4": {"k": 4, "answer": "Cyrus the Great", "latency_ms": 203.44185829162598, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2408}, "5": {"k": 5, "answer": "Haggai and Zechariah", "latency_ms": 354.0775775909424, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3104}, "6": {"k": 6, "answer": "Cyrus the Great", "latency_ms": 284.85584259033203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3710}}}
{"qid": "-6750750235985613277", "question": "who was executed for being an american spy during the revolutionary war", "gold_answers": ["Nathan Hale"], "shots": {"0": {"k": 0, "answer": "Richard Croft", "latency_ms": 69.42152976989746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Nathan Hale", "latency_ms": 68.86863708496094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 593}, "2": {"k": 2, "answer": "Nathan Hale", "latency_ms": 123.2450008392334, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1157}, "3": {"k": 3, "answer": "Nathan Hale", "latency_ms": 147.02296257019043, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1739}, "4": {"k": 4, "answer": "Nathan Hale", "latency_ms": 330.72757720947266, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2384}, "5": {"k": 5, "answer": "Nathan Hale, Joseph Bettys, David Williams, Thomas Hickey", "latency_ms": 714.1227722167969, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3008}, "6": {"k": 6, "answer": "Nathan Hale, Joseph Bettys, David Williams, Isaac Van Wart, Thomas Hickey", "latency_ms": 461.83252334594727, "em": 0.0, "f1": 0.3076923076923077, "containment": 1.0, "context_chars": 3594}}}
{"qid": "8770209312170080158", "question": "who sang the song suddenly with olivia newton john", "gold_answers": ["Cliff Richard"], "shots": {"0": {"k": 0, "answer": "Colin Hay", "latency_ms": 132.38906860351562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cliff Richard", "latency_ms": 220.7803726196289, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "Cliff Richard", "latency_ms": 132.59434700012207, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1198}, "3": {"k": 3, "answer": "Cliff Richard", "latency_ms": 143.28670501708984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1846}, "4": {"k": 4, "answer": "Cliff Richard", "latency_ms": 119.6596622467041, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2402}, "5": {"k": 5, "answer": "Cliff Richard", "latency_ms": 117.33603477478027, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3014}, "6": {"k": 6, "answer": "Cliff Richard", "latency_ms": 176.51987075805664, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3672}}}
{"qid": "-180549795106449014", "question": "who is command sergeant major of the army", "gold_answers": ["Daniel A. Dailey"], "shots": {"0": {"k": 0, "answer": "as of my last update, the Command Sergeant Major of the Army is Major General David A. M. Smith. However, this information may have changed. For the most current information, please consult the official U.S. Army website.", "latency_ms": 1714.6289348602295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 278.45191955566406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 652}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 371.3874816894531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1267}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 265.2122974395752, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1873}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 109.5731258392334, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2453}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 196.30789756774902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3123}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 305.9704303741455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3732}}}
{"qid": "7815138213728640354", "question": "who sings the original windmills of your mind", "gold_answers": ["Noel Harrison"], "shots": {"0": {"k": 0, "answer": "The Kinks", "latency_ms": 60.317277908325195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 157.50765800476074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 634}, "2": {"k": 2, "answer": "The Thomas Crown Affair (1968)", "latency_ms": 223.74844551086426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1239}, "3": {"k": 3, "answer": "The Thomas Crown Affair (1968)", "latency_ms": 515.723466873169, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1873}, "4": {"k": 4, "answer": "Noel Harrison", "latency_ms": 184.24034118652344, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2525}, "5": {"k": 5, "answer": "Noel Harrison", "latency_ms": 385.12110710144043, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3206}, "6": {"k": 6, "answer": "Noel Harrison", "latency_ms": 257.24267959594727, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3812}}}
{"qid": "2098168902147822379", "question": "where will the next summer and winter olympics be held", "gold_answers": ["Beijing", "Tokyo"], "shots": {"0": {"k": 0, "answer": "2024: Paris, France; 2026: Milan-Cortina, Italy", "latency_ms": 442.0933723449707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Beijing", "latency_ms": 156.38160705566406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 623}, "2": {"k": 2, "answer": "2024: Paris, France; 2026: Milan-Cortina, Italy", "latency_ms": 497.00307846069336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1261}, "3": {"k": 3, "answer": "Paris (2024), Beijing (2022)", "latency_ms": 873.7893104553223, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1863}, "4": {"k": 4, "answer": "Paris (2024), Beijing (2022) — next: Los Angeles (2028), Beijing (2026) — I cannot answer.", "latency_ms": 990.5321598052979, "em": 0.0, "f1": 0.125, "containment": 1.0, "context_chars": 2534}, "5": {"k": 5, "answer": "Paris (2024), Beijing (2022)", "latency_ms": 608.5498332977295, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3135}, "6": {"k": 6, "answer": "Paris (2024), Beijing (2022)", "latency_ms": 571.730375289917, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3784}}}
{"qid": "-1458476052377668874", "question": "when did ole miss beat alabama in football", "gold_answers": ["September 19 , 2015", "October 3 , 1970", "October 27 , 1894", "October 13 , 2001", "October 5 , 1968", "October 4 , 2014", "September 11 , 1976", "October 8 , 1988", "October 18 , 2003", "November 5 , 1910"], "shots": {"0": {"k": 0, "answer": "November 27, 1971", "latency_ms": 256.65974617004395, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 234.9376678466797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 618}, "2": {"k": 2, "answer": "1988", "latency_ms": 291.15748405456543, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1246}, "3": {"k": 3, "answer": "1988", "latency_ms": 143.16916465759277, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1892}, "4": {"k": 4, "answer": "1988", "latency_ms": 242.76065826416016, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2482}, "5": {"k": 5, "answer": "1988", "latency_ms": 382.92431831359863, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3154}, "6": {"k": 6, "answer": "1988", "latency_ms": 360.83054542541504, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3801}}}
{"qid": "-6861734002351236912", "question": "who sang the song how far is heaven", "gold_answers": ["Los Lonely Boys"], "shots": {"0": {"k": 0, "answer": "Linda Ronstadt", "latency_ms": 212.10193634033203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wham", "latency_ms": 102.74910926818848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 595}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 220.03674507141113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1219}, "3": {"k": 3, "answer": "Carol Sue Wells", "latency_ms": 129.8995018005371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1883}, "4": {"k": 4, "answer": "Carol Sue Wells", "latency_ms": 223.07276725769043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2488}, "5": {"k": 5, "answer": "Carol Sue Wells", "latency_ms": 205.12771606445312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3114}, "6": {"k": 6, "answer": "Carol Sue Wells", "latency_ms": 150.41160583496094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3725}}}
{"qid": "-1722216190521937227", "question": "where are the organ mountains in new mexico", "gold_answers": ["southern New Mexico"], "shots": {"0": {"k": 0, "answer": "southwestern New Mexico, near the border with Arizona", "latency_ms": 214.57362174987793, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 526.7202854156494, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 606}, "2": {"k": 2, "answer": "southern New Mexico, near Las Cruces in Doña Ana County", "latency_ms": 586.8453979492188, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1325}, "3": {"k": 3, "answer": "southern New Mexico, near Las Cruces in Doña Ana County", "latency_ms": 468.19448471069336, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1986}, "4": {"k": 4, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 766.4673328399658, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 2643}, "5": {"k": 5, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 1034.1076850891113, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3283}, "6": {"k": 6, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 397.8428840637207, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3880}}}
{"qid": "-2717119588239727064", "question": "where are the 10 plagues found in the bible", "gold_answers": ["Book of Exodus", "Exodus"], "shots": {"0": {"k": 0, "answer": "Exodus 7–12", "latency_ms": 124.14979934692383, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Book of Exodus", "latency_ms": 69.76079940795898, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 645}, "2": {"k": 2, "answer": "Book of Exodus", "latency_ms": 108.54792594909668, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1279}, "3": {"k": 3, "answer": "Book of Exodus", "latency_ms": 169.1122055053711, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1887}, "4": {"k": 4, "answer": "Book of Exodus", "latency_ms": 101.47404670715332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2506}, "5": {"k": 5, "answer": "Book of Exodus", "latency_ms": 143.0985927581787, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3129}, "6": {"k": 6, "answer": "Book of Exodus", "latency_ms": 231.53924942016602, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3701}}}
{"qid": "2988284357714596500", "question": "where are red blood cells made in adults", "gold_answers": ["in the bone marrow"], "shots": {"0": {"k": 0, "answer": "bone marrow", "latency_ms": 50.46653747558594, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "red bone marrow", "latency_ms": 122.94220924377441, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "bone marrow", "latency_ms": 85.90149879455566, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1295}, "3": {"k": 3, "answer": "red bone marrow", "latency_ms": 177.72603034973145, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1895}, "4": {"k": 4, "answer": "bodies of the vertebrae, the breastbone (sternum), the ribcage, the pelvic bones, and the bones of the upper arms and legs", "latency_ms": 1117.9697513580322, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2607}, "5": {"k": 5, "answer": "red bone marrow of large bones", "latency_ms": 212.0511531829834, "em": 0.0, "f1": 0.4444444444444444, "containment": 0.0, "context_chars": 3200}, "6": {"k": 6, "answer": "red bone marrow of large bones", "latency_ms": 160.84575653076172, "em": 0.0, "f1": 0.4444444444444444, "containment": 0.0, "context_chars": 3840}}}
{"qid": "-4616596799374362422", "question": "who was the pinkerton detective agency's first female detective", "gold_answers": ["Kate Warne"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 104.94446754455566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kate Warne", "latency_ms": 113.37399482727051, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 624}, "2": {"k": 2, "answer": "Kate Warne", "latency_ms": 99.29990768432617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1235}, "3": {"k": 3, "answer": "Kate Warne", "latency_ms": 115.97228050231934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1918}, "4": {"k": 4, "answer": "Kate Warne", "latency_ms": 220.36981582641602, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2539}, "5": {"k": 5, "answer": "Kate Warne", "latency_ms": 132.8413486480713, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3222}, "6": {"k": 6, "answer": "Kate Warne", "latency_ms": 130.91254234313965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3862}}}
{"qid": "-3650291155113659146", "question": "how many episodes are there in modern family", "gold_answers": ["232", "201"], "shots": {"0": {"k": 0, "answer": "216 episodes", "latency_ms": 79.26654815673828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "24 episodes", "latency_ms": 241.21475219726562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 638}, "2": {"k": 2, "answer": "24 episodes", "latency_ms": 107.20133781433105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1262}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 211.9154930114746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1855}, "4": {"k": 4, "answer": "24 episodes", "latency_ms": 327.8079032897949, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2500}, "5": {"k": 5, "answer": "24 episodes", "latency_ms": 189.31078910827637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3178}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 195.5239772796631, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3805}}}
{"qid": "3381924381590631417", "question": "who built the first temple for god in jerusalem", "gold_answers": ["Solomon"], "shots": {"0": {"k": 0, "answer": "King Solomon", "latency_ms": 121.30212783813477, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "King Solomon", "latency_ms": 164.1097068786621, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 661}, "2": {"k": 2, "answer": "Solomon", "latency_ms": 99.98154640197754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1251}, "3": {"k": 3, "answer": "Josiah", "latency_ms": 142.88735389709473, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1886}, "4": {"k": 4, "answer": "Israel Finkelstein and Neil Asher Silberman", "latency_ms": 486.8130683898926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2508}, "5": {"k": 5, "answer": "Israel Finkelstein and Neil Asher Silberman argue it was built by Josiah, not Solomon.", "latency_ms": 415.0424003601074, "em": 0.0, "f1": 0.13333333333333333, "containment": 1.0, "context_chars": 3145}, "6": {"k": 6, "answer": "Solomon", "latency_ms": 105.00597953796387, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3790}}}
{"qid": "-154783694579651082", "question": "what is a dropped pin on google maps for", "gold_answers": ["marks locations", "marks locations in Google Maps"], "shots": {"0": {"k": 0, "answer": "A dropped pin on Google Maps is typically a user-created marker indicating a location of interest, such as a business, event, or personal spot.", "latency_ms": 1073.9572048187256, "em": 0.0, "f1": 0.16000000000000003, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "a location", "latency_ms": 284.9702835083008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "a dropped pin on google maps marks a specific location", "latency_ms": 262.6776695251465, "em": 0.0, "f1": 0.4615384615384615, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "to mark a location on a map", "latency_ms": 344.16890144348145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1851}, "4": {"k": 4, "answer": "to accurately mark a point on a map without obscuring the location", "latency_ms": 487.0741367340088, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2445}, "5": {"k": 5, "answer": "to accurately mark a point on a map without obscuring the location.", "latency_ms": 470.17908096313477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3138}, "6": {"k": 6, "answer": "a location", "latency_ms": 259.78922843933105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3772}}}
{"qid": "6915606477668963399", "question": "what do the 3 dots mean in math", "gold_answers": ["the therefore sign", "therefore sign"], "shots": {"0": {"k": 0, "answer": "ellipses", "latency_ms": 122.46036529541016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "repeating decimal", "latency_ms": 160.3255271911621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 591}, "2": {"k": 2, "answer": "three dots mean \"ellipsis\" in math, indicating continuation or an infinite sequence.", "latency_ms": 886.6581916809082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1242}, "3": {"k": 3, "answer": "three dots mean \"ellipsis\" or \"infinitely repeating\" in decimal notation", "latency_ms": 391.2506103515625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1823}, "4": {"k": 4, "answer": "the 3 dots mean \"ellipsis\" or \"infinitely repeating\" in decimal notation", "latency_ms": 395.92671394348145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2407}, "5": {"k": 5, "answer": "three dots mean \"ellipsis\" or \"infinitely repeating\" in decimal notation", "latency_ms": 668.4391498565674, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2985}, "6": {"k": 6, "answer": "logical symbol ∧", "latency_ms": 457.3371410369873, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3645}}}
{"qid": "-8366545547296627039", "question": "who wrote the song photograph by ringo starr", "gold_answers": ["Richard Starkey", "George Harrison", "Ringo Starr"], "shots": {"0": {"k": 0, "answer": "Ringo Starr", "latency_ms": 134.8109245300293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ringo Starr and George Harrison", "latency_ms": 334.9113464355469, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 671}, "2": {"k": 2, "answer": "Ringo Starr and George Harrison", "latency_ms": 266.0064697265625, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1329}, "3": {"k": 3, "answer": "George Harrison", "latency_ms": 106.37021064758301, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1968}, "4": {"k": 4, "answer": "George Harrison", "latency_ms": 283.949613571167, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2578}, "5": {"k": 5, "answer": "George Harrison", "latency_ms": 138.45086097717285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3302}, "6": {"k": 6, "answer": "George Harrison", "latency_ms": 251.62410736083984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3958}}}
{"qid": "-5004457603684974952", "question": "who is playing the halftime show at super bowl 2016", "gold_answers": ["Beyoncé", "Coldplay", "Bruno Mars"], "shots": {"0": {"k": 0, "answer": "Coldplay", "latency_ms": 101.19509696960449, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 222.48363494873047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 95.96681594848633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1311}, "3": {"k": 3, "answer": "Coldplay", "latency_ms": 143.77307891845703, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1947}, "4": {"k": 4, "answer": "Coldplay", "latency_ms": 128.68332862854004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2581}, "5": {"k": 5, "answer": "Coldplay with Beyoncé and Bruno Mars", "latency_ms": 591.3772583007812, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3225}, "6": {"k": 6, "answer": "Coldplay with Beyoncé and Bruno Mars", "latency_ms": 485.02564430236816, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3862}}}
{"qid": "7420920477821898435", "question": "to aru kagaku no railgun s episode 3", "gold_answers": ["Project Radio Noise"], "shots": {"0": {"k": 0, "answer": "3", "latency_ms": 141.1144733428955, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 103.17754745483398, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 545}, "2": {"k": 2, "answer": "Only My Railgun", "latency_ms": 206.45880699157715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1150}, "3": {"k": 3, "answer": "Smile (You & Me)", "latency_ms": 174.37195777893066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1753}, "4": {"k": 4, "answer": "Only My Railgun", "latency_ms": 237.73789405822754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2389}, "5": {"k": 5, "answer": "Only My Railgun", "latency_ms": 199.22304153442383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3011}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 355.6668758392334, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3631}}}
{"qid": "7478795216476346339", "question": "who won the 2017 sports personality of the year", "gold_answers": ["Mo Farah"], "shots": {"0": {"k": 0, "answer": "Lionel Messi", "latency_ms": 187.0589256286621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dame Sarah Storey", "latency_ms": 185.03260612487793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 229.97212409973145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1306}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 219.97499465942383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1960}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 321.1352825164795, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2618}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 339.75863456726074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3278}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 567.3069953918457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3913}}}
{"qid": "7217222058435937287", "question": "where was the world economic forum held this year", "gold_answers": ["Davos"], "shots": {"0": {"k": 0, "answer": "Davos, Switzerland", "latency_ms": 130.68103790283203, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Davos", "latency_ms": 214.3557071685791, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 671}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 112.6561164855957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1326}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 155.11059761047363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1997}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 111.42420768737793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2673}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 146.52681350708008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3277}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 276.8819332122803, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3957}}}
{"qid": "-143054837169120955", "question": "where are the giant redwoods located in california", "gold_answers": ["the coast of northern California", "Del Norte County", "Humboldt County"], "shots": {"0": {"k": 0, "answer": "Redwood National and State Parks", "latency_ms": 506.7732334136963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Sequoia National Park", "latency_ms": 251.70636177062988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "Sequoia National Park and Kings Canyon National Park", "latency_ms": 189.40019607543945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1221}, "3": {"k": 3, "answer": "Giant Forest and Redwood Mountain Grove in California's Sierra Nevada", "latency_ms": 517.3003673553467, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1869}, "4": {"k": 4, "answer": "San Francisco Bay, Moraga and Lafayette, California", "latency_ms": 453.9778232574463, "em": 0.0, "f1": 0.18181818181818182, "containment": 0.0, "context_chars": 2531}, "5": {"k": 5, "answer": "Giant redwoods are located in Humboldt Redwoods State Park, California.", "latency_ms": 561.8014335632324, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 3252}, "6": {"k": 6, "answer": "Humboldt Redwoods State Park, Northern California; along the cool coastal belt from Monterey County to Oregon; in the San Francisco Bay area (Moraga and Lafayette); in Kings Canyon National Park and Giant Sequoia National Monument (Sierra Nevada)", "latency_ms": 2129.0292739868164, "em": 0.0, "f1": 0.1081081081081081, "containment": 0.0, "context_chars": 3893}}}
{"qid": "4892429640540595424", "question": "who has made the most premier league appearances", "gold_answers": ["Gareth Barry"], "shots": {"0": {"k": 0, "answer": "Alan Shearer", "latency_ms": 190.36173820495605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 140.5644416809082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 112.40053176879883, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 218.25766563415527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1937}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 211.11249923706055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2591}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 227.7088165283203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3245}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 242.1736717224121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3899}}}
{"qid": "5533906981191706877", "question": "who was the first chief minister of west bengal", "gold_answers": ["Prafulla Chandra Ghosh"], "shots": {"0": {"k": 0, "answer": "P. C. Bagchi", "latency_ms": 106.93502426147461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Prafulla Chandra Ghosh", "latency_ms": 291.4619445800781, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 693}, "2": {"k": 2, "answer": "Prafulla Chandra Ghosh", "latency_ms": 263.93723487854004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1347}, "3": {"k": 3, "answer": "Prafulla Chandra Ghosh", "latency_ms": 382.3063373565674, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1993}, "4": {"k": 4, "answer": "Prafulla Chandra Ghosh", "latency_ms": 204.00524139404297, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2708}, "5": {"k": 5, "answer": "Prafulla Chandra Ghosh", "latency_ms": 204.4045925140381, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3304}, "6": {"k": 6, "answer": "Prafulla Chandra Ghosh", "latency_ms": 193.94946098327637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3964}}}
{"qid": "8823076125131408628", "question": "who sings got my mind set on you", "gold_answers": ["George Harrison", "James Ray"], "shots": {"0": {"k": 0, "answer": "Frank Sinatra", "latency_ms": 87.96882629394531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "George Harrison", "latency_ms": 214.67232704162598, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 541}, "2": {"k": 2, "answer": "George Harrison", "latency_ms": 117.97142028808594, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1183}, "3": {"k": 3, "answer": "George Harrison", "latency_ms": 200.6521224975586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1740}, "4": {"k": 4, "answer": "George Harrison", "latency_ms": 359.0869903564453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2301}, "5": {"k": 5, "answer": "George Harrison", "latency_ms": 116.14394187927246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2861}, "6": {"k": 6, "answer": "George Harrison", "latency_ms": 200.52695274353027, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3406}}}
{"qid": "9168660859139861094", "question": "two main types of research methods used in psychology", "gold_answers": ["Correlational research", "Descriptive research", "quantitative data", "Quantitative psychological research", "qualitative", "quantitative", "Qualitative psychological research", "Experimental research", "qualitative data"], "shots": {"0": {"k": 0, "answer": "Experimental and observational", "latency_ms": 158.782958984375, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Experimental and statistical methods", "latency_ms": 207.0779800415039, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 822}, "2": {"k": 2, "answer": "Experimental and qualitative", "latency_ms": 225.07643699645996, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1631}, "3": {"k": 3, "answer": "qualitative and quantitative", "latency_ms": 305.6669235229492, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2337}, "4": {"k": 4, "answer": "qualitative and quantitative", "latency_ms": 227.8285026550293, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3074}, "5": {"k": 5, "answer": "qualitative and quantitative", "latency_ms": 152.2214412689209, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3821}, "6": {"k": 6, "answer": "qualitative and quantitative", "latency_ms": 203.30548286437988, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 4624}}}
{"qid": "-7073063191952855764", "question": "who plays peter in what we do in the shadows", "gold_answers": ["Ben Fransham"], "shots": {"0": {"k": 0, "answer": "Matt Berry", "latency_ms": 84.20968055725098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Hayden Szeto", "latency_ms": 222.66507148742676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 637}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 127.01225280761719, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1223}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 191.75338745117188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1881}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 294.27647590637207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2480}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 201.79319381713867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3092}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 230.27420043945312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3743}}}
{"qid": "7901746249864619718", "question": "when does the 14th season of grey's anatomy come out", "gold_answers": ["September 28 , 2017"], "shots": {"0": {"k": 0, "answer": "2023", "latency_ms": 80.62219619750977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 28, 2017", "latency_ms": 232.36608505249023, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "September 28, 2017", "latency_ms": 601.3619899749756, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "September 28, 2017", "latency_ms": 249.93610382080078, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "September 28, 2017", "latency_ms": 307.7859878540039, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2678}, "5": {"k": 5, "answer": "September 28, 2017", "latency_ms": 405.76887130737305, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3346}, "6": {"k": 6, "answer": "September 28, 2017", "latency_ms": 363.67130279541016, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3933}}}
{"qid": "-896127746191949637", "question": "when was the first hunger games book published", "gold_answers": ["September 14 , 2008", "2008"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 329.5609951019287, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 14, 2008", "latency_ms": 312.2875690460205, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 670}, "2": {"k": 2, "answer": "September 14, 2008", "latency_ms": 190.02676010131836, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1322}, "3": {"k": 3, "answer": "September 14, 2008", "latency_ms": 371.7026710510254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1939}, "4": {"k": 4, "answer": "September 14, 2008", "latency_ms": 336.90929412841797, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2563}, "5": {"k": 5, "answer": "September 14, 2008", "latency_ms": 202.68511772155762, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3225}, "6": {"k": 6, "answer": "September 14, 2008", "latency_ms": 660.3794097900391, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3854}}}
{"qid": "8044909700499569711", "question": "how much is the united states in debt to china", "gold_answers": ["$1.06 trillion", "1,200.5"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 276.40485763549805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "$1.16 trillion", "latency_ms": 116.62912368774414, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "$1.16 trillion", "latency_ms": 241.0600185394287, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": "$1.16 trillion", "latency_ms": 352.25391387939453, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1934}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 161.97705268859863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2582}, "5": {"k": 5, "answer": "$1.18 trillion", "latency_ms": 202.7268409729004, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3235}, "6": {"k": 6, "answer": "$1.18 trillion", "latency_ms": 411.3199710845947, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3867}}}
{"qid": "-3123234845590594961", "question": "where did the race in grease take place", "gold_answers": ["Thunder Road"], "shots": {"0": {"k": 0, "answer": "1950s San Diego, California", "latency_ms": 151.0329246520996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Los Angeles River", "latency_ms": 87.80360221862793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "Los Angeles River", "latency_ms": 201.83396339416504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "Los Angeles River, between the First and", "latency_ms": 369.69804763793945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1824}, "4": {"k": 4, "answer": "Los Angeles River, between the First and", "latency_ms": 573.925256729126, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2445}, "5": {"k": 5, "answer": "Los Angeles River, between the First and", "latency_ms": 439.44239616394043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3143}, "6": {"k": 6, "answer": "Los Angeles River, between the First and", "latency_ms": 236.28473281860352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3762}}}
{"qid": "4326992806019599297", "question": "what is the maximum data rate for the 802.11a standard select one", "gold_answers": ["54 Mbit / s"], "shots": {"0": {"k": 0, "answer": "54 Mbps", "latency_ms": 94.7113037109375, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "54 Mbit/s", "latency_ms": 99.78699684143066, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 628}, "2": {"k": 2, "answer": "54 Mbit/s", "latency_ms": 356.84895515441895, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1274}, "3": {"k": 3, "answer": "54 Mbit/s", "latency_ms": 223.50788116455078, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1950}, "4": {"k": 4, "answer": "54 Mbit/s", "latency_ms": 136.3379955291748, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2612}, "5": {"k": 5, "answer": "54 Mbit/s", "latency_ms": 509.2325210571289, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3234}, "6": {"k": 6, "answer": "54 Mbit/s", "latency_ms": 236.43040657043457, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3856}}}
{"qid": "6144099837933283715", "question": "a good that can be used in place of another good", "gold_answers": ["A substitute good", "substitute good"], "shots": {"0": {"k": 0, "answer": "substitute good", "latency_ms": 97.23615646362305, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "substitute good", "latency_ms": 211.9143009185791, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 558}, "2": {"k": 2, "answer": "substitute good", "latency_ms": 205.65319061279297, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1155}, "3": {"k": 3, "answer": "substitute good", "latency_ms": 247.19476699829102, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1734}, "4": {"k": 4, "answer": "substitute good", "latency_ms": 290.06457328796387, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2319}, "5": {"k": 5, "answer": "substitute good", "latency_ms": 233.21247100830078, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2925}, "6": {"k": 6, "answer": "substitute good", "latency_ms": 180.92823028564453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3488}}}
{"qid": "-4621948696356225060", "question": "where in the bible can i find the lord's prayer", "gold_answers": ["the Gospel of Luke", "the Gospel of Matthew"], "shots": {"0": {"k": 0, "answer": "Matthew 6:9-13", "latency_ms": 128.23772430419922, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 297.40047454833984, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 588}, "2": {"k": 2, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 507.32922554016113, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 1260}, "3": {"k": 3, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 224.99370574951172, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 1829}, "4": {"k": 4, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 471.56834602355957, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 2473}, "5": {"k": 5, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 312.868595123291, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 3166}, "6": {"k": 6, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 591.8831825256348, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 3854}}}
{"qid": "6941993254369956277", "question": "when was the last time nebraska voted democrat", "gold_answers": ["1964"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 130.30028343200684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 150.6195068359375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 704}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 144.5932388305664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1415}, "3": {"k": 3, "answer": "2016", "latency_ms": 229.45117950439453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2030}, "4": {"k": 4, "answer": "1936", "latency_ms": 353.96504402160645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2724}, "5": {"k": 5, "answer": "1936", "latency_ms": 338.52672576904297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3450}, "6": {"k": 6, "answer": "1936", "latency_ms": 343.3244228363037, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4148}}}
{"qid": "-8955197065025093046", "question": "who become the ceo of it wipro company in 2016", "gold_answers": ["Abid Ali Neemuchwala"], "shots": {"0": {"k": 0, "answer": "Ajay Bhatia", "latency_ms": 102.26035118103027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Abidali Neemuchwala", "latency_ms": 307.5122833251953, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 678}, "2": {"k": 2, "answer": "Abidali Neemuchwala", "latency_ms": 335.6444835662842, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1363}, "3": {"k": 3, "answer": "Abidali Neemuchwala", "latency_ms": 403.72729301452637, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1991}, "4": {"k": 4, "answer": "Abidali Neemuchwala", "latency_ms": 261.1656188964844, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2672}, "5": {"k": 5, "answer": "Abidali Neemuchwala", "latency_ms": 334.3474864959717, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3266}, "6": {"k": 6, "answer": "Abidali Neemuchwala", "latency_ms": 265.9468650817871, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3932}}}
{"qid": "-3010425579521817304", "question": "who does eric end up with in gossip girl", "gold_answers": ["Jenny"], "shots": {"0": {"k": 0, "answer": "Serena", "latency_ms": 80.31892776489258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 317.40522384643555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "Damien Darko", "latency_ms": 345.5781936645508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1198}, "3": {"k": 3, "answer": "Damien Darko", "latency_ms": 120.60546875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1856}, "4": {"k": 4, "answer": "Damien", "latency_ms": 131.46638870239258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2421}, "5": {"k": 5, "answer": "Val", "latency_ms": 92.24176406860352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2996}, "6": {"k": 6, "answer": "Val", "latency_ms": 212.9526138305664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3619}}}
{"qid": "7264112453286983469", "question": "who plays unis in she's the man", "gold_answers": ["Emily Perkins"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 227.46920585632324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 198.49300384521484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 205.51133155822754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 384.84883308410645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1874}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 278.8565158843994, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2512}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 235.21161079406738, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3167}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 256.5121650695801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3791}}}
{"qid": "2627695648792729859", "question": "who played booster in jingle all the way", "gold_answers": ["Curtis Armstrong"], "shots": {"0": {"k": 0, "answer": "James Caan", "latency_ms": 122.71690368652344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 331.38251304626465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 666}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 169.48485374450684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1329}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 150.69150924682617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1962}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 162.24002838134766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2520}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 129.26983833312988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3209}, "6": {"k": 6, "answer": "Tim Flattery", "latency_ms": 214.0660285949707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3890}}}
{"qid": "-9169976182548289414", "question": "who is recognized as the founder of islam", "gold_answers": ["the Islamic prophet Muhammad", "Muhammad"], "shots": {"0": {"k": 0, "answer": "Prophet Muhammad", "latency_ms": 138.80300521850586, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Muhammad", "latency_ms": 59.25893783569336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 606}, "2": {"k": 2, "answer": "Muhammad", "latency_ms": 69.54360008239746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1211}, "3": {"k": 3, "answer": "Muhammad", "latency_ms": 209.7630500793457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1839}, "4": {"k": 4, "answer": "Muhammad", "latency_ms": 90.26551246643066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2416}, "5": {"k": 5, "answer": "Muhammad", "latency_ms": 101.79853439331055, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3074}, "6": {"k": 6, "answer": "Muhammad", "latency_ms": 132.67016410827637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3627}}}
{"qid": "-6696747560626271522", "question": "who plays emma in air bud world pup", "gold_answers": ["Brittany Paige Bouck"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 111.1905574798584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 96.27652168273926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 623}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 208.03546905517578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1286}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 110.43143272399902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1922}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 232.96236991882324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2574}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 205.05619049072266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3201}, "6": {"k": 6, "answer": "Chantal Strand", "latency_ms": 228.51824760437012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3800}}}
{"qid": "5871681964137793716", "question": "how old was sasuke when his clan died", "gold_answers": ["seven"], "shots": {"0": {"k": 0, "answer": "12 years old", "latency_ms": 150.73418617248535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 213.24849128723145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 120.67413330078125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1206}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 170.43352127075195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1853}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 187.1628761291504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2487}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 132.3566436767578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3056}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 287.59312629699707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3674}}}
{"qid": "8552176802923865025", "question": "where do some of the natural air pollutants come from", "gold_answers": ["wildfires", "Dust", "Vegetation", "Carbon dioxide", "Volcanic activity"], "shots": {"0": {"k": 0, "answer": "Volcanic eruptions, wildfires, and natural decay processes.", "latency_ms": 261.40618324279785, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "natural environment", "latency_ms": 72.35550880432129, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 683}, "2": {"k": 2, "answer": "natural processes", "latency_ms": 148.55146408081055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1359}, "3": {"k": 3, "answer": "natural processes, volcanic ash, sea spray, meteoroids, dust, pollen, spores, water vapor, ice crystals, ozone layer", "latency_ms": 1144.935131072998, "em": 0.0, "f1": 0.1176470588235294, "containment": 1.0, "context_chars": 2089}, "4": {"k": 4, "answer": "natural processes, volcanic ash, sea spray, pollen, spores, and meteoroids", "latency_ms": 603.5366058349609, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 2789}, "5": {"k": 5, "answer": "methane, nitrous oxide, ozone, dust, pollen and spores, sea spray, volcanic ash, meteoroids", "latency_ms": 932.5745105743408, "em": 0.0, "f1": 0.14285714285714288, "containment": 1.0, "context_chars": 3467}, "6": {"k": 6, "answer": "methane, nitrous oxide, ozone, water vapor, dust, pollen and spores, sea spray, volcanic ash, meteoroids", "latency_ms": 1074.20015335083, "em": 0.0, "f1": 0.125, "containment": 1.0, "context_chars": 4184}}}
{"qid": "-2630525064809360052", "question": "who is the pastor of new birth missionary baptist church now", "gold_answers": ["Stephen A. Davis"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 187.77775764465332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 109.8473072052002, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 675}, "2": {"k": 2, "answer": "Jamal Bryant", "latency_ms": 199.60665702819824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1353}, "3": {"k": 3, "answer": "Jamal Bryant", "latency_ms": 113.26408386230469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2018}, "4": {"k": 4, "answer": "Jamal Bryant", "latency_ms": 100.9371280670166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2655}, "5": {"k": 5, "answer": "Jamal Bryant", "latency_ms": 220.78990936279297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3328}, "6": {"k": 6, "answer": "Jamal Bryant", "latency_ms": 199.96094703674316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3991}}}
{"qid": "6434931911289860123", "question": "who is doing 2018 super bowl half time show", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Beyoncé", "latency_ms": 75.29568672180176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Justin Timberlake", "latency_ms": 124.79519844055176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 658}, "2": {"k": 2, "answer": "Justin Timberlake", "latency_ms": 144.76895332336426, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1298}, "3": {"k": 3, "answer": "Justin Timberlake", "latency_ms": 265.5956745147705, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1921}, "4": {"k": 4, "answer": "Justin Timberlake", "latency_ms": 259.7043514251709, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2525}, "5": {"k": 5, "answer": "Justin Timberlake", "latency_ms": 176.17249488830566, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3123}, "6": {"k": 6, "answer": "Justin Timberlake", "latency_ms": 346.72069549560547, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3739}}}
{"qid": "8869318258439148973", "question": "who is playing the halftime show for the superbowl", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Dr. Dre and Snoop Dogg", "latency_ms": 345.461368560791, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Justin Timberlake", "latency_ms": 162.39166259765625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 658}, "2": {"k": 2, "answer": "Justin Timberlake", "latency_ms": 109.25865173339844, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1311}, "3": {"k": 3, "answer": "Justin Timberlake", "latency_ms": 227.52785682678223, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1943}, "4": {"k": 4, "answer": "Justin Timberlake", "latency_ms": 272.3524570465088, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2587}, "5": {"k": 5, "answer": "Justin Timberlake", "latency_ms": 138.82184028625488, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3213}, "6": {"k": 6, "answer": "Justin Timberlake", "latency_ms": 258.53538513183594, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3847}}}
{"qid": "-8359866474144720253", "question": "who hosted they think it's all over", "gold_answers": ["Des Lynam", "Lee Mack", "comedian Nick Hancock", "Nick Hancock"], "shots": {"0": {"k": 0, "answer": "Randy Jackson", "latency_ms": 62.58797645568848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Des Lynam", "latency_ms": 129.58598136901855, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 627}, "2": {"k": 2, "answer": "Nick Hancock", "latency_ms": 147.719144821167, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1261}, "3": {"k": 3, "answer": "Nick Hancock", "latency_ms": 106.46867752075195, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1941}, "4": {"k": 4, "answer": "Nick Hancock", "latency_ms": 103.71971130371094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2580}, "5": {"k": 5, "answer": "Nick Hancock", "latency_ms": 268.9356803894043, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3198}, "6": {"k": 6, "answer": "Nick Hancock", "latency_ms": 228.41763496398926, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3840}}}
{"qid": "-1206653570097564556", "question": "where does the movie proof of life take place", "gold_answers": ["The Republic of Tecala"], "shots": {"0": {"k": 0, "answer": "India", "latency_ms": 70.47104835510254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Tecala", "latency_ms": 198.46463203430176, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "Republic of Tecala", "latency_ms": 258.1908702850342, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1261}, "3": {"k": 3, "answer": "Republic of Tecala", "latency_ms": 389.7843360900879, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1865}, "4": {"k": 4, "answer": "Republic of Tecala", "latency_ms": 297.8544235229492, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2507}, "5": {"k": 5, "answer": "Republic of Tecala", "latency_ms": 456.64429664611816, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3105}, "6": {"k": 6, "answer": "Republic of Tecala", "latency_ms": 164.11757469177246, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3756}}}
{"qid": "6144521615402262404", "question": "where does us highway 1 start and end", "gold_answers": ["Fort Kent , Maine", "to Key West , Florida", "Key West , Florida"], "shots": {"0": {"k": 0, "answer": "Start: U.S. Coast, California; End: U.S. Coast, Florida", "latency_ms": 675.9002208709717, "em": 0.0, "f1": 0.18181818181818182, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Key West, Florida to Fort Kent, Maine", "latency_ms": 380.7637691497803, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "Key West, Florida to Fort Kent, Maine", "latency_ms": 340.3644561767578, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 1239}, "3": {"k": 3, "answer": "Key West, Florida to Fort Kent, Maine", "latency_ms": 289.22033309936523, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 1863}, "4": {"k": 4, "answer": "Key West, Florida to Fort Kent, Maine", "latency_ms": 478.38592529296875, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 2478}, "5": {"k": 5, "answer": "Key West, Florida to Fort Kent, Maine", "latency_ms": 453.7086486816406, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 3127}, "6": {"k": 6, "answer": "Key West, Florida to Fort Kent, Maine", "latency_ms": 512.9022598266602, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 3736}}}
{"qid": "-8631190913794477985", "question": "who scored the most points in a single game in the nba", "gold_answers": ["Wilt Chamberlain"], "shots": {"0": {"k": 0, "answer": "Wilt Chamberlain", "latency_ms": 233.22391510009766, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wilt Chamberlain", "latency_ms": 159.041166305542, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 664}, "2": {"k": 2, "answer": "Wilt Chamberlain", "latency_ms": 159.6536636352539, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1218}, "3": {"k": 3, "answer": "Wilt Chamberlain", "latency_ms": 110.22520065307617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1844}, "4": {"k": 4, "answer": "Wilt Chamberlain", "latency_ms": 257.92789459228516, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2476}, "5": {"k": 5, "answer": "Wilt Chamberlain", "latency_ms": 128.28373908996582, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3127}, "6": {"k": 6, "answer": "Wilt Chamberlain", "latency_ms": 182.14654922485352, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3734}}}
{"qid": "199896377614951764", "question": "where is natural gas found in the us", "gold_answers": ["Texas", "Pennsylvania", "Colorado", "New Mexico", "West Virginia", "Arkansas", "Wyoming", "Louisiana"], "shots": {"0": {"k": 0, "answer": "Texas, North Dakota, Wyoming, Pennsylvania, and Utah", "latency_ms": 408.4458351135254, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Pennsylvania", "latency_ms": 63.89117240905762, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 653}, "2": {"k": 2, "answer": "Pennsylvania", "latency_ms": 116.52779579162598, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1295}, "3": {"k": 3, "answer": "Texas, Pennsylvania, Louisiana", "latency_ms": 182.38353729248047, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1972}, "4": {"k": 4, "answer": "Texas, Pennsylvania, Louisiana, Barnett Shale, Antrim Shale", "latency_ms": 595.726728439331, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 2588}, "5": {"k": 5, "answer": "Texas, Pennsylvania, Louisiana", "latency_ms": 510.61487197875977, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3279}, "6": {"k": 6, "answer": "Texas, Pennsylvania, Louisiana", "latency_ms": 200.4404067993164, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3926}}}
{"qid": "8986775680996674901", "question": "when were the first general elections held in independent india", "gold_answers": ["1951 -- 52"], "shots": {"0": {"k": 0, "answer": "15 August 1947", "latency_ms": 266.2498950958252, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 816.1003589630127, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 641.2150859832764, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 695.5695152282715, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 1974}, "4": {"k": 4, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 803.0848503112793, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 2660}, "5": {"k": 5, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 877.0608901977539, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 3335}, "6": {"k": 6, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 833.8863849639893, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 4052}}}
{"qid": "700604097171850168", "question": "what was the tower of london originally used for", "gold_answers": ["as a royal residence", "a royal residence"], "shots": {"0": {"k": 0, "answer": "Royal palace and fortress", "latency_ms": 194.1680908203125, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "royal residence and prison for high-status individuals", "latency_ms": 175.7814884185791, "em": 0.0, "f1": 0.4444444444444445, "containment": 0.0, "context_chars": 626}, "2": {"k": 2, "answer": "royal castle", "latency_ms": 225.2805233001709, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1218}, "3": {"k": 3, "answer": "royal castle", "latency_ms": 251.97768211364746, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1874}, "4": {"k": 4, "answer": "royal residence and military stronghold", "latency_ms": 385.29109954833984, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 2497}, "5": {"k": 5, "answer": "royal residence and military fortress", "latency_ms": 367.01512336730957, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3153}, "6": {"k": 6, "answer": "royal residence and military fortress", "latency_ms": 319.0128803253174, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3775}}}
{"qid": "6745303307988470742", "question": "what type of fuel goes in a zippo", "gold_answers": ["butane", "lighter fluid"], "shots": {"0": {"k": 0, "answer": "Zippo light fluid", "latency_ms": 209.6579074859619, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "lighter fluid or naphtha", "latency_ms": 391.91651344299316, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 605}, "2": {"k": 2, "answer": "lighter fluid or naphtha", "latency_ms": 394.2735195159912, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1227}, "3": {"k": 3, "answer": "lighter fluid or naphtha", "latency_ms": 237.6251220703125, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1804}, "4": {"k": 4, "answer": "lighter fluid or naphtha", "latency_ms": 364.69030380249023, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2397}, "5": {"k": 5, "answer": "lighter fluid or naphtha", "latency_ms": 202.96502113342285, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3053}, "6": {"k": 6, "answer": "lighter fluid or naphtha", "latency_ms": 511.9912624359131, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3645}}}
{"qid": "-2060506905455252030", "question": "first jnanpith award was an autor of which language", "gold_answers": ["Malayalam"], "shots": {"0": {"k": 0, "answer": "Kannada", "latency_ms": 66.72787666320801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Bengali", "latency_ms": 97.34129905700684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 669}, "2": {"k": 2, "answer": "Bengali", "latency_ms": 192.78573989868164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1332}, "3": {"k": 3, "answer": "Bengali", "latency_ms": 115.90576171875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1980}, "4": {"k": 4, "answer": "Bengali", "latency_ms": 113.04426193237305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2644}, "5": {"k": 5, "answer": "Bengali", "latency_ms": 138.36908340454102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3321}, "6": {"k": 6, "answer": "Bengali", "latency_ms": 214.27154541015625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3961}}}
{"qid": "4738663028303377024", "question": "who played the mad hatter in the batman tv show", "gold_answers": ["Roddy McDowall", "David Wayne", "Benedict Samuel"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 81.451416015625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 140.0446891784668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 641}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 213.01007270812988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1250}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 223.1912612915039, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 113.93022537231445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2562}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.92849540710449, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3232}, "6": {"k": 6, "answer": "Ed Wynn", "latency_ms": 129.41408157348633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3887}}}
{"qid": "-1650946015201779846", "question": "how many cracker barrels in the united states", "gold_answers": ["639", "more than 600"], "shots": {"0": {"k": 0, "answer": "over 1,400", "latency_ms": 154.71982955932617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "314", "latency_ms": 93.33229064941406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 642}, "2": {"k": 2, "answer": "645", "latency_ms": 113.79027366638184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1280}, "3": {"k": 3, "answer": "645", "latency_ms": 136.58857345581055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1964}, "4": {"k": 4, "answer": "645", "latency_ms": 166.95833206176758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2581}, "5": {"k": 5, "answer": "645", "latency_ms": 138.94248008728027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3236}, "6": {"k": 6, "answer": "645", "latency_ms": 290.433406829834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3882}}}
{"qid": "-3422350952909582054", "question": "in photosynthesis the carbon in co2 is initially fixed to what molecule", "gold_answers": ["3 - phosphoglycerate"], "shots": {"0": {"k": 0, "answer": "3-phosphoglycerate (3-PGA)", "latency_ms": 511.59000396728516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 537.501335144043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 712}, "2": {"k": 2, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 246.63281440734863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1383}, "3": {"k": 3, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 424.1752624511719, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2086}, "4": {"k": 4, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 884.1502666473389, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2731}, "5": {"k": 5, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 219.496488571167, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3457}, "6": {"k": 6, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 803.8749694824219, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4116}}}
{"qid": "-2704708103393811482", "question": "who voices randy in f is for family", "gold_answers": ["T.J. Miller"], "shots": {"0": {"k": 0, "answer": "John DiMaggio", "latency_ms": 240.83948135375977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Bill Burr", "latency_ms": 260.6630325317383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 582}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 374.77827072143555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1255}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 198.97174835205078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1927}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 198.92597198486328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2574}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 229.34865951538086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3180}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 184.76200103759766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3833}}}
{"qid": "5482012214308896475", "question": "how many countries does cadbury sell its products", "gold_answers": ["more than 50", "more than 50 countries worldwide"], "shots": {"0": {"k": 0, "answer": "Over 150 countries", "latency_ms": 224.70712661743164, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "30", "latency_ms": 180.19771575927734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 652}, "2": {"k": 2, "answer": "more than 50 countries", "latency_ms": 148.38576316833496, "em": 0.0, "f1": 0.888888888888889, "containment": 1.0, "context_chars": 1360}, "3": {"k": 3, "answer": "over 50 countries", "latency_ms": 123.2614517211914, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1983}, "4": {"k": 4, "answer": "over 50 countries", "latency_ms": 126.85656547546387, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2682}, "5": {"k": 5, "answer": "over 50 countries", "latency_ms": 432.76286125183105, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3410}, "6": {"k": 6, "answer": "over 50 countries", "latency_ms": 317.105770111084, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 4035}}}
{"qid": "-5737509619250561344", "question": "ethiopia flight 961 crashes in to the sea", "gold_answers": ["23 November 1996"], "shots": {"0": {"k": 0, "answer": "1996", "latency_ms": 87.42427825927734, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indian Ocean near Grande Comore, Comoros Islands", "latency_ms": 279.17003631591797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "Indian Ocean near Grande Comore, Comoros Islands", "latency_ms": 209.885835647583, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1359}, "3": {"k": 3, "answer": "Indian Ocean near Grande Comore, Comoros Islands", "latency_ms": 448.61459732055664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2016}, "4": {"k": 4, "answer": "no", "latency_ms": 110.58759689331055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2677}, "5": {"k": 5, "answer": "no", "latency_ms": 149.57737922668457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3382}, "6": {"k": 6, "answer": "no", "latency_ms": 265.8843994140625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4019}}}
{"qid": "-7175456993713717070", "question": "what does istj mean in a personality test", "gold_answers": ["Extroverted Thinking ( Te )", "Extroverted Intuition ( Ne )", "Introverted Feeling ( Fi )", "Introverted Sensing ( Si )"], "shots": {"0": {"k": 0, "answer": "Introverted, Sensing, Thinking, Judging", "latency_ms": 221.19951248168945, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "ISTJ is a personality type in the Myers-Briggs Type Indicator.", "latency_ms": 748.2428550720215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 724}, "2": {"k": 2, "answer": "ISTJ means Introversion, Sensing, Thinking, Judgment in the Myers-Briggs Type Indicator personality test.", "latency_ms": 907.5889587402344, "em": 0.0, "f1": 0.13333333333333333, "containment": 0.0, "context_chars": 1424}, "3": {"k": 3, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 493.3962821960449, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2085}, "4": {"k": 4, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 378.65352630615234, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2796}, "5": {"k": 5, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 420.92299461364746, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3424}, "6": {"k": 6, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 560.3482723236084, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 4117}}}
{"qid": "4437170343606050664", "question": "who does the voice of ash in sing", "gold_answers": ["Scarlett Johansson"], "shots": {"0": {"k": 0, "answer": "John Mulaney", "latency_ms": 122.24531173706055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Taron Egerton", "latency_ms": 250.7610321044922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 688}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 167.40822792053223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1401}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 148.33498001098633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2049}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 227.39124298095703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2716}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 175.7516860961914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3372}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 192.19541549682617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4036}}}
{"qid": "-8773010039077552394", "question": "who scored the most points in their nba career", "gold_answers": ["Kareem Abdul - Jabbar"], "shots": {"0": {"k": 0, "answer": "Kareem Abdul-Jabbar", "latency_ms": 181.8990707397461, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 203.39536666870117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 552}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 265.9027576446533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1159}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 111.02056503295898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1778}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 127.53629684448242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2390}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 193.24851036071777, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3031}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 228.6524772644043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3682}}}
{"qid": "-3278224144622911032", "question": "with a land area of 54 314 square miles where does wisconsin rank among the 50 states", "gold_answers": ["25"], "shots": {"0": {"k": 0, "answer": "10th largest", "latency_ms": 176.03731155395508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "23rd", "latency_ms": 145.07627487182617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 604}, "2": {"k": 2, "answer": "23rd", "latency_ms": 204.11062240600586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1213}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 202.23021507263184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1868}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 129.1959285736084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2468}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 196.486234664917, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3106}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 256.26635551452637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3695}}}
{"qid": "-3466298422174960041", "question": "who plays charles on have and have nots", "gold_answers": ["Nick Sager", "Nick Sagar"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 227.56552696228027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 180.30428886413574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 648}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 250.6256103515625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1294}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 102.65851020812988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1945}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 277.1275043487549, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2554}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 378.5862922668457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3251}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 213.15741539001465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3895}}}
{"qid": "1803248910342766083", "question": "who wrote the book my country my life", "gold_answers": ["L.K. Advani"], "shots": {"0": {"k": 0, "answer": "Nelson Mandela", "latency_ms": 147.53270149230957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 90.10028839111328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "L. K. Advani", "latency_ms": 265.15650749206543, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "L. K. Advani", "latency_ms": 155.303955078125, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1849}, "4": {"k": 4, "answer": "L. K. Advani", "latency_ms": 288.27714920043945, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2485}, "5": {"k": 5, "answer": "L. K. Advani", "latency_ms": 351.75395011901855, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3149}, "6": {"k": 6, "answer": "L. K. Advani", "latency_ms": 270.45416831970215, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3800}}}
{"qid": "-1176280233636154422", "question": "when did the east india company take control of india", "gold_answers": ["1757", "1799", "1612", "in 1757"], "shots": {"0": {"k": 0, "answer": "1757", "latency_ms": 78.55916023254395, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1600", "latency_ms": 152.66752243041992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 642}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 295.3634262084961, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1271}, "3": {"k": 3, "answer": "1818", "latency_ms": 239.18843269348145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1936}, "4": {"k": 4, "answer": "1757", "latency_ms": 211.21573448181152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2585}, "5": {"k": 5, "answer": "1757", "latency_ms": 161.65924072265625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3255}, "6": {"k": 6, "answer": "1757", "latency_ms": 292.70219802856445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3891}}}
{"qid": "5911119091653953181", "question": "where does the water come from to fill lake eyre", "gold_answers": ["Warburton River"], "shots": {"0": {"k": 0, "answer": "Rainfall and inflows from surrounding rivers and groundwater.", "latency_ms": 244.09914016723633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "north-east part of the Lake Eyre Basin", "latency_ms": 150.27523040771484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "north-east part of the Lake Eyre Basin, through the Channel Country, from rivers like the Diamantina, Georgina, Cooper Creek, Neales, and Macumba Rivers", "latency_ms": 877.1238327026367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1228}, "3": {"k": 3, "answer": "north-east part of the Lake Eyre Basin, through the Channel Country, via rivers like the Diamantina, Georgina, and Cooper Creek, and the Neales and Macumba Rivers", "latency_ms": 1321.5322494506836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1850}, "4": {"k": 4, "answer": "north-east part of the Lake Eyre Basin, via rivers such as the Diamantina, Georgina, and Cooper Creek", "latency_ms": 589.0662670135498, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2432}, "5": {"k": 5, "answer": "north-east part of the Lake Eyre Basin (in outback (south-west and central) Queensland)", "latency_ms": 592.6625728607178, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3014}, "6": {"k": 6, "answer": "north-east part of the Lake Eyre Basin, including rivers like the Diamantina, Georgina, Cooper Creek, Neales, and Macumba", "latency_ms": 1114.2938137054443, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3635}}}
{"qid": "5557104327881436095", "question": "who is the founder of google and when was it founded", "gold_answers": ["1998", "Sergey Brin", "Larry Page and Sergey Brin", "Larry Page"], "shots": {"0": {"k": 0, "answer": "Larry Page and Sergey Brin; 1998", "latency_ms": 650.1307487487793, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Larry Page and Sergey Brin; September 15, 1997", "latency_ms": 374.1497993469238, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 676}, "2": {"k": 2, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 291.912317276001, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 1340}, "3": {"k": 3, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 518.8779830932617, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 1980}, "4": {"k": 4, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 636.6057395935059, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 2647}, "5": {"k": 5, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 367.57612228393555, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 3327}, "6": {"k": 6, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 516.7758464813232, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 3983}}}
{"qid": "-1333328812971005142", "question": "who did the dominican republic gain its independence from", "gold_answers": ["Haiti"], "shots": {"0": {"k": 0, "answer": "Spain", "latency_ms": 161.94510459899902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Spain", "latency_ms": 228.3914089202881, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "Haiti", "latency_ms": 225.3429889678955, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1304}, "3": {"k": 3, "answer": "Haiti", "latency_ms": 149.04117584228516, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1938}, "4": {"k": 4, "answer": "Haiti", "latency_ms": 197.7839469909668, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2613}, "5": {"k": 5, "answer": "Haiti", "latency_ms": 240.7095432281494, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3260}, "6": {"k": 6, "answer": "Haiti", "latency_ms": 200.1025676727295, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3974}}}
{"qid": "1122605495295045742", "question": "how did early humans make use of stones during the prehistoric period", "gold_answers": ["start fires", "hunt", "bury their dead"], "shots": {"0": {"k": 0, "answer": "Chipped and shaped for tools", "latency_ms": 134.8114013671875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Shaped or \"chipped\" flint and chert for cutting tools and weapons; used basalt and sandstone for ground stone tools like quern-stones.", "latency_ms": 627.4676322937012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "Chipped flint and chert to make sharp edges for cutting tools and weapons.", "latency_ms": 927.7544021606445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1258}, "3": {"k": 3, "answer": "Chipped flint and chert to make cutting tools and weapons; used basalt and sandstone for ground stone tools; heated silcrete to create tempered blades and arrowheads.", "latency_ms": 1156.6545963287354, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1944}, "4": {"k": 4, "answer": "Chipped flint and other stones to make cutting tools, scrapers, and weapons; used heat treatment to modify silcrete into blades and arrowheads.", "latency_ms": 683.1192970275879, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2575}, "5": {"k": 5, "answer": "Chipped flint and chert for cutting tools, used hammerstones to flake stone cores, heated silcrete to temper tools, used polished stone axes, and made ground stone tools from basalt and sandstone.", "latency_ms": 1026.2846946716309, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3221}, "6": {"k": 6, "answer": "Chipped flint and other stones to make cutting tools, scrapers, choppers, and later heated and tempered silcrete into blades or arrowheads.", "latency_ms": 747.8501796722412, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3893}}}
{"qid": "7402184777947527945", "question": "who wrote the text for jeanie with the light brown hair", "gold_answers": ["Stephen Foster"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 80.36065101623535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Stephen Foster", "latency_ms": 254.77361679077148, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 632}, "2": {"k": 2, "answer": "Stephen Foster", "latency_ms": 108.85167121887207, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1281}, "3": {"k": 3, "answer": "Stephen Foster", "latency_ms": 154.03509140014648, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1871}, "4": {"k": 4, "answer": "Stephen Foster", "latency_ms": 104.89130020141602, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2488}, "5": {"k": 5, "answer": "Stephen Foster", "latency_ms": 142.87233352661133, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3071}, "6": {"k": 6, "answer": "Stephen Foster", "latency_ms": 198.04120063781738, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3686}}}
{"qid": "1656745334095178455", "question": "the basic steps of rational decision making include defining goals and", "gold_answers": ["Implementation of plans / policies"], "shots": {"0": {"k": 0, "answer": "identifying alternatives", "latency_ms": 102.42724418640137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "defining goals and verifying the problem", "latency_ms": 438.95673751831055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 774}, "2": {"k": 2, "answer": "defining the problem", "latency_ms": 118.54815483093262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1407}, "3": {"k": 3, "answer": "defining goals and problem identification", "latency_ms": 455.5685520172119, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2061}, "4": {"k": 4, "answer": "defining goals and identifying problems", "latency_ms": 420.95351219177246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2713}, "5": {"k": 5, "answer": "defining goals and identifying problems", "latency_ms": 395.3723907470703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3431}, "6": {"k": 6, "answer": "defining the problem, gathering information, evaluating alternatives, choosing the best solution, implementing the solution, and monitoring outcomes.", "latency_ms": 659.2061519622803, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4101}}}
{"qid": "-216657635823030751", "question": "when was the design period known as art deco", "gold_answers": ["1910 -- 1939"], "shots": {"0": {"k": 0, "answer": "1920s–1930s", "latency_ms": 212.2204303741455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "just before World War I", "latency_ms": 103.10482978820801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 680}, "2": {"k": 2, "answer": "just before World War I", "latency_ms": 230.4832935333252, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1362}, "3": {"k": 3, "answer": "just before World War I to 1939", "latency_ms": 318.30358505249023, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 2013}, "4": {"k": 4, "answer": "just before World War I to 1939", "latency_ms": 351.6535758972168, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 2721}, "5": {"k": 5, "answer": "just before World War I to 1939", "latency_ms": 497.69020080566406, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 3391}, "6": {"k": 6, "answer": "just before World War I to the latter 1930s", "latency_ms": 281.83913230895996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4054}}}
{"qid": "250542332339248886", "question": "where does the bob and tom show broadcast from", "gold_answers": ["WFBQ in Indianapolis , Indiana", "Indianapolis , Indiana"], "shots": {"0": {"k": 0, "answer": "Winston-Salem, North Carolina", "latency_ms": 113.31796646118164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indianapolis, Indiana", "latency_ms": 192.11077690124512, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "Indianapolis, Indiana", "latency_ms": 301.6476631164551, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "Indianapolis, Indiana", "latency_ms": 111.97638511657715, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1867}, "4": {"k": 4, "answer": "Indianapolis, Indiana", "latency_ms": 234.11107063293457, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2495}, "5": {"k": 5, "answer": "Indianapolis, Indiana", "latency_ms": 374.0062713623047, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3083}, "6": {"k": 6, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 462.51678466796875, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3695}}}
{"qid": "7610758795954724809", "question": "star wars the clone wars season 3 episode 1", "gold_answers": ["Clone Cadets"], "shots": {"0": {"k": 0, "answer": "The Clone Wars: Season 3, Episode 1 is titled \"The Clone Wars: Season 3, Episode 1\" and is known as \"The Clone Wars: Season 3, Episode 1\" in the series.", "latency_ms": 1306.082010269165, "em": 0.0, "f1": 0.07142857142857144, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Volume Two", "latency_ms": 89.15185928344727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 678}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 98.07276725769043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1321}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 140.86294174194336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1940}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 284.29126739501953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2574}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 240.3733730316162, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3224}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 196.14696502685547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3837}}}
{"qid": "2997223939033016160", "question": "what part of brain is responsible for complex thinking", "gold_answers": ["the frontal lobe"], "shots": {"0": {"k": 0, "answer": "prefrontal cortex", "latency_ms": 373.0273246765137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "frontal lobe", "latency_ms": 82.31687545776367, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "frontal lobe", "latency_ms": 141.36409759521484, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1386}, "3": {"k": 3, "answer": "frontal lobe", "latency_ms": 303.8818836212158, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2073}, "4": {"k": 4, "answer": "frontal lobe", "latency_ms": 133.3475112915039, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2747}, "5": {"k": 5, "answer": "frontal lobe", "latency_ms": 182.86824226379395, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3443}, "6": {"k": 6, "answer": "frontal lobe", "latency_ms": 334.60307121276855, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4141}}}
{"qid": "2680196226239522881", "question": "who won the champions league final in 2016", "gold_answers": ["Real Madrid"], "shots": {"0": {"k": 0, "answer": "Real Madrid", "latency_ms": 56.27155303955078, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Real Madrid", "latency_ms": 64.05973434448242, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 616}, "2": {"k": 2, "answer": "Real Madrid", "latency_ms": 263.2031440734863, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1211}, "3": {"k": 3, "answer": "Real Madrid", "latency_ms": 124.4966983795166, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1837}, "4": {"k": 4, "answer": "Real Madrid", "latency_ms": 113.9688491821289, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2442}, "5": {"k": 5, "answer": "Real Madrid", "latency_ms": 122.04670906066895, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3058}, "6": {"k": 6, "answer": "Real Madrid", "latency_ms": 267.1360969543457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3689}}}
{"qid": "-8458006812258546025", "question": "who has the most gold medals in the winter olympics of all time", "gold_answers": ["Norway"], "shots": {"0": {"k": 0, "answer": "Cross-country skier, Norwegian, with multiple golds, but the most total gold medals in Winter Olympics history is held by **Anders Haugen**? No — correct answer is **Knut Hjelme**? No.\n\nCorrect answer: **Eddie Eagan**? No.\n\nCorrect answer: **Sven Hannawald**? No.\n\nCorrect answer: **Bjørn Dæhlie** — he has the most Winter Olympic gold medals in history.\n\nFinal answer: **Bjørn Dæhlie**. \n\nBut actually, the record holder for most Winter Olympic gold medals is **Bjørn Dæhlie** with 5 golds.\n\nYes.\n\nAnswer: **Bjørn Dæhlie**", "latency_ms": 3917.1841144561768, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 178.9417266845703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Canada", "latency_ms": 102.91314125061035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1228}, "3": {"k": 3, "answer": "Norway", "latency_ms": 108.2909107208252, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1835}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 275.5441665649414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2480}, "5": {"k": 5, "answer": "Bjørn Dæhlie", "latency_ms": 408.59460830688477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3095}, "6": {"k": 6, "answer": "Bjørn Dæhlie", "latency_ms": 445.3315734863281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3750}}}
