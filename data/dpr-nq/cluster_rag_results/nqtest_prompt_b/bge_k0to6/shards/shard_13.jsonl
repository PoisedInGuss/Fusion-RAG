{"qid": "175824256524792922", "question": "who plays harry's mom in harry potter", "gold_answers": ["Geraldine Margaret Agnew - Somerville"], "shots": {"0": {"k": 0, "answer": "Rita Skeeter", "latency_ms": 156.60619735717773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Julie Walters", "latency_ms": 83.5113525390625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 676}, "2": {"k": 2, "answer": "Lily Evans", "latency_ms": 218.27983856201172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1287}, "3": {"k": 3, "answer": "Lily Potter", "latency_ms": 192.75975227355957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1890}, "4": {"k": 4, "answer": "Lily Potter", "latency_ms": 257.82108306884766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2504}, "5": {"k": 5, "answer": "Lily Potter", "latency_ms": 112.41674423217773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3198}, "6": {"k": 6, "answer": "Lily Potter", "latency_ms": 161.2071990966797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3801}}}
{"qid": "-8223571798173928629", "question": "who sang you'll be a woman soon in pulp fiction", "gold_answers": ["Urge Overkill", "rock band Urge Overkill"], "shots": {"0": {"k": 0, "answer": "Minnie Riperton", "latency_ms": 204.1606903076172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Urge Overkill", "latency_ms": 179.23712730407715, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 653}, "2": {"k": 2, "answer": "Urge Overkill", "latency_ms": 98.42848777770996, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1277}, "3": {"k": 3, "answer": "Urge Overkill", "latency_ms": 197.41559028625488, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1899}, "4": {"k": 4, "answer": "Urge Overkill", "latency_ms": 141.95942878723145, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2572}, "5": {"k": 5, "answer": "Urge Overkill", "latency_ms": 321.7751979827881, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3204}, "6": {"k": 6, "answer": "Urge Overkill", "latency_ms": 230.74078559875488, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3838}}}
{"qid": "-8095180930653230478", "question": "where does a roadrunner live in the desert", "gold_answers": ["arid lowland", "mountainous shrubland"], "shots": {"0": {"k": 0, "answer": "Southwestern United States and Mexico", "latency_ms": 266.86739921569824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 168.00522804260254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 684}, "2": {"k": 2, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 344.6331024169922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1328}, "3": {"k": 3, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 381.5305233001709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1977}, "4": {"k": 4, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 713.7908935546875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2622}, "5": {"k": 5, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 344.8033332824707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3354}, "6": {"k": 6, "answer": "southwestern United States, Mexico, and Central America", "latency_ms": 452.84485816955566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3968}}}
{"qid": "-5489426796364143729", "question": "who was the first president of the constituent assembly", "gold_answers": ["Dr. Rajendra Prasad"], "shots": {"0": {"k": 0, "answer": "Dr. Rajendra Prasad", "latency_ms": 293.17641258239746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dr. Rajendra Prasad", "latency_ms": 132.79438018798828, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 743}, "2": {"k": 2, "answer": "Dr. Rajendra Prasad", "latency_ms": 236.5279197692871, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1429}, "3": {"k": 3, "answer": "Dr. Rajendra Prasad", "latency_ms": 232.89179801940918, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2127}, "4": {"k": 4, "answer": "Dr. Rajendra Prasad", "latency_ms": 350.02970695495605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2802}, "5": {"k": 5, "answer": "Dr. Rajendra Prasad", "latency_ms": 159.61337089538574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3473}, "6": {"k": 6, "answer": "Dr. Rajendra Prasad", "latency_ms": 163.43283653259277, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4153}}}
{"qid": "7835345385704208729", "question": "when does the new episode of scorpion come on", "gold_answers": ["January 15 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 75.30426979064941, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 169.83270645141602, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 665}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 125.76675415039062, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1302}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 130.45334815979004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1907}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 231.46486282348633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2503}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 308.8414669036865, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3103}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 269.5574760437012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3720}}}
{"qid": "907238223200158003", "question": "when did first fast and furious come out", "gold_answers": ["2001"], "shots": {"0": {"k": 0, "answer": "2001", "latency_ms": 267.9426670074463, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "June 22, 2001", "latency_ms": 315.92416763305664, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 672}, "2": {"k": 2, "answer": "June 22, 2001", "latency_ms": 255.38277626037598, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1304}, "3": {"k": 3, "answer": "June 22, 2001", "latency_ms": 541.9745445251465, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1934}, "4": {"k": 4, "answer": "June 22, 2001", "latency_ms": 523.6697196960449, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2592}, "5": {"k": 5, "answer": "June 22, 2001", "latency_ms": 331.77924156188965, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3216}, "6": {"k": 6, "answer": "June 22, 2001", "latency_ms": 424.9274730682373, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3847}}}
{"qid": "-6782664238831495856", "question": "when does season 5 of the blacklist resume", "gold_answers": ["January 31 , 2018"], "shots": {"0": {"k": 0, "answer": "2018", "latency_ms": 160.55607795715332, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wednesday at 8:00 PM", "latency_ms": 160.3870391845703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 663}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 223.39916229248047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1361}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 233.55436325073242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2052}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 317.0034885406494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2724}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 188.19856643676758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3353}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 262.44473457336426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4035}}}
{"qid": "3492797766982308854", "question": "when do mr schuester and emma get together", "gold_answers": ["the fourth season"], "shots": {"0": {"k": 0, "answer": "Season 4, Episode 10 \"The First Time\"", "latency_ms": 662.6443862915039, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 103.1942367553711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 562}, "2": {"k": 2, "answer": "after Sectionals", "latency_ms": 104.83169555664062, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1194}, "3": {"k": 3, "answer": "after the mid-season finale episode \"Sectionals\"", "latency_ms": 560.1017475128174, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1811}, "4": {"k": 4, "answer": "after the mid-season finale episode \"Sectionals\"", "latency_ms": 493.2210445404053, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2424}, "5": {"k": 5, "answer": "after the mid-season finale episode \"Sectionals\"", "latency_ms": 336.0116481781006, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3082}, "6": {"k": 6, "answer": "after the mid-season finale episode \"Sectionals\"", "latency_ms": 578.6478519439697, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3671}}}
{"qid": "-2654989306076235475", "question": "when does the next episode of izombie air", "gold_answers": ["May 14 , 2018", "May 7 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 102.44607925415039, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 100.83174705505371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 596}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 120.57375907897949, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1192}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 184.97157096862793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1809}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 258.5477828979492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2409}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 156.23021125793457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3046}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 232.85222053527832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3691}}}
{"qid": "3066393401707107613", "question": "where was the u.s.s maine when it exploded in 1898", "gold_answers": ["Havana Harbor"], "shots": {"0": {"k": 0, "answer": "Havana, Cuba", "latency_ms": 232.16891288757324, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Havana, Cuba", "latency_ms": 205.7499885559082, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 594}, "2": {"k": 2, "answer": "Havana, Cuba", "latency_ms": 149.52588081359863, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1213}, "3": {"k": 3, "answer": "Havana, Cuba", "latency_ms": 331.29262924194336, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1877}, "4": {"k": 4, "answer": "Havana, Cuba", "latency_ms": 139.59860801696777, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2459}, "5": {"k": 5, "answer": "Havana Harbor, Cuba", "latency_ms": 210.34789085388184, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3093}, "6": {"k": 6, "answer": "Havana Harbor, Cuba", "latency_ms": 421.15116119384766, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3717}}}
{"qid": "6972056626325669866", "question": "in the honour of which god is anant chaturdashi celebrated", "gold_answers": ["Ganesh"], "shots": {"0": {"k": 0, "answer": "Shiva", "latency_ms": 120.92852592468262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 488.43955993652344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 130.92041015625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1272}, "3": {"k": 3, "answer": "Anant Chaturdashi is not specifically celebrated in honour of a single god. It is a festival observed by both Jains and Hindus, with significance in the Jain calendar as the last day of Paryushana. While it may have connections to various deities, it is not primarily dedicated to one specific god. \n\nI cannot answer.", "latency_ms": 2248.509168624878, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1935}, "4": {"k": 4, "answer": "Anant Chaturdashi is not specifically celebrated in honour of a single god. It is a festival observed by both Jains and Hindus, with Jain observances centered on Paryushana and Kshamavani, and no specific deity mentioned in the context. Therefore, the answer is: I cannot answer.", "latency_ms": 2403.249502182007, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2576}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 347.78308868408203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3171}, "6": {"k": 6, "answer": "Jain calendar festivities", "latency_ms": 423.97475242614746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3800}}}
{"qid": "6043389452532567502", "question": "which central american nations border the pacific ocean and the caribbean sea", "gold_answers": ["Costa Rica", "Nicaragua"], "shots": {"0": {"k": 0, "answer": "Belize, Guatemala, Honduras, Nicaragua, Costa Rica", "latency_ms": 416.54157638549805, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Honduras", "latency_ms": 69.44394111633301, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 597}, "2": {"k": 2, "answer": "Honduras, Guatemala", "latency_ms": 305.50384521484375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1271}, "3": {"k": 3, "answer": "Honduras, Guatemala, Nicaragua", "latency_ms": 336.2104892730713, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1958}, "4": {"k": 4, "answer": "Honduras, Guatemala", "latency_ms": 331.41183853149414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2618}, "5": {"k": 5, "answer": "Honduras, Guatemala, Nicaragua, Panama", "latency_ms": 394.5460319519043, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3282}, "6": {"k": 6, "answer": "Honduras, Nicaragua, Guatemala, Belize, Costa Rica, Panama", "latency_ms": 344.1650867462158, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 3948}}}
{"qid": "2541316470469570888", "question": "what kind of sentence contains an independent clause and a dependent clause", "gold_answers": ["A complex sentence", "complex sentence"], "shots": {"0": {"k": 0, "answer": "complex sentence", "latency_ms": 103.35040092468262, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "compound-complex sentence", "latency_ms": 87.9368782043457, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 708}, "2": {"k": 2, "answer": "complex sentence", "latency_ms": 70.77693939208984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1372}, "3": {"k": 3, "answer": "complex sentence", "latency_ms": 98.92129898071289, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2028}, "4": {"k": 4, "answer": "complex sentence", "latency_ms": 138.3523941040039, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2776}, "5": {"k": 5, "answer": "complex sentence", "latency_ms": 282.2418212890625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3435}, "6": {"k": 6, "answer": "complex sentence", "latency_ms": 136.92736625671387, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4059}}}
{"qid": "3239205946903665891", "question": "what was the book wealth of nations about", "gold_answers": ["what builds nations ' wealth"], "shots": {"0": {"k": 0, "answer": "economic systems and trade principles", "latency_ms": 176.76830291748047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "The division of labour, productivity, and free markets", "latency_ms": 269.4892883300781, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "The Wealth of Nations was about the nature and causes of the wealth of nations, including division of labour, productivity, and free markets.", "latency_ms": 1003.382682800293, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 1326}, "3": {"k": 3, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of national wealth, including the division of labour, productivity, and free markets.", "latency_ms": 1014.7016048431396, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 1998}, "4": {"k": 4, "answer": "The book \"Wealth of Nations\" is about the nature and causes of national wealth, including the division of labor, productivity, and free markets, and it provides a foundational work in classical economics.", "latency_ms": 1318.8903331756592, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 2664}, "5": {"k": 5, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of national wealth, including the division of labour, productivity, and free markets, and it provides a foundational analysis of economic systems during the Industrial Revolution.", "latency_ms": 1817.162036895752, "em": 0.0, "f1": 0.11428571428571428, "containment": 0.0, "context_chars": 3334}, "6": {"k": 6, "answer": "The book \"The Wealth of Nations\" is about the nature and causes of national wealth, including the division of labor, productivity, and free markets.", "latency_ms": 1462.86940574646, "em": 0.0, "f1": 0.16666666666666669, "containment": 0.0, "context_chars": 3953}}}
{"qid": "-4659426898177460954", "question": "when will the flash season 4 be released", "gold_answers": ["October 10 , 2017"], "shots": {"0": {"k": 0, "answer": "2024", "latency_ms": 247.4672794342041, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "October 10, 2017", "latency_ms": 718.7972068786621, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "October 10, 2017", "latency_ms": 315.96970558166504, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "August 28, 2018", "latency_ms": 312.9425048828125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1852}, "4": {"k": 4, "answer": "August 28, 2018", "latency_ms": 336.7156982421875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2541}, "5": {"k": 5, "answer": "August 28, 2018", "latency_ms": 407.0155620574951, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3156}, "6": {"k": 6, "answer": "August 28, 2018", "latency_ms": 400.82287788391113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3780}}}
{"qid": "-1204722541964024977", "question": "what is australia's location in the world and region", "gold_answers": ["Oceania"], "shots": {"0": {"k": 0, "answer": "Southern Hemisphere, Eastern Hemisphere, Oceania region", "latency_ms": 229.51388359069824, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Oceania, between the Indian Ocean and the South Pacific Ocean", "latency_ms": 200.52814483642578, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 640}, "2": {"k": 2, "answer": "Oceania, between the Indian Ocean and the South Pacific Ocean", "latency_ms": 270.95484733581543, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 1341}, "3": {"k": 3, "answer": "Oceania, between the Indian Ocean and the South Pacific Ocean", "latency_ms": 472.3339080810547, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 1985}, "4": {"k": 4, "answer": "Oceania, between the Indian Ocean and the South Pacific Ocean", "latency_ms": 387.2380256652832, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 2644}, "5": {"k": 5, "answer": "Oceania, between the Indian Ocean and the South Pacific Ocean", "latency_ms": 279.90245819091797, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 3332}, "6": {"k": 6, "answer": "Oceania, between the Indian Ocean and the South Pacific Ocean", "latency_ms": 238.2030487060547, "em": 0.0, "f1": 0.2222222222222222, "containment": 1.0, "context_chars": 4057}}}
{"qid": "-6291722830448463311", "question": "who does luke skywalker fall in love with", "gold_answers": ["Mara Jade"], "shots": {"0": {"k": 0, "answer": "Padmé Amidala", "latency_ms": 75.80399513244629, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Camie, Nakari Kelen, Mara Jade", "latency_ms": 263.60344886779785, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 632}, "2": {"k": 2, "answer": "Camie, Nakari Kelen, Mara Jade", "latency_ms": 224.81799125671387, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1219}, "3": {"k": 3, "answer": "Camie, Nakari Kelen, Mara Jade", "latency_ms": 430.71961402893066, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1845}, "4": {"k": 4, "answer": "Leia", "latency_ms": 137.01820373535156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2450}, "5": {"k": 5, "answer": "Leia, Nakari Kelen, Mara Jade", "latency_ms": 414.26944732666016, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3080}, "6": {"k": 6, "answer": "Leia, Nakari Kelen, Mara Jade", "latency_ms": 649.9013900756836, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3675}}}
{"qid": "-5631283176059213935", "question": "who was the original host of jokers wild", "gold_answers": ["Jack Barry"], "shots": {"0": {"k": 0, "answer": "Ralph Edwards", "latency_ms": 148.2541561126709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 186.5520477294922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "Allen Ludden", "latency_ms": 88.5324478149414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1229}, "3": {"k": 3, "answer": "Allen Ludden", "latency_ms": 243.83091926574707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1822}, "4": {"k": 4, "answer": "Allen Ludden", "latency_ms": 125.14472007751465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2461}, "5": {"k": 5, "answer": "Allen Ludden", "latency_ms": 193.80712509155273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3091}, "6": {"k": 6, "answer": "Allen Ludden", "latency_ms": 299.00145530700684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3641}}}
{"qid": "5360223163814883105", "question": "which organelle is the site of atp production and storage", "gold_answers": ["mitochondrion"], "shots": {"0": {"k": 0, "answer": "mitochondria", "latency_ms": 101.44734382629395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "mitochondrion", "latency_ms": 143.9073085784912, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 707}, "2": {"k": 2, "answer": "mitochondria", "latency_ms": 270.65110206604004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1355}, "3": {"k": 3, "answer": "mitochondria", "latency_ms": 296.109676361084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2072}, "4": {"k": 4, "answer": "mitochondria", "latency_ms": 174.29828643798828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2713}, "5": {"k": 5, "answer": "mitochondria", "latency_ms": 131.42085075378418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3372}, "6": {"k": 6, "answer": "mitochondria", "latency_ms": 116.52970314025879, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4069}}}
{"qid": "-718217992489656032", "question": "five tools of security management to overcome computer crime", "gold_answers": ["post-evaluation", "operative planning", "implementation", "Pre-evaluation", "strategic planning"], "shots": {"0": {"k": 0, "answer": "Risk assessment, access control, encryption, intrusion detection, employee training", "latency_ms": 306.71048164367676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Background checks, keystroke tracking, biometric ID, additional ID for online purchases, camera surveillance at ATMs and point of sale", "latency_ms": 810.7120990753174, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 771}, "2": {"k": 2, "answer": "Background checks, keystroke tracking, biometric ID, additional ID for purchases, camera surveillance at ATMs and POS", "latency_ms": 853.804349899292, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1464}, "3": {"k": 3, "answer": "background checks, keystroke tracking, biometric ID, spam monitoring, incident response management", "latency_ms": 378.71551513671875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2173}, "4": {"k": 4, "answer": "background checks, keystroke tracking, biometric ID, camera surveillance, virus scanning", "latency_ms": 458.1449031829834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2897}, "5": {"k": 5, "answer": "Reinforcing authentication, monitoring Internet sites, harsh penalties for hacking, virus scan, reverse engineering", "latency_ms": 740.2911186218262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3580}, "6": {"k": 6, "answer": "Firewalls, encryption, card/password access, virus scanning, incident response management", "latency_ms": 257.7347755432129, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4298}}}
{"qid": "5584540254904933863", "question": "who sang what are we doing in love", "gold_answers": ["Dottie West", "Kenny Rogers"], "shots": {"0": {"k": 0, "answer": "The Supremes", "latency_ms": 132.89642333984375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dottie West and Kenny Rogers", "latency_ms": 306.9627285003662, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 583}, "2": {"k": 2, "answer": "Dottie West and Kenny Rogers", "latency_ms": 226.41968727111816, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1170}, "3": {"k": 3, "answer": "Dottie West and Kenny Rogers", "latency_ms": 502.0439624786377, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1840}, "4": {"k": 4, "answer": "Dottie West and Kenny Rogers", "latency_ms": 275.4838466644287, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2442}, "5": {"k": 5, "answer": "Dottie West and Kenny Rogers", "latency_ms": 152.74739265441895, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3118}, "6": {"k": 6, "answer": "Dottie West and Kenny Rogers", "latency_ms": 261.86227798461914, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3729}}}
{"qid": "97369777493072047", "question": "at what age are you no longer a toddler", "gold_answers": ["36 months"], "shots": {"0": {"k": 0, "answer": "3 years old", "latency_ms": 66.97344779968262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "36 months", "latency_ms": 100.20303726196289, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 601}, "2": {"k": 2, "answer": "36 months", "latency_ms": 241.71710014343262, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1220}, "3": {"k": 3, "answer": "36 months", "latency_ms": 348.8473892211914, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1855}, "4": {"k": 4, "answer": "36 months", "latency_ms": 89.98632431030273, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2438}, "5": {"k": 5, "answer": "36 months", "latency_ms": 197.2799301147461, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3138}, "6": {"k": 6, "answer": "36 months", "latency_ms": 213.1330966949463, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3823}}}
{"qid": "3537093784371401720", "question": "how many episodes are there in ordeal by innocence", "gold_answers": ["three", "three part"], "shots": {"0": {"k": 0, "answer": "100", "latency_ms": 103.75690460205078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 107.74016380310059, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 180.10425567626953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1284}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 142.47846603393555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1881}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 313.90857696533203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2472}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 371.51288986206055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3068}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 382.39002227783203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3699}}}
{"qid": "-2126094349681352450", "question": "who played the hobbits in the lord of the rings", "gold_answers": ["Ian Holm", "Dominic Monaghan", "Elijah Wood as Frodo Baggins", "Sean Astin", "Elijah Wood", "Billy Boyd"], "shots": {"0": {"k": 0, "answer": "Elijah Wood, Vivien Leigh, Ian McKellen, Orlando Bloom, Sean Astin, Dominic Monaghan, and Billy Boyd", "latency_ms": 841.3550853729248, "em": 0.0, "f1": 0.2352941176470588, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Elijah Wood", "latency_ms": 125.90622901916504, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 695}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 217.56529808044434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1302}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 332.6084613800049, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1968}, "4": {"k": 4, "answer": "Elijah Wood, Sean Astin, Billy Boyd, Dominic Monaghan", "latency_ms": 565.0491714477539, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2622}, "5": {"k": 5, "answer": "Elijah Wood, Sean Astin, Billy Boyd, Dominic Monaghan", "latency_ms": 362.63370513916016, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3283}, "6": {"k": 6, "answer": "Elijah Wood, Sean Astin, Billy Boyd, Dominic Monaghan", "latency_ms": 705.3744792938232, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3902}}}
{"qid": "-6982991192528081677", "question": "what does fancy dress mean in the uk", "gold_answers": ["costumes", "A costume", "guests dress up in costumes"], "shots": {"0": {"k": 0, "answer": "Wearing costumes for a party or event, often for fun or celebration.", "latency_ms": 221.1897373199463, "em": 0.0, "f1": 0.16666666666666669, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 365.62514305114746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 597}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 269.8497772216797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1171}, "3": {"k": 3, "answer": "formal dress", "latency_ms": 108.54411125183105, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1793}, "4": {"k": 4, "answer": "frocks up", "latency_ms": 169.03352737426758, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2424}, "5": {"k": 5, "answer": "frocks or gowns for special occasions", "latency_ms": 312.49475479125977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3038}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 226.1519432067871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3692}}}
{"qid": "-6751220433242447969", "question": "how many seasons of the bastard executioner are there", "gold_answers": ["one season", "one"], "shots": {"0": {"k": 0, "answer": "3", "latency_ms": 82.63421058654785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 91.65358543395996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 671}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 97.7182388305664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1366}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 109.14015769958496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2009}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 147.85432815551758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2627}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 205.336332321167, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3253}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 222.3494052886963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3866}}}
{"qid": "-7607921794902030568", "question": "why did jean valjean take care of cosette", "gold_answers": ["he had promised"], "shots": {"0": {"k": 0, "answer": "To save her from being taken by the authorities and to protect her from suffering.", "latency_ms": 298.63452911376953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "to protect Cosette from Thénardier's corruption and to prevent harm to her and her family", "latency_ms": 668.4055328369141, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 627}, "2": {"k": 2, "answer": "to raise her as his own", "latency_ms": 219.75135803222656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1254}, "3": {"k": 3, "answer": "to raise her as his own", "latency_ms": 139.1470432281494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1897}, "4": {"k": 4, "answer": "to raise her as his own", "latency_ms": 176.57113075256348, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2503}, "5": {"k": 5, "answer": "to raise her as his own after promising Fantine he would do so", "latency_ms": 616.5492534637451, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 3103}, "6": {"k": 6, "answer": "to raise her as his own after promising Fantine", "latency_ms": 572.4034309387207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3723}}}
{"qid": "-5341729935918652247", "question": "who does the democratic republic of congo trade with", "gold_answers": ["Belgium", "World Bank", "China", "Zambia", "African Development Bank", "SADC", "South Africa", "AU", "WTO", "France", "Kenya", "Zimbabwe", "Group of 77", "IMF"], "shots": {"0": {"k": 0, "answer": "China, Belgium, France, Zambia, Angola, South Africa, Brazil, Russia, United States, Nigeria, Kenya, Tanzania, Rwanda, Uganda, Zambia, Zimbabwe, Namibia, Botswana, South Africa, Egypt, Saudi Arabia, India, Japan, South Korea, Germany, Italy, United Kingdom, Canada, Australia, New Zealand, South Korea, United Arab Emirates, Qatar, Singapore, Malaysia, Indonesia, Thailand, Vietnam, Philippines, Mexico, Colombia, Peru, Chile, Argentina, Brazil, South Africa, Nigeria, Ghana, Kenya, Ethiopia, Sudan, Egypt, Saudi Arabia, UAE, India, Japan, South Korea, Germany, Italy, UK, Canada, Australia, New Zealand, Mexico, Colombia, Peru, Chile, Argentina, Brazil, South Africa, Nigeria, Ghana, Kenya, Ethiopia, Sudan, Egypt, Saudi Arabia, UAE, India, Japan, South Korea, Germany, Italy, UK, Canada, Australia, New Zealand, Mexico, Colombia, Peru, Chile, Argentina, Brazil, South Africa, Nigeria, Ghana, Kenya, Ethiopia, Sudan, Egypt, Saudi Arabia, UAE, India, Japan, South Korea, Germany, Italy, UK, Canada, Australia, New Zealand, Mexico, Colombia, Peru, Chile, Argentina,", "latency_ms": 6698.110580444336, "em": 0.0, "f1": 0.02857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kenya", "latency_ms": 124.3889331817627, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 692}, "2": {"k": 2, "answer": "Kenya, China", "latency_ms": 176.06163024902344, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1395}, "3": {"k": 3, "answer": "Kenya, China", "latency_ms": 109.37118530273438, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2096}, "4": {"k": 4, "answer": "Kenya, China, India", "latency_ms": 254.69708442687988, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2807}, "5": {"k": 5, "answer": "Kenya, China, India", "latency_ms": 437.9448890686035, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3497}, "6": {"k": 6, "answer": "Kenya, China, India", "latency_ms": 228.43337059020996, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 4317}}}
{"qid": "5518515944351776633", "question": "what was the purpose of the bantu education act", "gold_answers": ["enforcing racially separated educational facilities"], "shots": {"0": {"k": 0, "answer": "To provide a racially segregated and inferior education for Black South Africans.", "latency_ms": 285.6152057647705, "em": 0.0, "f1": 0.12500000000000003, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "enforce racially separated educational facilities", "latency_ms": 388.54408264160156, "em": 0.0, "f1": 0.8000000000000002, "containment": 0.0, "context_chars": 675}, "2": {"k": 2, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 404.53553199768066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1289}, "3": {"k": 3, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 511.821985244751, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2005}, "4": {"k": 4, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 608.0424785614014, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2681}, "5": {"k": 5, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 552.7427196502686, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3380}, "6": {"k": 6, "answer": "to ensure that blacks would have only just enough education to work as unskilled laborers", "latency_ms": 744.7288036346436, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4082}}}
{"qid": "-4271099756149693907", "question": "what type of snake is kaa in jungle book", "gold_answers": ["Indian rock python"], "shots": {"0": {"k": 0, "answer": "Python", "latency_ms": 170.05014419555664, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kaa is a giant python.", "latency_ms": 119.05097961425781, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "Kaa is a giant python.", "latency_ms": 345.9146022796631, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1174}, "3": {"k": 3, "answer": "Kaa is a python.", "latency_ms": 258.4800720214844, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1759}, "4": {"k": 4, "answer": "Kaa is a snake.", "latency_ms": 651.5078544616699, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2455}, "5": {"k": 5, "answer": "Kaa is a snake.", "latency_ms": 271.49081230163574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3029}, "6": {"k": 6, "answer": "Kaa is a giant, powerful snake.", "latency_ms": 241.84703826904297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3615}}}
{"qid": "-6527462410412242988", "question": "what is the snow on mars made of", "gold_answers": ["water ice", "Frozen carbon dioxide"], "shots": {"0": {"k": 0, "answer": "water ice and carbon dioxide ice", "latency_ms": 234.87448692321777, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "CO2 frost", "latency_ms": 106.35685920715332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "CO2 frost", "latency_ms": 209.71393585205078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1207}, "3": {"k": 3, "answer": "carbon dioxide ice (dry ice) and water ice", "latency_ms": 436.2952709197998, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1833}, "4": {"k": 4, "answer": "ice and dust", "latency_ms": 125.2748966217041, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2480}, "5": {"k": 5, "answer": "carbon dioxide ice (dry ice)", "latency_ms": 493.2410717010498, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3174}, "6": {"k": 6, "answer": "carbon dioxide ice (dry ice)", "latency_ms": 252.66122817993164, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3756}}}
{"qid": "-2155345050664800035", "question": "when does elena turn into a vampire in the tv series", "gold_answers": ["season four", "fourth season"], "shots": {"0": {"k": 0, "answer": "Season 1, Episode 4", "latency_ms": 191.31064414978027, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "fourth season", "latency_ms": 89.43796157836914, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 577}, "2": {"k": 2, "answer": "fourth season", "latency_ms": 78.27210426330566, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1178}, "3": {"k": 3, "answer": "season four", "latency_ms": 69.3519115447998, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1770}, "4": {"k": 4, "answer": "season 4", "latency_ms": 117.64359474182129, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2349}, "5": {"k": 5, "answer": "season 4", "latency_ms": 148.95868301391602, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2949}, "6": {"k": 6, "answer": "season 4", "latency_ms": 116.75524711608887, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3537}}}
{"qid": "5491319565508469860", "question": "who gets first pick of fa cup games", "gold_answers": ["the BBC", "BBC"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 144.74081993103027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dennis Wise and Dion Dublin", "latency_ms": 490.57650566101074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 574}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 218.74237060546875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1163}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 135.33639907836914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1751}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 342.36621856689453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2352}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 178.21168899536133, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2948}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 159.80100631713867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3521}}}
{"qid": "-816889472900831237", "question": "what part of dc is howard university in", "gold_answers": ["northwest", "northwest Washington", "in northwest Washington"], "shots": {"0": {"k": 0, "answer": "NW (Northeast Washington)", "latency_ms": 372.93171882629395, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Washington, D.C.", "latency_ms": 276.6718864440918, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 691}, "2": {"k": 2, "answer": "Northwest quadrant of Washington, D.C.", "latency_ms": 388.547420501709, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1348}, "3": {"k": 3, "answer": "Northwest quadrant of Washington, D.C.", "latency_ms": 555.7348728179932, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2005}, "4": {"k": 4, "answer": "Northwest quadrant of Washington, D.C.", "latency_ms": 267.9269313812256, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2689}, "5": {"k": 5, "answer": "Northwest quadrant of Washington, D.C.", "latency_ms": 513.0703449249268, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3339}, "6": {"k": 6, "answer": "Northwest quadrant of Washington, D.C.", "latency_ms": 367.3849105834961, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 4052}}}
{"qid": "479254158383415496", "question": "what is the first covenant in the old testament", "gold_answers": ["the Noahic Covenant"], "shots": {"0": {"k": 0, "answer": "Covenant of Noah", "latency_ms": 151.4453887939453, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Noahic", "latency_ms": 242.23875999450684, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 648}, "2": {"k": 2, "answer": "Noahic", "latency_ms": 309.0198040008545, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1320}, "3": {"k": 3, "answer": "Noahic", "latency_ms": 253.44300270080566, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1958}, "4": {"k": 4, "answer": "Noahic covenant", "latency_ms": 319.90504264831543, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2569}, "5": {"k": 5, "answer": "Noahic covenant", "latency_ms": 356.001615524292, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3125}, "6": {"k": 6, "answer": "Noahic covenant", "latency_ms": 200.85644721984863, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3745}}}
{"qid": "-8025421046364238920", "question": "who sings the theme song for living single", "gold_answers": ["Queen Latifah"], "shots": {"0": {"k": 0, "answer": "Lil' Kim", "latency_ms": 197.0803737640381, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 84.38229560852051, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 593}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 179.3806552886963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1224}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 174.46017265319824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1812}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 253.22675704956055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2424}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 128.97205352783203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3035}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 129.5490264892578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3665}}}
{"qid": "-2347826883564742884", "question": "who said life is like a bowl of cherries", "gold_answers": ["Lew Brown"], "shots": {"0": {"k": 0, "answer": "W. C. Fields", "latency_ms": 117.4008846282959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 83.66990089416504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 577}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 179.87942695617676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 149.44076538085938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1918}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 212.29958534240723, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2548}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 295.20130157470703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3197}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 218.10030937194824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3992}}}
{"qid": "2467509477831469982", "question": "where is fort myers located at in florida", "gold_answers": ["Lee County", "the Southwest"], "shots": {"0": {"k": 0, "answer": "South Florida", "latency_ms": 246.53863906860352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lee County, Florida", "latency_ms": 109.40217971801758, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 642}, "2": {"k": 2, "answer": "Fort Myers, Florida is located in Lee County, United States.", "latency_ms": 480.5595874786377, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1259}, "3": {"k": 3, "answer": "Lee County, Florida", "latency_ms": 113.33227157592773, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1906}, "4": {"k": 4, "answer": "Fort Myers, Florida", "latency_ms": 229.28190231323242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2553}, "5": {"k": 5, "answer": "Lee County, Florida", "latency_ms": 371.4182376861572, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3204}, "6": {"k": 6, "answer": "Lee County, Florida", "latency_ms": 334.3484401702881, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3848}}}
{"qid": "4440911260005876349", "question": "state the position of india in terms of population size", "gold_answers": ["second"], "shots": {"0": {"k": 0, "answer": "2nd", "latency_ms": 102.0517349243164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2nd most populated country", "latency_ms": 292.41275787353516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 610}, "2": {"k": 2, "answer": "second-most populous country", "latency_ms": 205.0948143005371, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1281}, "3": {"k": 3, "answer": "second-most populous country", "latency_ms": 262.1779441833496, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1885}, "4": {"k": 4, "answer": "second most populous country", "latency_ms": 309.68284606933594, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2567}, "5": {"k": 5, "answer": "second-most populous country", "latency_ms": 164.21747207641602, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3228}, "6": {"k": 6, "answer": "2nd most populous country in the world", "latency_ms": 269.1774368286133, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3937}}}
{"qid": "3172949410241605868", "question": "who became a leader of the indian national congress", "gold_answers": ["Mahatma Gandhi"], "shots": {"0": {"k": 0, "answer": "Mahatma Gandhi", "latency_ms": 165.63773155212402, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Jawaharlal Nehru", "latency_ms": 360.80384254455566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 699}, "2": {"k": 2, "answer": "Jawaharlal Nehru", "latency_ms": 327.93164253234863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1390}, "3": {"k": 3, "answer": "Sonia Gandhi", "latency_ms": 320.0337886810303, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2064}, "4": {"k": 4, "answer": "Jawaharlal Nehru", "latency_ms": 532.1624279022217, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2740}, "5": {"k": 5, "answer": "Jawaharlal Nehru", "latency_ms": 293.46299171447754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3415}, "6": {"k": 6, "answer": "Jawaharlal Nehru", "latency_ms": 464.977502822876, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4088}}}
{"qid": "-5136645336730213305", "question": "winner of worst cooks in america season 5", "gold_answers": ["Amber Brauner"], "shots": {"0": {"k": 0, "answer": "Terry Jones", "latency_ms": 116.60265922546387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Christina Machamer", "latency_ms": 161.02933883666992, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "Bobby Flay", "latency_ms": 95.60775756835938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1248}, "3": {"k": 3, "answer": "Bobby Flay", "latency_ms": 222.86701202392578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1890}, "4": {"k": 4, "answer": "Bobby Flay", "latency_ms": 237.24722862243652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2492}, "5": {"k": 5, "answer": "Bobby Flay", "latency_ms": 247.50971794128418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3115}, "6": {"k": 6, "answer": "Bobby Flay", "latency_ms": 178.3146858215332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3743}}}
{"qid": "159123015669900402", "question": "what is the filename extension used for all java source files", "gold_answers": [". java"], "shots": {"0": {"k": 0, "answer": ".java", "latency_ms": 113.48986625671387, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": ".java", "latency_ms": 49.75533485412598, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 301.8503189086914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1306}, "3": {"k": 3, "answer": ".codice_22", "latency_ms": 203.84550094604492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1995}, "4": {"k": 4, "answer": ".codice_22", "latency_ms": 184.12351608276367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2610}, "5": {"k": 5, "answer": "codice_22", "latency_ms": 438.7044906616211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3229}, "6": {"k": 6, "answer": "codice_22", "latency_ms": 317.46888160705566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3927}}}
{"qid": "6860341019198485637", "question": "who cracked the enigma code in world war 2", "gold_answers": ["Turing"], "shots": {"0": {"k": 0, "answer": "Alan Turing", "latency_ms": 69.75841522216797, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Alan Turing", "latency_ms": 164.43729400634766, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 648}, "2": {"k": 2, "answer": "Alan Turing", "latency_ms": 156.61978721618652, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1373}, "3": {"k": 3, "answer": "Alan Turing and Polish cryptologists", "latency_ms": 139.00208473205566, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 2108}, "4": {"k": 4, "answer": "Alan Turing and Polish cryptologists", "latency_ms": 165.37761688232422, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 2817}, "5": {"k": 5, "answer": "Alan Turing and Polish cryptologists", "latency_ms": 376.345157623291, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3508}, "6": {"k": 6, "answer": "Alan Turing and Polish cryptologists", "latency_ms": 563.5230541229248, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 4209}}}
{"qid": "7990281571644070368", "question": "what is the job of the whip in congress", "gold_answers": ["to ensure party discipline"], "shots": {"0": {"k": 0, "answer": "To ensure party members vote according to party leadership.", "latency_ms": 444.6830749511719, "em": 0.0, "f1": 0.4615384615384615, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "keeps track of legislation and ensures party members are present for votes", "latency_ms": 217.59581565856934, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 669}, "2": {"k": 2, "answer": "ensure party members attend and vote as leadership desires", "latency_ms": 593.4739112854004, "em": 0.0, "f1": 0.30769230769230765, "containment": 0.0, "context_chars": 1281}, "3": {"k": 3, "answer": "ensure members attend and vote as party leadership desires", "latency_ms": 165.73476791381836, "em": 0.0, "f1": 0.30769230769230765, "containment": 0.0, "context_chars": 1915}, "4": {"k": 4, "answer": "ensure member attendance, count votes, and communicate the majority position on issues", "latency_ms": 708.9991569519043, "em": 0.0, "f1": 0.13333333333333333, "containment": 0.0, "context_chars": 2634}, "5": {"k": 5, "answer": "ensure members attend and vote as party leadership desires; track legislation; coordinate party strategy; count votes; communicate party positions", "latency_ms": 931.0410022735596, "em": 0.0, "f1": 0.17391304347826086, "containment": 0.0, "context_chars": 3348}, "6": {"k": 6, "answer": "ensure members adhere to party policies and vote as desired", "latency_ms": 478.3642292022705, "em": 0.0, "f1": 0.4285714285714285, "containment": 0.0, "context_chars": 3982}}}
{"qid": "-3089533754017582743", "question": "when was the last time unc did not make the ncaa tournament", "gold_answers": ["2003"], "shots": {"0": {"k": 0, "answer": "2020", "latency_ms": 175.20642280578613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2001", "latency_ms": 120.40138244628906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 679}, "2": {"k": 2, "answer": "2001", "latency_ms": 96.73380851745605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "2001", "latency_ms": 213.3641242980957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1902}, "4": {"k": 4, "answer": "2001", "latency_ms": 336.52520179748535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2484}, "5": {"k": 5, "answer": "2010", "latency_ms": 187.12353706359863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3062}, "6": {"k": 6, "answer": "2010", "latency_ms": 202.79240608215332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3796}}}
{"qid": "-3530165048900528552", "question": "which foreign currency option is the​ right but not the​ obligation to buy foreign​ currency", "gold_answers": ["foreign exchange option"], "shots": {"0": {"k": 0, "answer": "Call option", "latency_ms": 88.85836601257324, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "FX option", "latency_ms": 63.173532485961914, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 676}, "2": {"k": 2, "answer": "FX option", "latency_ms": 88.56892585754395, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1364}, "3": {"k": 3, "answer": "FX option", "latency_ms": 117.86270141601562, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1972}, "4": {"k": 4, "answer": "FX option", "latency_ms": 186.75875663757324, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2662}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 350.0099182128906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3311}, "6": {"k": 6, "answer": "call option", "latency_ms": 135.56599617004395, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3874}}}
{"qid": "2795932191977151266", "question": "how many house of representative do we have in nigeria", "gold_answers": ["360", "360 members"], "shots": {"0": {"k": 0, "answer": "360", "latency_ms": 318.5601234436035, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "360", "latency_ms": 221.42982482910156, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 721}, "2": {"k": 2, "answer": "360", "latency_ms": 284.47771072387695, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1352}, "3": {"k": 3, "answer": "360", "latency_ms": 167.7229404449463, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2011}, "4": {"k": 4, "answer": "360", "latency_ms": 146.23141288757324, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2756}, "5": {"k": 5, "answer": "360", "latency_ms": 239.18986320495605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3450}, "6": {"k": 6, "answer": "360", "latency_ms": 395.8902359008789, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4146}}}
{"qid": "4714895284721516061", "question": "who was the actor who played grizzly adams", "gold_answers": ["`` Dan '' Haggerty"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 200.9415626525879, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dan Haggerty", "latency_ms": 287.97459602355957, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 615}, "2": {"k": 2, "answer": "Dan Haggerty", "latency_ms": 331.5272331237793, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1254}, "3": {"k": 3, "answer": "Dan Haggerty", "latency_ms": 208.79721641540527, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1922}, "4": {"k": 4, "answer": "Dan Haggerty", "latency_ms": 147.2311019897461, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2573}, "5": {"k": 5, "answer": "Dan Haggerty", "latency_ms": 170.45331001281738, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3209}, "6": {"k": 6, "answer": "Dan Haggerty", "latency_ms": 143.35012435913086, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3850}}}
{"qid": "-6101054679785992133", "question": "who sings blame it on the bossa nova", "gold_answers": ["Eydie Gormé"], "shots": {"0": {"k": 0, "answer": "Bobby \"Blue\" Bland", "latency_ms": 314.4261837005615, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Eydie Gormé", "latency_ms": 193.2694911956787, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 580}, "2": {"k": 2, "answer": "Eydie Gormé", "latency_ms": 265.000581741333, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1157}, "3": {"k": 3, "answer": "Eydie Gormé", "latency_ms": 304.5992851257324, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1758}, "4": {"k": 4, "answer": "Eydie Gormé", "latency_ms": 377.40373611450195, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2359}, "5": {"k": 5, "answer": "Eydie Gormé", "latency_ms": 291.90850257873535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2935}, "6": {"k": 6, "answer": "Eydie Gormé", "latency_ms": 262.7980709075928, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3552}}}
{"qid": "1882117704293632224", "question": "who is the supreme court judge was a former chief justice of kerala high court", "gold_answers": ["Konakuppakatil Gopinathan Balakrishnan"], "shots": {"0": {"k": 0, "answer": "Justice D. K. Jain", "latency_ms": 357.8922748565674, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 218.8396453857422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 186.12289428710938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1277}, "3": {"k": 3, "answer": "Mohan Shantanagoudar", "latency_ms": 176.10883712768555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1889}, "4": {"k": 4, "answer": "Mohan Shantanagoudar", "latency_ms": 343.8122272491455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2518}, "5": {"k": 5, "answer": "Mohan Shantanagoudar", "latency_ms": 387.9120349884033, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3139}, "6": {"k": 6, "answer": "Mohan Shantanagoudar", "latency_ms": 293.09558868408203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3742}}}
{"qid": "2524852627963543698", "question": "who has won more grand slam titles in tennis", "gold_answers": ["Roger Federer"], "shots": {"0": {"k": 0, "answer": "Novak Djokovic", "latency_ms": 313.0309581756592, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Serena Williams", "latency_ms": 214.2164707183838, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 669}, "2": {"k": 2, "answer": "Serena Williams", "latency_ms": 184.09466743469238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1245}, "3": {"k": 3, "answer": "Roger Federer", "latency_ms": 122.05648422241211, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1846}, "4": {"k": 4, "answer": "Roger Federer", "latency_ms": 136.81650161743164, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2483}, "5": {"k": 5, "answer": "Roger Federer", "latency_ms": 412.4176502227783, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3057}, "6": {"k": 6, "answer": "Roger Federer", "latency_ms": 349.58696365356445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3631}}}
{"qid": "-6230694167244306035", "question": "how many times has the saints won the super bowl", "gold_answers": ["1"], "shots": {"0": {"k": 0, "answer": "5", "latency_ms": 40.95745086669922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1", "latency_ms": 45.40109634399414, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 608}, "2": {"k": 2, "answer": "1", "latency_ms": 101.96566581726074, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1228}, "3": {"k": 3, "answer": "1", "latency_ms": 141.85547828674316, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1864}, "4": {"k": 4, "answer": "1", "latency_ms": 110.60786247253418, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2479}, "5": {"k": 5, "answer": "1", "latency_ms": 179.41856384277344, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3092}, "6": {"k": 6, "answer": "1", "latency_ms": 142.2429084777832, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3691}}}
{"qid": "4186485631454362041", "question": "when was the last time the los angeles lakers won a championship", "gold_answers": ["2010", "in 2010"], "shots": {"0": {"k": 0, "answer": "2020", "latency_ms": 120.2247142791748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2009", "latency_ms": 164.8404598236084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "2009", "latency_ms": 147.07159996032715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1235}, "3": {"k": 3, "answer": "2009", "latency_ms": 451.86734199523926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1890}, "4": {"k": 4, "answer": "2009", "latency_ms": 323.8997459411621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2589}, "5": {"k": 5, "answer": "2009", "latency_ms": 228.04903984069824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3187}, "6": {"k": 6, "answer": "2009", "latency_ms": 267.8682804107666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3839}}}
{"qid": "6332740420650663139", "question": "how many gold medals did australia win in the 2000 olympics", "gold_answers": ["16"], "shots": {"0": {"k": 0, "answer": "32", "latency_ms": 150.03180503845215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1", "latency_ms": 94.2220687866211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "1", "latency_ms": 212.937593460083, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": "1", "latency_ms": 115.62705039978027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1988}, "4": {"k": 4, "answer": "11", "latency_ms": 141.49975776672363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2594}, "5": {"k": 5, "answer": "1 gold", "latency_ms": 257.13348388671875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3265}, "6": {"k": 6, "answer": "1 gold", "latency_ms": 259.2167854309082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3916}}}
{"qid": "-2418032234376469176", "question": "where is the eagle creek fire located in oregon", "gold_answers": ["Cascade Locks", "Columbia River Gorge"], "shots": {"0": {"k": 0, "answer": "Wallowa County, Oregon", "latency_ms": 136.10506057739258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Columbia River Gorge", "latency_ms": 234.05814170837402, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 624}, "2": {"k": 2, "answer": "Columbia River Gorge", "latency_ms": 189.04709815979004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1241}, "3": {"k": 3, "answer": "Columbia River Gorge", "latency_ms": 123.9616870880127, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1914}, "4": {"k": 4, "answer": "Columbia River Gorge", "latency_ms": 177.5374412536621, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2556}, "5": {"k": 5, "answer": "Columbia River Gorge", "latency_ms": 156.95691108703613, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3207}, "6": {"k": 6, "answer": "Columbia River Gorge", "latency_ms": 156.80646896362305, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3838}}}
{"qid": "-2622854017829301835", "question": "different ways to spell corey for a boy", "gold_answers": ["Kori", "Cory", "Kory", "Coire", "Corrie", "Correy", "Corie", "Corey", "Khouri"], "shots": {"0": {"k": 0, "answer": "Corey, Corry, Kory, Cory, Koorie", "latency_ms": 416.6371822357178, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 144.17552947998047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 676}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 249.44233894348145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1274}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 227.00786590576172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1991}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 240.19598960876465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2659}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 248.7936019897461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3309}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 271.984338760376, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3926}}}
{"qid": "3480908309420822259", "question": "who sang the song should have been a cowboy", "gold_answers": ["Toby Keith"], "shots": {"0": {"k": 0, "answer": "Linda Ronstadt", "latency_ms": 140.6550407409668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Toby Keith", "latency_ms": 280.9760570526123, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 608}, "2": {"k": 2, "answer": "Toby Keith", "latency_ms": 197.12543487548828, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1229}, "3": {"k": 3, "answer": "Toby Keith", "latency_ms": 329.7264575958252, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1829}, "4": {"k": 4, "answer": "Toby Keith", "latency_ms": 155.7910442352295, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2402}, "5": {"k": 5, "answer": "Toby Keith", "latency_ms": 322.46947288513184, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3013}, "6": {"k": 6, "answer": "Toby Keith", "latency_ms": 131.67214393615723, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3633}}}
{"qid": "-4198273171611748168", "question": "how many counties does the state of georgia have", "gold_answers": ["159", "159 counties"], "shots": {"0": {"k": 0, "answer": "159", "latency_ms": 200.00267028808594, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "159", "latency_ms": 77.21066474914551, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "159", "latency_ms": 286.23461723327637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1254}, "3": {"k": 3, "answer": "159", "latency_ms": 256.10804557800293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1896}, "4": {"k": 4, "answer": "159", "latency_ms": 175.4453182220459, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2626}, "5": {"k": 5, "answer": "159", "latency_ms": 399.7187614440918, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3316}, "6": {"k": 6, "answer": "159", "latency_ms": 295.304536819458, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3989}}}
{"qid": "2077711785168359308", "question": "who sang picking up pebbles and throwing them into the sea", "gold_answers": ["Matt Flinders"], "shots": {"0": {"k": 0, "answer": "Tom Jones", "latency_ms": 131.88457489013672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Matt Flinders", "latency_ms": 269.93250846862793, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 604}, "2": {"k": 2, "answer": "Matt Flinders", "latency_ms": 193.84002685546875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1298}, "3": {"k": 3, "answer": "Matt Flinders", "latency_ms": 121.85168266296387, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1963}, "4": {"k": 4, "answer": "Matt Flinders", "latency_ms": 165.1782989501953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2562}, "5": {"k": 5, "answer": "Matt Flinders", "latency_ms": 342.5028324127197, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3209}, "6": {"k": 6, "answer": "Matt Flinders", "latency_ms": 358.40439796447754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3869}}}
{"qid": "9062886943819260445", "question": "who made the most free throws in nba history", "gold_answers": ["Karl Malone"], "shots": {"0": {"k": 0, "answer": "Stephen Curry", "latency_ms": 177.10256576538086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 129.79841232299805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 602}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 332.3843479156494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1241}, "3": {"k": 3, "answer": "Wilt Chamberlain", "latency_ms": 137.47715950012207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1836}, "4": {"k": 4, "answer": "Dwight Howard", "latency_ms": 237.60652542114258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2437}, "5": {"k": 5, "answer": "Dwight Howard", "latency_ms": 173.7833023071289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3035}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 265.4390335083008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3687}}}
{"qid": "1278533364941960992", "question": "which is the ring finger for male in india", "gold_answers": ["the left ring finger", "left ring finger"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 119.45295333862305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "right hand", "latency_ms": 144.10114288330078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 583}, "2": {"k": 2, "answer": "right hand", "latency_ms": 298.92468452453613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1137}, "3": {"k": 3, "answer": "right hand", "latency_ms": 90.81649780273438, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1739}, "4": {"k": 4, "answer": "right hand", "latency_ms": 285.0842475891113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2303}, "5": {"k": 5, "answer": "right hand ring finger", "latency_ms": 385.75100898742676, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 2897}, "6": {"k": 6, "answer": "right hand ring finger", "latency_ms": 131.84142112731934, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3533}}}
{"qid": "-8788207958970648395", "question": "what disney cartoon character's middle name is fauntleroy", "gold_answers": ["Donald Fauntleroy Duck", "Donald Duck"], "shots": {"0": {"k": 0, "answer": "Pinocchio", "latency_ms": 235.0139617919922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 269.4563865661621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 627}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 324.171781539917, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1266}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 467.693567276001, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1914}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 169.12436485290527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2551}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 347.6574420928955, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3185}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 293.3974266052246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3821}}}
{"qid": "-4323000711881657078", "question": "who played king theoden in lord of the rings", "gold_answers": ["Bernard Hill"], "shots": {"0": {"k": 0, "answer": "Waited (no, that's incorrect)  \nCorrect answer: Sean Astin  \nNo, correction: Christopher Lee?  \nFinal correct answer: Sean Astin is not correct.  \nCorrect answer: John Rhys-Davies? No.  \nCorrect answer: Sir Ian McKellen? No.  \nCorrect answer: Sir Peter O'Toole? No.  \nCorrect answer: Sir Anthony Hopkins? No.  \nCorrect answer: Sir Robert De Niro? No.  \nCorrect answer: Sir Tom Hardy? No.  \nCorrect answer: Sir Ben Kingsley? No.  \nCorrect answer: Sir Michael Fassbender? No.  \nCorrect answer: Sir Daniel Craig? No.  \nCorrect answer: Sir Eddie Redmayne? No.  \nCorrect answer: Sir Tom Hanks? No.  \nCorrect answer: Sir Hugh Jackman? No.  \nCorrect answer: Sir Chris Hemsworth? No.  \nCorrect answer: Sir Daniel Craig? No.  \nCorrect answer: Sir John Hurt? No.  \nCorrect answer: Sir Ian McKellen? No.  \nCorrect answer: Sir Robert Downey Jr.? No.  \nCorrect answer: Sir Gary Oldman? No.  \nCorrect answer: Sir Tom Hardy? No.  \nCorrect answer: Sir Benedict Cumber", "latency_ms": 7258.721828460693, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Bernard Hill", "latency_ms": 145.14946937561035, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 644}, "2": {"k": 2, "answer": "Bernard Hill", "latency_ms": 215.87777137756348, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1251}, "3": {"k": 3, "answer": "Bernard Hill", "latency_ms": 151.67784690856934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1882}, "4": {"k": 4, "answer": "Bernard Hill", "latency_ms": 118.83187294006348, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2579}, "5": {"k": 5, "answer": "Bernard Hill", "latency_ms": 239.17484283447266, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3233}, "6": {"k": 6, "answer": "Bernard Hill", "latency_ms": 184.43560600280762, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3821}}}
{"qid": "-2664158830468622213", "question": "dark parables return of the salt princess release date", "gold_answers": ["September 2017"], "shots": {"0": {"k": 0, "answer": "2023", "latency_ms": 226.38392448425293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "March 15, 2018", "latency_ms": 394.1307067871094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 573}, "2": {"k": 2, "answer": "March 15, 2018", "latency_ms": 207.77583122253418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1219}, "3": {"k": 3, "answer": "March 15, 2018", "latency_ms": 713.8676643371582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1865}, "4": {"k": 4, "answer": "March 15, 2018", "latency_ms": 562.6273155212402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2483}, "5": {"k": 5, "answer": "March 15, 2018", "latency_ms": 409.3432426452637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3113}, "6": {"k": 6, "answer": "March 15, 2018", "latency_ms": 248.70562553405762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3704}}}
{"qid": "9078092620239351027", "question": "where was the movie 500 days of summer filmed", "gold_answers": ["Los Angeles"], "shots": {"0": {"k": 0, "answer": "Los Angeles, California", "latency_ms": 173.8729476928711, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Los Angeles", "latency_ms": 89.26081657409668, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 641}, "2": {"k": 2, "answer": "Los Angeles", "latency_ms": 269.8664665222168, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1261}, "3": {"k": 3, "answer": "Los Angeles", "latency_ms": 84.7015380859375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1855}, "4": {"k": 4, "answer": "Los Angeles", "latency_ms": 189.00609016418457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2487}, "5": {"k": 5, "answer": "Los Angeles", "latency_ms": 103.23619842529297, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3086}, "6": {"k": 6, "answer": "Los Angeles", "latency_ms": 221.10247611999512, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3708}}}
{"qid": "4747795168824294053", "question": "who played emma in h2o just add water", "gold_answers": ["Claire Rhiannon Holt"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 84.67936515808105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 84.1069221496582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 615}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 94.63286399841309, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1251}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 178.63965034484863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1891}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 112.75982856750488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2521}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 228.1782627105713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3147}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 418.1997776031494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3844}}}
{"qid": "-1368633715963532113", "question": "where can carbon be found in the biosphere", "gold_answers": ["other living organisms", "plants", "soil", "The terrestrial biosphere"], "shots": {"0": {"k": 0, "answer": "atmosphere, oceans, soil, living organisms", "latency_ms": 412.77050971984863, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "in plants and other living organisms, soils, and inorganic forms like calcium carbonate", "latency_ms": 302.4942874908447, "em": 0.0, "f1": 0.375, "containment": 1.0, "context_chars": 648}, "2": {"k": 2, "answer": "atmosphere, soils, plants, animals, oceans (as dissolved organic carbon)", "latency_ms": 782.6931476593018, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 1333}, "3": {"k": 3, "answer": "in plants, soils, and organic matter of land-living organisms", "latency_ms": 325.8543014526367, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 2040}, "4": {"k": 4, "answer": "in plants and other living organisms, soils, and forests", "latency_ms": 374.13954734802246, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2652}, "5": {"k": 5, "answer": "in plants and other living organisms, soils, and in organic forms such as dead plant material and animal consumption products", "latency_ms": 960.6211185455322, "em": 0.0, "f1": 0.2608695652173913, "containment": 1.0, "context_chars": 3295}, "6": {"k": 6, "answer": "in plants and other living organisms, soils, and forests", "latency_ms": 442.6686763763428, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3862}}}
{"qid": "-1173252802989599019", "question": "who is the winner of bigg boss kannada season", "gold_answers": ["rapper Chandan Shetty", "Chandan Shetty"], "shots": {"0": {"k": 0, "answer": "2023: Naveen Kumar", "latency_ms": 675.5809783935547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Pratham", "latency_ms": 58.00890922546387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 642}, "2": {"k": 2, "answer": "Pratham", "latency_ms": 69.75913047790527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1271}, "3": {"k": 3, "answer": "Pratham", "latency_ms": 93.6882495880127, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1883}, "4": {"k": 4, "answer": "Pratham", "latency_ms": 116.57476425170898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2496}, "5": {"k": 5, "answer": "Pratham", "latency_ms": 105.67140579223633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3163}, "6": {"k": 6, "answer": "Pratham", "latency_ms": 115.73290824890137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3776}}}
{"qid": "-8950878510839541193", "question": "where was the outlaw of josey wales filmed", "gold_answers": ["Utah", "Arizona", "Kanab , Utah", "Lake Powell", "Wyoming", "Oroville , California"], "shots": {"0": {"k": 0, "answer": "Tennessee", "latency_ms": 106.40215873718262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lake Powell", "latency_ms": 71.43211364746094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 651}, "2": {"k": 2, "answer": "Lake Powell", "latency_ms": 117.96140670776367, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1274}, "3": {"k": 3, "answer": "Lake Powell, Utah", "latency_ms": 334.22231674194336, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1892}, "4": {"k": 4, "answer": "Lake Powell, Utah", "latency_ms": 217.30470657348633, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2516}, "5": {"k": 5, "answer": "Lake Powell and Kanab, Utah", "latency_ms": 162.66727447509766, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3110}, "6": {"k": 6, "answer": "Lake Powell, Utah", "latency_ms": 273.71692657470703, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3828}}}
{"qid": "445108246725869963", "question": "who plays the dad in the new beauty and the beast", "gold_answers": ["Kevin Kline"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 82.62085914611816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dan Stevens", "latency_ms": 129.81939315795898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "Tom Turner", "latency_ms": 117.22803115844727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1288}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 164.85834121704102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1983}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 161.76605224609375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2621}, "5": {"k": 5, "answer": "Dan Stevens", "latency_ms": 104.77495193481445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3282}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 194.68283653259277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3897}}}
{"qid": "-5745040113272333526", "question": "where does the saskatchewan river start and end", "gold_answers": ["the Hudson Bay", "central Saskatchewan", "Saskatchewan River Forks , Saskatchewan", "Saskatchewan Glacier , Alberta"], "shots": {"0": {"k": 0, "answer": "Starts in the Rocky Mountains, ends in the Hudson Bay", "latency_ms": 254.1790008544922, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Starts in central Saskatchewan, ends in Lake Winnipeg", "latency_ms": 316.8625831604004, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 728}, "2": {"k": 2, "answer": "Starts in central Saskatchewan, east of Prince Albert; ends in Lake Winnipeg", "latency_ms": 591.1715030670166, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 1438}, "3": {"k": 3, "answer": "Starts in central Saskatchewan, east of Prince Albert; ends in Lake Winnipeg.", "latency_ms": 868.3874607086182, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 2093}, "4": {"k": 4, "answer": "Starts in central Saskatchewan, east of Prince Albert; ends in Lake Winnipeg.", "latency_ms": 592.2567844390869, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 2761}, "5": {"k": 5, "answer": "Starts in central Saskatchewan, east of Prince Albert; ends in Lake Winnipeg.", "latency_ms": 1032.4082374572754, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 3459}, "6": {"k": 6, "answer": "Starts at confluence of Bow and Oldman Rivers near Grassy Lake, Alberta; ends in Lake Winnipeg.", "latency_ms": 610.4526519775391, "em": 0.0, "f1": 0.10526315789473684, "containment": 0.0, "context_chars": 4060}}}
{"qid": "2370076891269438265", "question": "where are the spanish steps located in italy", "gold_answers": ["Rome"], "shots": {"0": {"k": 0, "answer": "Rome", "latency_ms": 76.65753364562988, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Rome, Italy", "latency_ms": 94.67554092407227, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 611}, "2": {"k": 2, "answer": "Rome, Italy", "latency_ms": 236.19556427001953, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1242}, "3": {"k": 3, "answer": "Rome, Italy", "latency_ms": 295.1855659484863, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1843}, "4": {"k": 4, "answer": "Rome, Italy", "latency_ms": 311.0671043395996, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2487}, "5": {"k": 5, "answer": "Rome, Italy", "latency_ms": 252.61807441711426, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3102}, "6": {"k": 6, "answer": "Rome, Italy", "latency_ms": 273.55408668518066, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3764}}}
{"qid": "-1701212376306091221", "question": "what does the m number mean on a pint glass", "gold_answers": ["imperial fluid ounces", "the manufacturing company or site"], "shots": {"0": {"k": 0, "answer": "M number refers to the volume of liquid in milliliters.", "latency_ms": 242.9962158203125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 204.17451858520508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 221.04811668395996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1197}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 142.9758071899414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1801}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 238.95549774169922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2364}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 235.85247993469238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2987}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 149.50990676879883, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3608}}}
{"qid": "3631867991857999976", "question": "other than water what else has hydrogen bonds", "gold_answers": ["inorganic molecules such as water"], "shots": {"0": {"k": 0, "answer": "ammonia, alcohols, carboxylic acids", "latency_ms": 605.5312156677246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "alcohols, ammonia, acids", "latency_ms": 377.39062309265137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 671}, "2": {"k": 2, "answer": "ammonia, alcohols, acids", "latency_ms": 464.85042572021484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1271}, "3": {"k": 3, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 551.6815185546875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1940}, "4": {"k": 4, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 468.33133697509766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2531}, "5": {"k": 5, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 409.38377380371094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3216}, "6": {"k": 6, "answer": "hydrogen fluoride, ammonia, methanol", "latency_ms": 549.2599010467529, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3820}}}
{"qid": "-1665528830781573810", "question": "when boy meets girl waiting for a star to fall", "gold_answers": ["1988"], "shots": {"0": {"k": 0, "answer": "1988", "latency_ms": 126.88183784484863, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 107.71656036376953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 566}, "2": {"k": 2, "answer": "The Little Match Girl", "latency_ms": 140.60306549072266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1142}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 196.61855697631836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1702}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 133.77094268798828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2287}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 235.48650741577148, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2933}, "6": {"k": 6, "answer": "The Little Match Girl", "latency_ms": 252.64334678649902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3494}}}
{"qid": "4289442278067074619", "question": "trick taking card game name derived from spanish for man", "gold_answers": ["Skat"], "shots": {"0": {"k": 0, "answer": "Trick-taking card game name derived from Spanish for man: \"Ombre\" or \"Talon\" — but the most accurate and directly derived from Spanish for \"man\" is: **\"Talon\"** — though not directly from \"man\", the correct answer based on derivation from Spanish for \"man\" is: **\"Mano\"** — which means \"hand\" in Spanish, and is used in many card games.\n\nBut the game directly named from Spanish for \"man\" is: **\"El Mano\"** — not a known game.\n\nCorrect answer: **\"Mano\"** — as in \"mano\" meaning hand, and the game \"Mano\" is not standard.\n\nActually, the game derived from Spanish for \"man\" is: **\"Truco\"** — from Spanish \"truco\", meaning \"trick\" or \"deception\", not \"man\".\n\nWait — the Spanish word for \"man\" is \"hombre\".\n\nNo known trick-taking game is named directly from \"hombre\".\n\nBut the game \"Truco\" is a trick-taking game from Spanish-speaking countries, and \"truco\" means \"trick\" — not \"man\".\n\nSo, is there a game derived from Spanish for \"man", "latency_ms": 6058.651685714722, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Napoleon", "latency_ms": 125.92101097106934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 599}, "2": {"k": 2, "answer": "Triomphe", "latency_ms": 118.19219589233398, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1234}, "3": {"k": 3, "answer": "Tarot", "latency_ms": 231.248140335083, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1840}, "4": {"k": 4, "answer": "Tarot", "latency_ms": 323.697566986084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2413}, "5": {"k": 5, "answer": "Tarot", "latency_ms": 230.61180114746094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3036}, "6": {"k": 6, "answer": "Tarot", "latency_ms": 187.12544441223145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3612}}}
{"qid": "-7311149600272472027", "question": "what percentage of the population is naturally blonde", "gold_answers": ["2 %"], "shots": {"0": {"k": 0, "answer": "2%", "latency_ms": 169.6174144744873, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 144.1798210144043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 610}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 165.83752632141113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1183}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 111.8764877319336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1769}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 150.15268325805664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2354}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 364.85934257507324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3009}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 366.78600311279297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3543}}}
{"qid": "7110705857253017304", "question": "when was the last time miss texas won miss america", "gold_answers": ["1975"], "shots": {"0": {"k": 0, "answer": "2018", "latency_ms": 204.542875289917, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1975", "latency_ms": 85.36362648010254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 610}, "2": {"k": 2, "answer": "1975", "latency_ms": 169.78144645690918, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1241}, "3": {"k": 3, "answer": "1975", "latency_ms": 220.65448760986328, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1940}, "4": {"k": 4, "answer": "1975", "latency_ms": 342.510461807251, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2515}, "5": {"k": 5, "answer": "1975", "latency_ms": 193.01700592041016, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3108}, "6": {"k": 6, "answer": "1975", "latency_ms": 184.6299171447754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3852}}}
{"qid": "-3653614166480549899", "question": "who sang first line of we are the world", "gold_answers": ["Billy Joel", "Stevie Wonder", "Lionel Richie", "James Ingram", "Tina Turner", "Paul Simon", "Kenny Rogers"], "shots": {"0": {"k": 0, "answer": "Michael Jackson", "latency_ms": 232.15627670288086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lionel Richie", "latency_ms": 196.59924507141113, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 647}, "2": {"k": 2, "answer": "Lionel Richie", "latency_ms": 175.52685737609863, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1273}, "3": {"k": 3, "answer": "Lionel Richie", "latency_ms": 363.2810115814209, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1840}, "4": {"k": 4, "answer": "Lionel Richie, Stevie Wonder, Paul Simon, Kenny Rogers, James Ingram, Tina Turner, and Billy Joel", "latency_ms": 1081.4135074615479, "em": 0.0, "f1": 0.2352941176470588, "containment": 1.0, "context_chars": 2426}, "5": {"k": 5, "answer": "Lionel Richie", "latency_ms": 132.82227516174316, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3005}, "6": {"k": 6, "answer": "Lionel Richie, Stevie Wonder, Paul Simon, Kenny Rogers, James Ingram, Tina Turner, and Billy Joel", "latency_ms": 1387.2296810150146, "em": 0.0, "f1": 0.2352941176470588, "containment": 1.0, "context_chars": 3636}}}
{"qid": "-6284503594240226071", "question": "who formed and first came to the colony of maryland", "gold_answers": ["the English"], "shots": {"0": {"k": 0, "answer": "Lord Baltimore", "latency_ms": 150.87246894836426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cecil Calvert", "latency_ms": 134.56225395202637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "Cecil Calvert", "latency_ms": 322.7415084838867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1297}, "3": {"k": 3, "answer": "Cecil Calvert", "latency_ms": 236.25707626342773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1927}, "4": {"k": 4, "answer": "George Calvert", "latency_ms": 246.2599277496338, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2584}, "5": {"k": 5, "answer": "George Calvert", "latency_ms": 355.86023330688477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3218}, "6": {"k": 6, "answer": "George Calvert", "latency_ms": 181.3957691192627, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3871}}}
{"qid": "6948592513354589991", "question": "who has played in the most masters tournaments", "gold_answers": ["Gary Player"], "shots": {"0": {"k": 0, "answer": "Tiger Woods", "latency_ms": 117.78402328491211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Jack Nicklaus", "latency_ms": 179.2612075805664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 264.48726654052734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1268}, "3": {"k": 3, "answer": "Gary Player", "latency_ms": 109.43913459777832, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1875}, "4": {"k": 4, "answer": "Gary Player", "latency_ms": 196.05684280395508, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2480}, "5": {"k": 5, "answer": "Gary Player", "latency_ms": 143.33796501159668, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3077}, "6": {"k": 6, "answer": "Gary Player", "latency_ms": 193.2971477508545, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3827}}}
{"qid": "3311962143974666464", "question": "how much venom can a king cobra produce", "gold_answers": ["200 to 500 mg", "420 mg", "up to 7 ml"], "shots": {"0": {"k": 0, "answer": "100 milliliters", "latency_ms": 259.7339153289795, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "200 to 500 mg (dry weight)", "latency_ms": 713.9437198638916, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 620}, "2": {"k": 2, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 221.03095054626465, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1269}, "3": {"k": 3, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 556.1733245849609, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1889}, "4": {"k": 4, "answer": "420 mg (dry weight)", "latency_ms": 520.3080177307129, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2607}, "5": {"k": 5, "answer": "200 to 500 mg up to 7 ml", "latency_ms": 255.05399703979492, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3226}, "6": {"k": 6, "answer": "420 mg (dry weight)", "latency_ms": 406.6507816314697, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3886}}}
{"qid": "4829683075369336935", "question": "last team to win fa cup outside top flight", "gold_answers": ["Arsenal", "West Ham United"], "shots": {"0": {"k": 0, "answer": "Barnsley 2002", "latency_ms": 363.66772651672363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "West Bromwich (1931)", "latency_ms": 469.8328971862793, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 578}, "2": {"k": 2, "answer": "West Ham United", "latency_ms": 94.26379203796387, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1166}, "3": {"k": 3, "answer": "West Ham United", "latency_ms": 123.61812591552734, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1758}, "4": {"k": 4, "answer": "West Ham United (1980)", "latency_ms": 539.5829677581787, "em": 0.0, "f1": 0.8571428571428571, "containment": 1.0, "context_chars": 2394}, "5": {"k": 5, "answer": "West Ham United", "latency_ms": 118.74055862426758, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2966}, "6": {"k": 6, "answer": "West Ham United", "latency_ms": 373.52967262268066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3562}}}
{"qid": "947113025953164512", "question": "where did you go to drink during prohibition", "gold_answers": ["Speakeasies"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 135.5133056640625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Beer parlours", "latency_ms": 250.49614906311035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 619}, "2": {"k": 2, "answer": "Beer parlours", "latency_ms": 233.69646072387695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1367}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 269.79756355285645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2067}, "4": {"k": 4, "answer": "Lorneville or Winton", "latency_ms": 344.4855213165283, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2683}, "5": {"k": 5, "answer": "outside the city, such as Lorneville or Winton", "latency_ms": 468.6100482940674, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3335}, "6": {"k": 6, "answer": "outside the city (such as the nearby township of Lorneville or the town of Winton)", "latency_ms": 922.7814674377441, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3983}}}
{"qid": "2162587275527723755", "question": "where does the edinburgh fringe festival take place", "gold_answers": ["in Edinburgh , Scotland"], "shots": {"0": {"k": 0, "answer": "Edinburgh, Scotland", "latency_ms": 251.1897087097168, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Edinburgh, Scotland", "latency_ms": 203.15194129943848, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 694}, "2": {"k": 2, "answer": "Edinburgh, Scotland", "latency_ms": 117.68341064453125, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1372}, "3": {"k": 3, "answer": "Edinburgh, Scotland", "latency_ms": 114.86124992370605, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2057}, "4": {"k": 4, "answer": "Edinburgh, Scotland", "latency_ms": 267.458438873291, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2733}, "5": {"k": 5, "answer": "Edinburgh, Scotland", "latency_ms": 286.5438461303711, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3406}, "6": {"k": 6, "answer": "Edinburgh, Scotland", "latency_ms": 372.1957206726074, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 4109}}}
{"qid": "1875157261182375834", "question": "who won the wwe championship elimination chamber 2018", "gold_answers": ["Roman Reigns"], "shots": {"0": {"k": 0, "answer": "Braun Strowman", "latency_ms": 426.8033504486084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 244.97342109680176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 706}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 204.6372890472412, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1413}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 345.80230712890625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2090}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 176.57709121704102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2692}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 182.48820304870605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3360}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 302.0026683807373, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4007}}}
{"qid": "8926902974073457645", "question": "where does safe haven take place in the book", "gold_answers": ["Southport , North Carolina"], "shots": {"0": {"k": 0, "answer": "Florida", "latency_ms": 187.61110305786133, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Southport", "latency_ms": 219.41447257995605, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 560}, "2": {"k": 2, "answer": "Southport", "latency_ms": 131.37364387512207, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1144}, "3": {"k": 3, "answer": "Southport", "latency_ms": 251.0085105895996, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1780}, "4": {"k": 4, "answer": "Southport", "latency_ms": 138.58342170715332, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2343}, "5": {"k": 5, "answer": "Southport", "latency_ms": 148.65541458129883, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2939}, "6": {"k": 6, "answer": "Southport", "latency_ms": 140.15650749206543, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3573}}}
{"qid": "8445703091969850957", "question": "who came up with the term baby boomer", "gold_answers": ["The Washington Post"], "shots": {"0": {"k": 0, "answer": "David Owen", "latency_ms": 178.70712280273438, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 262.5892162322998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 705}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 105.06534576416016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1366}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 160.26067733764648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1971}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 117.74754524230957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2609}, "5": {"k": 5, "answer": "Sylvia F. Porter", "latency_ms": 485.4443073272705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3222}, "6": {"k": 6, "answer": "Sylvia F. Porter", "latency_ms": 389.18256759643555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3831}}}
{"qid": "-2961860751699827965", "question": "who was elected president of the united states in 1928", "gold_answers": ["Herbert Hoover"], "shots": {"0": {"k": 0, "answer": "Herbert Hoover", "latency_ms": 127.36058235168457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Herbert Hoover", "latency_ms": 106.90665245056152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 714}, "2": {"k": 2, "answer": "Herbert Hoover", "latency_ms": 164.39199447631836, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1460}, "3": {"k": 3, "answer": "Herbert Hoover", "latency_ms": 152.73237228393555, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2166}, "4": {"k": 4, "answer": "Herbert Hoover", "latency_ms": 241.84060096740723, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2903}, "5": {"k": 5, "answer": "Herbert Hoover", "latency_ms": 244.9350357055664, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3650}, "6": {"k": 6, "answer": "Herbert Hoover", "latency_ms": 191.0531520843506, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4386}}}
{"qid": "-3997272399784107764", "question": "dogs name in the grinch who stole christmas", "gold_answers": ["Max"], "shots": {"0": {"k": 0, "answer": "Max", "latency_ms": 40.51470756530762, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Max", "latency_ms": 45.60708999633789, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 600}, "2": {"k": 2, "answer": "Max", "latency_ms": 70.23239135742188, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1163}, "3": {"k": 3, "answer": "Max", "latency_ms": 152.0681381225586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1791}, "4": {"k": 4, "answer": "Max", "latency_ms": 161.912202835083, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2426}, "5": {"k": 5, "answer": "Max", "latency_ms": 95.09825706481934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3027}, "6": {"k": 6, "answer": "Max", "latency_ms": 174.04532432556152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3671}}}
{"qid": "119543419987074002", "question": "who played anne in anne with an e", "gold_answers": ["Amybeth McNulty"], "shots": {"0": {"k": 0, "answer": "Sasha Lane", "latency_ms": 149.9040126800537, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Amybeth McNulty", "latency_ms": 271.6093063354492, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 649}, "2": {"k": 2, "answer": "Amybeth McNulty", "latency_ms": 271.7270851135254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1271}, "3": {"k": 3, "answer": "Amybeth McNulty", "latency_ms": 258.2418918609619, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1845}, "4": {"k": 4, "answer": "Amybeth McNulty", "latency_ms": 124.42946434020996, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2464}, "5": {"k": 5, "answer": "Amybeth McNulty", "latency_ms": 220.93749046325684, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3119}, "6": {"k": 6, "answer": "Amybeth McNulty", "latency_ms": 281.03089332580566, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3786}}}
{"qid": "4362450141225960365", "question": "what was going on in the world in 1900", "gold_answers": ["Boxer Rebellion", "U.S. population exceeds 75 million", "Galveston hurricane"], "shots": {"0": {"k": 0, "answer": "Industrialization, imperialism, and the rise of global powers", "latency_ms": 270.2362537384033, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "British Empire at height of power", "latency_ms": 311.69891357421875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 651}, "2": {"k": 2, "answer": "Flu pandemic, World War I, nationalism, decolonization, Cold War", "latency_ms": 405.886173248291, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1246}, "3": {"k": 3, "answer": "Edwardian era, nationalism, British Empire at height, early rise of Germany and Italy, global competition for power, beginnings of flu pandemic, pre-World War I tensions", "latency_ms": 865.0147914886475, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1959}, "4": {"k": 4, "answer": "Edwardian era began", "latency_ms": 284.8191261291504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2565}, "5": {"k": 5, "answer": "Flu pandemic, World War I and World War II, nuclear power and space exploration, nationalism and decolonization, the Cold War and post-Cold War conflicts", "latency_ms": 1360.9507083892822, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3252}, "6": {"k": 6, "answer": "British Empire at height of power, Australia federated, Russo-Japanese War (1904-1905) began, Japanese militarism strengthened, Russian Revolution of 1905 anticipated", "latency_ms": 1514.8730278015137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3846}}}
{"qid": "-599106694350296477", "question": "theme of the song roar by katy perry", "gold_answers": ["standing up for oneself", "self - empowerment"], "shots": {"0": {"k": 0, "answer": "Self-empowerment and confidence", "latency_ms": 393.2955265045166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "self-empowerment", "latency_ms": 289.11638259887695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "self-empowerment", "latency_ms": 104.44283485412598, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "self-empowerment", "latency_ms": 339.62512016296387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1987}, "4": {"k": 4, "answer": "self-empowerment", "latency_ms": 354.32982444763184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2629}, "5": {"k": 5, "answer": "self-empowerment", "latency_ms": 173.44331741333008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3212}, "6": {"k": 6, "answer": "self-empowerment", "latency_ms": 302.95515060424805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3834}}}
{"qid": "6341281025332348318", "question": "who picks the players in the nfl draft", "gold_answers": ["each team"], "shots": {"0": {"k": 0, "answer": "NFL teams", "latency_ms": 103.5771369934082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "teams", "latency_ms": 44.97265815734863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 616}, "2": {"k": 2, "answer": "teams", "latency_ms": 51.56278610229492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1247}, "3": {"k": 3, "answer": "teams", "latency_ms": 58.27975273132324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1836}, "4": {"k": 4, "answer": "NFL teams", "latency_ms": 296.81897163391113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2476}, "5": {"k": 5, "answer": "Teams pick players in the NFL draft.", "latency_ms": 441.9207572937012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3063}, "6": {"k": 6, "answer": "NFL teams", "latency_ms": 175.87542533874512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3664}}}
{"qid": "1656636013570720778", "question": "who discovered gold in the sierra nevada of california", "gold_answers": ["James W. Marshall"], "shots": {"0": {"k": 0, "answer": "James W. Marshall", "latency_ms": 70.76764106750488, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 168.40434074401855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "James W. Marshall", "latency_ms": 96.42362594604492, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1285}, "3": {"k": 3, "answer": "James W. Marshall", "latency_ms": 199.84722137451172, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1895}, "4": {"k": 4, "answer": "James W. Marshall", "latency_ms": 218.3067798614502, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2573}, "5": {"k": 5, "answer": "James W. Marshall", "latency_ms": 214.11752700805664, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3200}, "6": {"k": 6, "answer": "James W. Marshall", "latency_ms": 175.0955581665039, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3846}}}
{"qid": "6797662554091549033", "question": "which mirror is used in vehicles for rear view", "gold_answers": ["rear - view mirror"], "shots": {"0": {"k": 0, "answer": "Convex mirror", "latency_ms": 175.25434494018555, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "rear-view mirror", "latency_ms": 206.51817321777344, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "rear-view mirror", "latency_ms": 133.74781608581543, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1233}, "3": {"k": 3, "answer": "rear-view mirror", "latency_ms": 147.4316120147705, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1897}, "4": {"k": 4, "answer": "rear-view mirror", "latency_ms": 105.80587387084961, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2528}, "5": {"k": 5, "answer": "rear-view mirror", "latency_ms": 111.8781566619873, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3154}, "6": {"k": 6, "answer": "rear-view mirror", "latency_ms": 155.43603897094727, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3800}}}
{"qid": "7788386234127027369", "question": "how many medals did austria win in the 2011 alpine skiing world championships", "gold_answers": ["8"], "shots": {"0": {"k": 0, "answer": "1 medal", "latency_ms": 54.20804023742676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 252.1047592163086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 579}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 306.5452575683594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1266}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 311.2971782684326, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1895}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 145.9200382232666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2511}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 183.0296516418457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3150}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 289.39151763916016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3814}}}
{"qid": "647605647914971565", "question": "who wrote lyrics for phantom of the opera", "gold_answers": ["Richard Stilgoe", "Charles Hart and Richard Stilgoe", "Charles Hart"], "shots": {"0": {"k": 0, "answer": "Charles Hart", "latency_ms": 95.3664779663086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Charles Hart and Richard Stilgoe, and additional lyrics by Mike Batt", "latency_ms": 361.88507080078125, "em": 0.0, "f1": 0.625, "containment": 1.0, "context_chars": 617}, "2": {"k": 2, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 167.2651767730713, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1253}, "3": {"k": 3, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 351.0160446166992, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1885}, "4": {"k": 4, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 220.34978866577148, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2576}, "5": {"k": 5, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 376.1601448059082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3240}, "6": {"k": 6, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 299.8173236846924, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3850}}}
{"qid": "-7980433566693172177", "question": "what culture region is germany a part of", "gold_answers": ["Central Germany"], "shots": {"0": {"k": 0, "answer": "Central Europe", "latency_ms": 113.94810676574707, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Central Germany", "latency_ms": 152.58193016052246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 646}, "2": {"k": 2, "answer": "Central Germany", "latency_ms": 103.5001277923584, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1374}, "3": {"k": 3, "answer": "Central Germany (cultural area)", "latency_ms": 355.121374130249, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2020}, "4": {"k": 4, "answer": "Central Germany (cultural area)", "latency_ms": 395.3061103820801, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2704}, "5": {"k": 5, "answer": "German culture region", "latency_ms": 167.66715049743652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3349}, "6": {"k": 6, "answer": "German culture region", "latency_ms": 253.3586025238037, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4031}}}
{"qid": "6268706985209815767", "question": "mount and blade with fire and sword time period", "gold_answers": ["1648 - 51"], "shots": {"0": {"k": 0, "answer": "13th century", "latency_ms": 289.1116142272949, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "post-medieval era", "latency_ms": 87.5852108001709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "1648–1651", "latency_ms": 474.3032455444336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1301}, "3": {"k": 3, "answer": "1648–1651", "latency_ms": 359.38167572021484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1978}, "4": {"k": 4, "answer": "1648–1651", "latency_ms": 173.15244674682617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2661}, "5": {"k": 5, "answer": "1648–1651", "latency_ms": 241.20783805847168, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3339}, "6": {"k": 6, "answer": "1648–1651 war against the Khmelnytsky Uprising in present-day Ukraine", "latency_ms": 689.2516613006592, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3964}}}
