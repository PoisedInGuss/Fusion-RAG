{"qid": "5306300389470118812", "question": "what was the religion in persia before islam", "gold_answers": ["the Zoroastrian religion", "Zoroastrian"], "shots": {"0": {"k": 0, "answer": "Zoroastrianism", "latency_ms": 186.12051010131836, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Zoroastrianism", "latency_ms": 136.65199279785156, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 637}, "2": {"k": 2, "answer": "Zoroastrianism", "latency_ms": 327.5439739227295, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1302}, "3": {"k": 3, "answer": "Zoroastrianism", "latency_ms": 114.43758010864258, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1940}, "4": {"k": 4, "answer": "Zoroastrianism", "latency_ms": 239.43066596984863, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 2601}, "5": {"k": 5, "answer": "Zoroastrianism", "latency_ms": 254.3778419494629, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3286}, "6": {"k": 6, "answer": "Zoroastrianism", "latency_ms": 220.46852111816406, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3917}}}
{"qid": "4492088526674104172", "question": "why does kerosene oil rise up in the wick of lantern", "gold_answers": ["capillary action"], "shots": {"0": {"k": 0, "answer": "Capillary action", "latency_ms": 83.06336402893066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "capillary action", "latency_ms": 137.7394199371338, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 653}, "2": {"k": 2, "answer": "capillary action", "latency_ms": 188.7948513031006, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1270}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 254.82416152954102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1905}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 139.34969902038574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2580}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 240.00811576843262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3248}, "6": {"k": 6, "answer": "capillary action", "latency_ms": 348.22559356689453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3851}}}
{"qid": "-4373262132142058334", "question": "mention the chemical change that proinsulin undergo to be able to act as mature insulin", "gold_answers": ["post translational modification"], "shots": {"0": {"k": 0, "answer": "Proteolytic cleavage", "latency_ms": 240.52977561950684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cleavage of the C peptide-A chain bond", "latency_ms": 391.48640632629395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 606}, "2": {"k": 2, "answer": "C-peptide is cleaved off", "latency_ms": 132.23743438720703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1253}, "3": {"k": 3, "answer": "C-peptide is cleaved off", "latency_ms": 310.6193542480469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1886}, "4": {"k": 4, "answer": "C-peptide is cleaved off", "latency_ms": 331.59780502319336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2523}, "5": {"k": 5, "answer": "C-peptide is cleaved off", "latency_ms": 253.53264808654785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3227}, "6": {"k": 6, "answer": "C-peptide is cleaved", "latency_ms": 262.22777366638184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3776}}}
{"qid": "-5298560410953979569", "question": "where does the sweetness of fruit come from", "gold_answers": ["Fructose , or fruit sugar", "Fructose"], "shots": {"0": {"k": 0, "answer": "Fructose", "latency_ms": 130.17845153808594, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "sugars", "latency_ms": 169.00038719177246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 677}, "2": {"k": 2, "answer": "fruit flavors and sugars", "latency_ms": 235.14342308044434, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 1341}, "3": {"k": 3, "answer": "fruit flavors are not inherently sweet; sweetness is altered by substances like lactisole, which suppresses sweet taste to enhance fruit flavors. The sweetness of fruit itself comes from natural sugars such as fructose, glucose, and sucrose. However, in the context provided, the source of sweetness alteration in fruit preserves is not directly stated. Therefore, based on the given context, the answer is:\n\nI cannot answer.", "latency_ms": 2157.0887565612793, "em": 0.0, "f1": 0.0625, "containment": 1.0, "context_chars": 1980}, "4": {"k": 4, "answer": "glycyrrhizin", "latency_ms": 239.49742317199707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2589}, "5": {"k": 5, "answer": "sugars, glycosides, and sweet proteins", "latency_ms": 407.8242778778076, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3293}, "6": {"k": 6, "answer": "sugars, glycosides, mogrosides, amino acids, and sweet proteins", "latency_ms": 1122.6122379302979, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3911}}}
{"qid": "4344648368832551136", "question": "an object that moves around an external axis is said to be", "gold_answers": ["orbit", "revolution or orbital revolution"], "shots": {"0": {"k": 0, "answer": "rotating", "latency_ms": 226.5486717224121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "revolving", "latency_ms": 111.85979843139648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "revolving", "latency_ms": 273.9081382751465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1169}, "3": {"k": 3, "answer": "revolving", "latency_ms": 138.8533115386963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1803}, "4": {"k": 4, "answer": "revolving", "latency_ms": 122.99275398254395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2409}, "5": {"k": 5, "answer": "revolving", "latency_ms": 200.65712928771973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3088}, "6": {"k": 6, "answer": "revolving", "latency_ms": 146.64173126220703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3748}}}
{"qid": "-724426540432437711", "question": "what was the name of the plant on little shop of horrors", "gold_answers": ["Audrey II"], "shots": {"0": {"k": 0, "answer": "Audrey II", "latency_ms": 115.27657508850098, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Audrey II", "latency_ms": 189.58330154418945, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 615}, "2": {"k": 2, "answer": "Audrey II", "latency_ms": 135.3323459625244, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1244}, "3": {"k": 3, "answer": "Audrey II", "latency_ms": 216.69602394104004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1852}, "4": {"k": 4, "answer": "Audrey II", "latency_ms": 130.842924118042, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2522}, "5": {"k": 5, "answer": "Audrey II", "latency_ms": 130.10597229003906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3124}, "6": {"k": 6, "answer": "Audrey II", "latency_ms": 275.6180763244629, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3773}}}
{"qid": "-5541610970112826260", "question": "who signed the declaration of independence for maryland", "gold_answers": ["Samuel Chase", "Charles Carroll of Carrollton", "Thomas Stone", "William Paca"], "shots": {"0": {"k": 0, "answer": "George Wythe", "latency_ms": 221.65346145629883, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Samuel Chase", "latency_ms": 76.27677917480469, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 690}, "2": {"k": 2, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 568.82643699646, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 1365}, "3": {"k": 3, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll of Carrollton", "latency_ms": 610.5821132659912, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2050}, "4": {"k": 4, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll of Carrollton", "latency_ms": 575.1914978027344, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2704}, "5": {"k": 5, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll of Carrollton", "latency_ms": 307.74688720703125, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3324}, "6": {"k": 6, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll of Carrollton", "latency_ms": 396.3000774383545, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3940}}}
{"qid": "-86997217831791339", "question": "when does wonder woman come out on blue ray", "gold_answers": ["September 19 , 2017"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 158.80799293518066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 19, 2017", "latency_ms": 164.8716926574707, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 606}, "2": {"k": 2, "answer": "September 19, 2017", "latency_ms": 166.48435592651367, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1206}, "3": {"k": 3, "answer": "September 19, 2017", "latency_ms": 437.86096572875977, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1870}, "4": {"k": 4, "answer": "September 19, 2017", "latency_ms": 570.49560546875, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2458}, "5": {"k": 5, "answer": "September 19, 2017", "latency_ms": 203.70984077453613, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3085}, "6": {"k": 6, "answer": "September 19, 2017", "latency_ms": 366.344690322876, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3739}}}
{"qid": "-8062812506672279762", "question": "what nba team did derrick rose play for", "gold_answers": ["New York Knicks", "Minnesota Timberwolves", "Chicago Bulls", "Cleveland Cavaliers"], "shots": {"0": {"k": 0, "answer": "Chicago Bulls", "latency_ms": 51.751136779785156, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Minnesota Timberwolves", "latency_ms": 80.77383041381836, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 630}, "2": {"k": 2, "answer": "Chicago Bulls, Minnesota Timberwolves, Cleveland Cavaliers", "latency_ms": 465.2895927429199, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1238}, "3": {"k": 3, "answer": "Chicago Bulls, Minnesota Timberwolves, Cleveland Cavaliers", "latency_ms": 501.9197463989258, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1871}, "4": {"k": 4, "answer": "Chicago Bulls, Minnesota Timberwolves, Cleveland Cavaliers", "latency_ms": 686.5789890289307, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2467}, "5": {"k": 5, "answer": "Chicago Bulls, Minnesota Timberwolves, Cleveland Cavaliers, New York Knicks", "latency_ms": 429.88085746765137, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3045}, "6": {"k": 6, "answer": "Chicago Bulls, Minnesota Timberwolves, Cleveland Cavaliers, New York Knicks", "latency_ms": 470.26872634887695, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3615}}}
{"qid": "7933002036740390435", "question": "where did the term liberal arts come from", "gold_answers": ["the Roman Empire"], "shots": {"0": {"k": 0, "answer": "Rome", "latency_ms": 52.96921730041504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Roman Empire", "latency_ms": 56.06865882873535, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Marcus Tullius Cicero", "latency_ms": 217.6828384399414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1259}, "3": {"k": 3, "answer": "Marcus Tullius Cicero", "latency_ms": 207.01193809509277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1921}, "4": {"k": 4, "answer": "Marcus Tullius Cicero", "latency_ms": 212.62836456298828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2579}, "5": {"k": 5, "answer": "Roman Empire", "latency_ms": 183.93659591674805, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3313}, "6": {"k": 6, "answer": "Roman Empire", "latency_ms": 171.06032371520996, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4008}}}
{"qid": "-2991400700385487821", "question": "who heads the executive department of west virginia government", "gold_answers": ["the Governor of West Virginia", "the governor of West Virginia", "Jim Justice"], "shots": {"0": {"k": 0, "answer": "Governor of West Virginia", "latency_ms": 135.44821739196777, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "governor", "latency_ms": 238.95716667175293, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "Governor of West Virginia", "latency_ms": 283.6804389953613, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Governor of West Virginia", "latency_ms": 150.9876251220703, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2022}, "4": {"k": 4, "answer": "Governor of West Virginia", "latency_ms": 401.48043632507324, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2703}, "5": {"k": 5, "answer": "Governor of West Virginia", "latency_ms": 171.1721420288086, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3380}, "6": {"k": 6, "answer": "Governor of West Virginia", "latency_ms": 194.27967071533203, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3982}}}
{"qid": "-9186689755642837558", "question": "how long is the bridge between new brunswick and prince edward island", "gold_answers": ["12.9 - kilometre"], "shots": {"0": {"k": 0, "answer": "1.4 kilometers", "latency_ms": 131.37245178222656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "12.9 km", "latency_ms": 99.33328628540039, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "The context does not specify the length of the bridge between New Brunswick and Prince Edward Island. I cannot answer.", "latency_ms": 764.3930912017822, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1321}, "3": {"k": 3, "answer": "The context does not specify the length of the Confederation Bridge. I cannot answer.", "latency_ms": 605.003833770752, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2009}, "4": {"k": 4, "answer": "The context does not specify the length of the Confederation Bridge. I cannot answer.", "latency_ms": 505.0506591796875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2732}, "5": {"k": 5, "answer": "13 kilometres", "latency_ms": 168.9605712890625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3398}, "6": {"k": 6, "answer": "The context does not specify the length of the Confederation Bridge between New Brunswick and Prince Edward Island. I cannot answer.", "latency_ms": 1106.5654754638672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4073}}}
{"qid": "8029901619424338449", "question": "when did the uk and us become allies", "gold_answers": ["1940", "Since 1940"], "shots": {"0": {"k": 0, "answer": "1941", "latency_ms": 86.30037307739258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 162.03737258911133, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 583}, "2": {"k": 2, "answer": "1945", "latency_ms": 90.47722816467285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1205}, "3": {"k": 3, "answer": "1953", "latency_ms": 105.06391525268555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1821}, "4": {"k": 4, "answer": "1945", "latency_ms": 350.0227928161621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2459}, "5": {"k": 5, "answer": "1949", "latency_ms": 346.9400405883789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3057}, "6": {"k": 6, "answer": "1945", "latency_ms": 169.37851905822754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3696}}}
{"qid": "4828649525820722736", "question": "who sang the theme song for the man with the golden gun", "gold_answers": ["Lulu"], "shots": {"0": {"k": 0, "answer": "Lulu", "latency_ms": 75.78682899475098, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lulu", "latency_ms": 95.19672393798828, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 582}, "2": {"k": 2, "answer": "Lulu", "latency_ms": 92.18621253967285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1225}, "3": {"k": 3, "answer": "Lulu", "latency_ms": 205.1413059234619, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1838}, "4": {"k": 4, "answer": "Lulu", "latency_ms": 130.10430335998535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2431}, "5": {"k": 5, "answer": "Lulu", "latency_ms": 155.26938438415527, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3016}, "6": {"k": 6, "answer": "Lulu", "latency_ms": 106.5065860748291, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3673}}}
{"qid": "-7924663090486742985", "question": "american horror story freak show girl kidnapped by clown", "gold_answers": ["Bonnie Lipton"], "shots": {"0": {"k": 0, "answer": "Cassandra Freeman", "latency_ms": 134.2484951019287, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Twisty", "latency_ms": 59.17167663574219, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "Twisty", "latency_ms": 105.94582557678223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1288}, "3": {"k": 3, "answer": "Twisty", "latency_ms": 147.76372909545898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1987}, "4": {"k": 4, "answer": "Twisty", "latency_ms": 82.76104927062988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2681}, "5": {"k": 5, "answer": "Twisty", "latency_ms": 228.21807861328125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3340}, "6": {"k": 6, "answer": "Twisty", "latency_ms": 140.30170440673828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3919}}}
{"qid": "-360675773049242516", "question": "the probability of making a type i error when retaining ho at the .05 level of significance is", "gold_answers": ["5 %"], "shots": {"0": {"k": 0, "answer": "0.05", "latency_ms": 77.32582092285156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "0.05", "latency_ms": 112.60867118835449, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 575}, "2": {"k": 2, "answer": "0.05", "latency_ms": 172.45030403137207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1143}, "3": {"k": 3, "answer": "0.05", "latency_ms": 182.2037696838379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1778}, "4": {"k": 4, "answer": "0.05", "latency_ms": 194.00691986083984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2423}, "5": {"k": 5, "answer": "0.05", "latency_ms": 132.83634185791016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3015}, "6": {"k": 6, "answer": "0.05", "latency_ms": 195.2073574066162, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3591}}}
{"qid": "5819386267283467034", "question": "what year did the us hockey team won the olympics", "gold_answers": ["1960", "1980", "1960 and 1980"], "shots": {"0": {"k": 0, "answer": "1980", "latency_ms": 112.77008056640625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 158.1406593322754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 674}, "2": {"k": 2, "answer": "1920", "latency_ms": 203.1424045562744, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1306}, "3": {"k": 3, "answer": "1920", "latency_ms": 128.10611724853516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2025}, "4": {"k": 4, "answer": "1920, 1960, 1980", "latency_ms": 532.1404933929443, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2686}, "5": {"k": 5, "answer": "1920, 1960, 1980", "latency_ms": 628.4518241882324, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3365}, "6": {"k": 6, "answer": "1920, 1960, 1980", "latency_ms": 380.5363178253174, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 4027}}}
{"qid": "3916394566028297311", "question": "who wrote it's a long long way to pasadena", "gold_answers": ["Harry Vanda", "John Young", "David Hemmings", "George Young"], "shots": {"0": {"k": 0, "answer": "John Phillips", "latency_ms": 49.99351501464844, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 125.84304809570312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 604}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 110.96906661987305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 125.81634521484375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1857}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 176.12028121948242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2492}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 356.45532608032227, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3146}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 268.3537006378174, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3781}}}
{"qid": "-2432292250757146771", "question": "who wrote the song i hate you i love you", "gold_answers": ["Angel Benitez", "Olivia O'Brien", "Garrett Nash"], "shots": {"0": {"k": 0, "answer": "Lil Wayne", "latency_ms": 212.69512176513672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ne-Yo, StarGate", "latency_ms": 185.13989448547363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "Ne-Yo and StarGate", "latency_ms": 339.5199775695801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "Ne-Yo, StarGate", "latency_ms": 311.1693859100342, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1810}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 115.71168899536133, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2451}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 230.46445846557617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3046}, "6": {"k": 6, "answer": "Ne-Yo", "latency_ms": 128.4048557281494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3704}}}
{"qid": "2393389448844632496", "question": "what is loihi and where does it originate from", "gold_answers": ["from the Hawaii hotspot"], "shots": {"0": {"k": 0, "answer": "Loihi Seamount, Hawaii", "latency_ms": 383.71729850769043, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 84.70034599304199, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "Lōʻihi Seamount; Hawaii hotspot", "latency_ms": 340.4395580291748, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1310}, "3": {"k": 3, "answer": "Lōʻihi Seamount; Hawaiian-Emperor seamount chain", "latency_ms": 735.7642650604248, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1902}, "4": {"k": 4, "answer": "Loihi is a seamount in Hawaii that originates from a hotspot volcano, forming part of the Hawaiian-Emperor seamount chain. It began forming around 400,000 years ago and is expected to emerge above sea level in 10,000–100,000 years. The name \"Loihi\" is also used for Intel's neuromorphic chip, named after the seamount.", "latency_ms": 3932.5809478759766, "em": 0.0, "f1": 0.1224489795918367, "containment": 0.0, "context_chars": 2627}, "5": {"k": 5, "answer": "Lōʻihi Seamount; Hawaiian hotspot volcano", "latency_ms": 506.3142776489258, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 3287}, "6": {"k": 6, "answer": "Lōʻihi Seamount; Hawaiian-Emperor seamount chain", "latency_ms": 513.7674808502197, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3919}}}
{"qid": "-6677440871585683891", "question": "who rebuilt the temple after the babylonian captivity", "gold_answers": ["Herod"], "shots": {"0": {"k": 0, "answer": "Zerubbabel", "latency_ms": 81.74514770507812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 124.67718124389648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "Cyrus", "latency_ms": 327.76570320129395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "Cyrus", "latency_ms": 172.11008071899414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1823}, "4": {"k": 4, "answer": "Cyrus", "latency_ms": 119.70210075378418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2493}, "5": {"k": 5, "answer": "Persian king Cyrus", "latency_ms": 287.83249855041504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3112}, "6": {"k": 6, "answer": "Persian king Cyrus", "latency_ms": 126.23071670532227, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3710}}}
{"qid": "-6750750235985613277", "question": "who was executed for being an american spy during the revolutionary war", "gold_answers": ["Nathan Hale"], "shots": {"0": {"k": 0, "answer": "Richard Croft", "latency_ms": 63.974857330322266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Nathan Hale", "latency_ms": 204.90312576293945, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 593}, "2": {"k": 2, "answer": "Nathan Hale", "latency_ms": 95.4582691192627, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1271}, "3": {"k": 3, "answer": "Nathan Hale", "latency_ms": 141.0219669342041, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1870}, "4": {"k": 4, "answer": "Nathan Hale", "latency_ms": 153.11074256896973, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2542}, "5": {"k": 5, "answer": "Nathan Hale", "latency_ms": 310.18614768981934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3236}, "6": {"k": 6, "answer": "Nathan Hale", "latency_ms": 360.10122299194336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3859}}}
{"qid": "8770209312170080158", "question": "who sang the song suddenly with olivia newton john", "gold_answers": ["Cliff Richard"], "shots": {"0": {"k": 0, "answer": "Colin Hay", "latency_ms": 139.30535316467285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cliff Richard", "latency_ms": 156.74901008605957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "Cliff Richard", "latency_ms": 269.43302154541016, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1270}, "3": {"k": 3, "answer": "Cliff Richard", "latency_ms": 125.86283683776855, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1899}, "4": {"k": 4, "answer": "Cliff Richard", "latency_ms": 263.55981826782227, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2553}, "5": {"k": 5, "answer": "Cliff Richard", "latency_ms": 282.59754180908203, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3206}, "6": {"k": 6, "answer": "Cliff Richard", "latency_ms": 445.37806510925293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3893}}}
{"qid": "-180549795106449014", "question": "who is command sergeant major of the army", "gold_answers": ["Daniel A. Dailey"], "shots": {"0": {"k": 0, "answer": "as of my last update, the Command Sergeant Major of the Army is Major General David A. M. Smith. However, this information may have changed. For the most current information, please consult the official U.S. Army website.", "latency_ms": 1697.8070735931396, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 120.34296989440918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 669}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 195.7850456237793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1316}, "3": {"k": 3, "answer": "Command Sergeant-Major Jon R. Stanley", "latency_ms": 340.83008766174316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2128}, "4": {"k": 4, "answer": "Command Sergeant Major of the Army", "latency_ms": 389.7378444671631, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2750}, "5": {"k": 5, "answer": "Raymond F. Chandler III", "latency_ms": 328.68170738220215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3427}, "6": {"k": 6, "answer": "Raymond F. Chandler III", "latency_ms": 527.3857116699219, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4034}}}
{"qid": "7815138213728640354", "question": "who sings the original windmills of your mind", "gold_answers": ["Noel Harrison"], "shots": {"0": {"k": 0, "answer": "The Kinks", "latency_ms": 84.8243236541748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Frank Sinatra", "latency_ms": 199.80835914611816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "The song \"The Windmills of Your Mind\" was introduced in the film *The Thomas Crown Affair* (1968), and the original version was performed by **Frank Sinatra**. However, based on the provided context, it does not explicitly state who sings the original version.\n\nI cannot answer.", "latency_ms": 2307.133197784424, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "Noel Harrison", "latency_ms": 144.8976993560791, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1918}, "4": {"k": 4, "answer": "Noel Harrison", "latency_ms": 156.68582916259766, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2554}, "5": {"k": 5, "answer": "Noel Harrison", "latency_ms": 200.4683017730713, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3206}, "6": {"k": 6, "answer": "Noel Harrison", "latency_ms": 270.1144218444824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3795}}}
{"qid": "2098168902147822379", "question": "where will the next summer and winter olympics be held", "gold_answers": ["Beijing", "Tokyo"], "shots": {"0": {"k": 0, "answer": "2024: Paris, France; 2026: Milan-Cortina, Italy", "latency_ms": 342.30542182922363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2022 Winter Olympics in Beijing, China; next Summer Olympics not specified in context.", "latency_ms": 776.0007381439209, "em": 0.0, "f1": 0.14285714285714288, "containment": 1.0, "context_chars": 669}, "2": {"k": 2, "answer": "Tokyo and Beijing", "latency_ms": 179.58426475524902, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1294}, "3": {"k": 3, "answer": "Tokyo (Summer 2020), Beijing (Winter 2022)", "latency_ms": 1042.2930717468262, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 1887}, "4": {"k": 4, "answer": "Los Angeles, California (2028 Summer); Beijing, China (2022 Winter)", "latency_ms": 550.1058101654053, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 2520}, "5": {"k": 5, "answer": "Los Angeles (2028 Summer), Beijing (2022 Winter)", "latency_ms": 556.3740730285645, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 3118}, "6": {"k": 6, "answer": "2028 Summer Olympics in Los Angeles, California, United States; 2022 Winter Olympics in Beijing, China", "latency_ms": 811.9709491729736, "em": 0.0, "f1": 0.125, "containment": 1.0, "context_chars": 3779}}}
{"qid": "-1458476052377668874", "question": "when did ole miss beat alabama in football", "gold_answers": ["September 19 , 2015", "October 3 , 1970", "October 27 , 1894", "October 13 , 2001", "October 5 , 1968", "October 4 , 2014", "September 11 , 1976", "October 8 , 1988", "October 18 , 2003", "November 5 , 1910"], "shots": {"0": {"k": 0, "answer": "November 27, 1971", "latency_ms": 476.52649879455566, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "the game where the Rebels never trailed and led by as many as twenty points and as many as nineteen in the fourth quarter.", "latency_ms": 728.6643981933594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 588}, "2": {"k": 2, "answer": "1988", "latency_ms": 321.6972351074219, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1229}, "3": {"k": 3, "answer": "2014", "latency_ms": 348.4835624694824, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1864}, "4": {"k": 4, "answer": "1988", "latency_ms": 118.15476417541504, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2466}, "5": {"k": 5, "answer": "1988", "latency_ms": 163.3017063140869, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3094}, "6": {"k": 6, "answer": "1988", "latency_ms": 288.00225257873535, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3713}}}
{"qid": "-6861734002351236912", "question": "who sang the song how far is heaven", "gold_answers": ["Los Lonely Boys"], "shots": {"0": {"k": 0, "answer": "Luther Vandross", "latency_ms": 155.63225746154785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Justin Moore", "latency_ms": 93.09577941894531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "Justin Moore", "latency_ms": 84.64670181274414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1213}, "3": {"k": 3, "answer": "Justin Moore", "latency_ms": 185.3964328765869, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1863}, "4": {"k": 4, "answer": "Justin Moore", "latency_ms": 108.5200309753418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2474}, "5": {"k": 5, "answer": "Justin Moore", "latency_ms": 205.93762397766113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3142}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 197.72005081176758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3759}}}
{"qid": "-1722216190521937227", "question": "where are the organ mountains in new mexico", "gold_answers": ["southern New Mexico"], "shots": {"0": {"k": 0, "answer": "southwest corner of New Mexico, near the border with Arizona", "latency_ms": 591.7222499847412, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 539.0269756317139, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 606}, "2": {"k": 2, "answer": "southern New Mexico, near Las Cruces in Doña Ana County", "latency_ms": 495.342493057251, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1325}, "3": {"k": 3, "answer": "southern New Mexico, near Las Cruces in Doña Ana County", "latency_ms": 524.3511199951172, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1986}, "4": {"k": 4, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 325.7784843444824, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 2607}, "5": {"k": 5, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 436.4445209503174, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3264}, "6": {"k": 6, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 674.1340160369873, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3960}}}
{"qid": "-2717119588239727064", "question": "where are the 10 plagues found in the bible", "gold_answers": ["Book of Exodus", "Exodus"], "shots": {"0": {"k": 0, "answer": "Exodus 7–12", "latency_ms": 276.16024017333984, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Book of Exodus", "latency_ms": 164.92366790771484, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 645}, "2": {"k": 2, "answer": "Book of Exodus", "latency_ms": 115.13614654541016, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1279}, "3": {"k": 3, "answer": "Book of Exodus", "latency_ms": 263.9157772064209, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1887}, "4": {"k": 4, "answer": "Book of Exodus", "latency_ms": 159.4836711883545, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2459}, "5": {"k": 5, "answer": "Book of Exodus", "latency_ms": 299.49951171875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3127}, "6": {"k": 6, "answer": "Book of Exodus", "latency_ms": 136.51371002197266, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3814}}}
{"qid": "2988284357714596500", "question": "where are red blood cells made in adults", "gold_answers": ["in the bone marrow"], "shots": {"0": {"k": 0, "answer": "bone marrow", "latency_ms": 97.19705581665039, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "red bone marrow", "latency_ms": 71.69842720031738, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "bone marrow", "latency_ms": 142.6534652709961, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1295}, "3": {"k": 3, "answer": "bone marrow", "latency_ms": 107.58662223815918, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1911}, "4": {"k": 4, "answer": "bone marrow", "latency_ms": 94.67411041259766, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2609}, "5": {"k": 5, "answer": "bone marrow", "latency_ms": 184.2958927154541, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3259}, "6": {"k": 6, "answer": "bone marrow", "latency_ms": 107.87677764892578, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3901}}}
{"qid": "-4616596799374362422", "question": "who was the pinkerton detective agency's first female detective", "gold_answers": ["Kate Warne"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 132.948637008667, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kate Warne", "latency_ms": 76.62677764892578, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 609}, "2": {"k": 2, "answer": "Kate Warne", "latency_ms": 83.3737850189209, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1235}, "3": {"k": 3, "answer": "Kate Warne", "latency_ms": 92.78297424316406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1918}, "4": {"k": 4, "answer": "Kate Warne", "latency_ms": 134.39512252807617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2543}, "5": {"k": 5, "answer": "Kate Warne", "latency_ms": 221.37045860290527, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3232}, "6": {"k": 6, "answer": "Kate Warne", "latency_ms": 174.58343505859375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3910}}}
{"qid": "-3650291155113659146", "question": "how many episodes are there in modern family", "gold_answers": ["232", "201"], "shots": {"0": {"k": 0, "answer": "216 episodes", "latency_ms": 167.29497909545898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 298.49863052368164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 685}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 108.08253288269043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1325}, "3": {"k": 3, "answer": "24 episodes", "latency_ms": 182.9838752746582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1970}, "4": {"k": 4, "answer": "24 episodes", "latency_ms": 186.04397773742676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2641}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 411.6342067718506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3268}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 317.5921440124512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3917}}}
{"qid": "3381924381590631417", "question": "who built the first temple for god in jerusalem", "gold_answers": ["Solomon"], "shots": {"0": {"k": 0, "answer": "King Solomon", "latency_ms": 53.73525619506836, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "King Solomon", "latency_ms": 112.6565933227539, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 643}, "2": {"k": 2, "answer": "Josiah", "latency_ms": 105.87096214294434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1278}, "3": {"k": 3, "answer": "Solomon", "latency_ms": 168.82896423339844, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1868}, "4": {"k": 4, "answer": "Solomon", "latency_ms": 267.453670501709, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2469}, "5": {"k": 5, "answer": "Solomon", "latency_ms": 159.57188606262207, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3095}, "6": {"k": 6, "answer": "Solomon", "latency_ms": 158.3399772644043, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3758}}}
{"qid": "-154783694579651082", "question": "what is a dropped pin on google maps for", "gold_answers": ["marks locations", "marks locations in Google Maps"], "shots": {"0": {"k": 0, "answer": "A dropped pin on Google Maps is typically a user-created marker indicating a location of interest, such as a business, event, or personal spot.", "latency_ms": 465.58642387390137, "em": 0.0, "f1": 0.16000000000000003, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "a location marker", "latency_ms": 218.19567680358887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "a dropped pin on google maps marks a specific location", "latency_ms": 653.3243656158447, "em": 0.0, "f1": 0.4615384615384615, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "a location", "latency_ms": 152.01926231384277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1838}, "4": {"k": 4, "answer": "to accurately mark a point on a map without obscuring the location", "latency_ms": 433.8538646697998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2466}, "5": {"k": 5, "answer": "a location on a map", "latency_ms": 336.93480491638184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3073}, "6": {"k": 6, "answer": "a location marker", "latency_ms": 151.17216110229492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3668}}}
{"qid": "6915606477668963399", "question": "what do the 3 dots mean in math", "gold_answers": ["the therefore sign", "therefore sign"], "shots": {"0": {"k": 0, "answer": "ellipses", "latency_ms": 170.0592041015625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "therefore", "latency_ms": 57.86299705505371, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "therefore", "latency_ms": 70.79434394836426, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1298}, "3": {"k": 3, "answer": "\"therefore\"", "latency_ms": 316.3292407989502, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1896}, "4": {"k": 4, "answer": "\"therefore\"", "latency_ms": 311.7046356201172, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2592}, "5": {"k": 5, "answer": "\"therefore\"", "latency_ms": 190.2792453765869, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3251}, "6": {"k": 6, "answer": "\"therefore\"", "latency_ms": 143.17607879638672, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3868}}}
{"qid": "-8366545547296627039", "question": "who wrote the song photograph by ringo starr", "gold_answers": ["Richard Starkey", "George Harrison", "Ringo Starr"], "shots": {"0": {"k": 0, "answer": "Ringo Starr", "latency_ms": 65.96755981445312, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ringo Starr and George Harrison", "latency_ms": 218.89662742614746, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 671}, "2": {"k": 2, "answer": "Ringo Starr and George Harrison", "latency_ms": 345.6840515136719, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1329}, "3": {"k": 3, "answer": "George Harrison", "latency_ms": 228.29151153564453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1957}, "4": {"k": 4, "answer": "Ringo Starr and George Harrison", "latency_ms": 338.68861198425293, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2613}, "5": {"k": 5, "answer": "Ringo Starr and George Harrison", "latency_ms": 420.54009437561035, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3308}, "6": {"k": 6, "answer": "Ringo Starr and George Harrison", "latency_ms": 388.5033130645752, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3963}}}
{"qid": "-5004457603684974952", "question": "who is playing the halftime show at super bowl 2016", "gold_answers": ["Beyoncé", "Coldplay", "Bruno Mars"], "shots": {"0": {"k": 0, "answer": "Coldplay", "latency_ms": 171.39697074890137, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Coldplay", "latency_ms": 108.1399917602539, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 634}, "2": {"k": 2, "answer": "Coldplay with special guest performers Beyoncé and Bruno Mars", "latency_ms": 607.7775955200195, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 1271}, "3": {"k": 3, "answer": "Coldplay with Beyoncé and Bruno Mars", "latency_ms": 389.33300971984863, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1924}, "4": {"k": 4, "answer": "Coldplay with Beyoncé and Bruno Mars", "latency_ms": 584.6400260925293, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2560}, "5": {"k": 5, "answer": "Coldplay, Beyoncé, Bruno Mars", "latency_ms": 178.65538597106934, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3222}, "6": {"k": 6, "answer": "Coldplay with Beyoncé and Bruno Mars", "latency_ms": 491.4522171020508, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3882}}}
{"qid": "7420920477821898435", "question": "to aru kagaku no railgun s episode 3", "gold_answers": ["Project Radio Noise"], "shots": {"0": {"k": 0, "answer": "1999", "latency_ms": 80.03687858581543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 192.51275062561035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 599}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 213.2110595703125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1204}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 215.70563316345215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1846}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 243.0870532989502, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2510}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 163.26498985290527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3130}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 163.49387168884277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3766}}}
{"qid": "7478795216476346339", "question": "who won the 2017 sports personality of the year", "gold_answers": ["Mo Farah"], "shots": {"0": {"k": 0, "answer": "Lionel Messi", "latency_ms": 280.98058700561523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dara Ó Briain", "latency_ms": 332.0605754852295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 144.86074447631836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 290.24648666381836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1976}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 344.85340118408203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2587}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 122.99227714538574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3252}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 183.81142616271973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3891}}}
{"qid": "7217222058435937287", "question": "where was the world economic forum held this year", "gold_answers": ["Davos"], "shots": {"0": {"k": 0, "answer": "Davos, Switzerland", "latency_ms": 154.53457832336426, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 207.7484130859375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 689}, "2": {"k": 2, "answer": "Davos-Klosters", "latency_ms": 187.38174438476562, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1360}, "3": {"k": 3, "answer": "Davos-Klosters", "latency_ms": 151.60751342773438, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 2024}, "4": {"k": 4, "answer": "Davos-Klosters", "latency_ms": 228.1363010406494, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 2725}, "5": {"k": 5, "answer": "Davos-Klosters", "latency_ms": 179.5206069946289, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3424}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 151.89361572265625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4103}}}
{"qid": "-143054837169120955", "question": "where are the giant redwoods located in california", "gold_answers": ["the coast of northern California", "Del Norte County", "Humboldt County"], "shots": {"0": {"k": 0, "answer": "Redwood National and State Parks", "latency_ms": 173.3987331390381, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 203.51147651672363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 518.3157920837402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1276}, "3": {"k": 3, "answer": "Redwood Mountain Grove in Kings Canyon National Park and Giant Sequoia National Monument, California", "latency_ms": 618.6511516571045, "em": 0.0, "f1": 0.11111111111111112, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 336.9472026824951, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2536}, "5": {"k": 5, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 486.01722717285156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3192}, "6": {"k": 6, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 638.3559703826904, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3801}}}
{"qid": "4892429640540595424", "question": "who has made the most premier league appearances", "gold_answers": ["Gareth Barry"], "shots": {"0": {"k": 0, "answer": "Alan Shearer", "latency_ms": 101.0284423828125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 220.73698043823242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 346.3003635406494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1286}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 157.32479095458984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1867}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 164.56055641174316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2516}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 152.45914459228516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3116}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 203.56988906860352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3770}}}
{"qid": "5533906981191706877", "question": "who was the first chief minister of west bengal", "gold_answers": ["Prafulla Chandra Ghosh"], "shots": {"0": {"k": 0, "answer": "P. C. Bagchi", "latency_ms": 190.18840789794922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Prafulla Chandra Ghosh", "latency_ms": 344.0680503845215, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 693}, "2": {"k": 2, "answer": "Prafulla Chandra Ghosh", "latency_ms": 353.7709712982178, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1408}, "3": {"k": 3, "answer": "Prafulla Chandra Ghosh", "latency_ms": 252.01892852783203, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2035}, "4": {"k": 4, "answer": "Prafulla Chandra Ghosh", "latency_ms": 486.92917823791504, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2764}, "5": {"k": 5, "answer": "Prafulla Chandra Ghosh", "latency_ms": 340.55471420288086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3366}, "6": {"k": 6, "answer": "Prafulla Chandra Ghosh", "latency_ms": 256.9303512573242, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3984}}}
{"qid": "8823076125131408628", "question": "who sings got my mind set on you", "gold_answers": ["George Harrison", "James Ray"], "shots": {"0": {"k": 0, "answer": "Frank Sinatra", "latency_ms": 117.49267578125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "George Harrison", "latency_ms": 216.84002876281738, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 541}, "2": {"k": 2, "answer": "George Harrison", "latency_ms": 132.65085220336914, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1101}, "3": {"k": 3, "answer": "George Harrison", "latency_ms": 278.1994342803955, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1688}, "4": {"k": 4, "answer": "George Harrison", "latency_ms": 108.4282398223877, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2279}, "5": {"k": 5, "answer": "George Harrison", "latency_ms": 248.98433685302734, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2861}, "6": {"k": 6, "answer": "George Harrison", "latency_ms": 192.44813919067383, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3418}}}
{"qid": "9168660859139861094", "question": "two main types of research methods used in psychology", "gold_answers": ["Correlational research", "Descriptive research", "quantitative data", "Quantitative psychological research", "qualitative", "quantitative", "Qualitative psychological research", "Experimental research", "qualitative data"], "shots": {"0": {"k": 0, "answer": "Experimental and observational", "latency_ms": 125.77009201049805, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "qualitative, quantitative", "latency_ms": 195.06096839904785, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 807}, "2": {"k": 2, "answer": "qualitative, quantitative", "latency_ms": 278.0568599700928, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1631}, "3": {"k": 3, "answer": "qualitative and quantitative", "latency_ms": 386.69514656066895, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2323}, "4": {"k": 4, "answer": "qualitative and quantitative", "latency_ms": 226.12881660461426, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3060}, "5": {"k": 5, "answer": "qualitative and quantitative", "latency_ms": 288.7744903564453, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3863}, "6": {"k": 6, "answer": "qualitative and quantitative", "latency_ms": 389.7252082824707, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 4560}}}
{"qid": "-7073063191952855764", "question": "who plays peter in what we do in the shadows", "gold_answers": ["Ben Fransham"], "shots": {"0": {"k": 0, "answer": "Matt Berry", "latency_ms": 152.58526802062988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 191.15185737609863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 634}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 331.56728744506836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1276}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 263.03791999816895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1909}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 202.04854011535645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2540}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 194.9758529663086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3196}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 244.4479465484619, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3816}}}
{"qid": "7901746249864619718", "question": "when does the 14th season of grey's anatomy come out", "gold_answers": ["September 28 , 2017"], "shots": {"0": {"k": 0, "answer": "2023", "latency_ms": 258.1772804260254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 28, 2017", "latency_ms": 331.4850330352783, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "September 28, 2017", "latency_ms": 354.0017604827881, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "September 28, 2017", "latency_ms": 437.64638900756836, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "September 28, 2017", "latency_ms": 650.5134105682373, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2706}, "5": {"k": 5, "answer": "September 28, 2017", "latency_ms": 581.5978050231934, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3389}, "6": {"k": 6, "answer": "September 28, 2017", "latency_ms": 587.4917507171631, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3976}}}
{"qid": "-896127746191949637", "question": "when was the first hunger games book published", "gold_answers": ["September 14 , 2008", "2008"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 176.35464668273926, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 14, 2008", "latency_ms": 165.51637649536133, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 670}, "2": {"k": 2, "answer": "September 14, 2008", "latency_ms": 569.939374923706, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "September 14, 2008", "latency_ms": 745.795726776123, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1947}, "4": {"k": 4, "answer": "September 14, 2008", "latency_ms": 555.5317401885986, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2599}, "5": {"k": 5, "answer": "September 14, 2008", "latency_ms": 418.2257652282715, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3282}, "6": {"k": 6, "answer": "September 14, 2008", "latency_ms": 431.6399097442627, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3956}}}
{"qid": "8044909700499569711", "question": "how much is the united states in debt to china", "gold_answers": ["$1.06 trillion", "1,200.5"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 79.9860954284668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "10 percent of total U.S. public debt", "latency_ms": 312.5748634338379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "$1.16 trillion", "latency_ms": 354.89964485168457, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1278}, "3": {"k": 3, "answer": "$1.16 trillion", "latency_ms": 139.65272903442383, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1898}, "4": {"k": 4, "answer": "$1.18 trillion", "latency_ms": 326.80773735046387, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2551}, "5": {"k": 5, "answer": "$1.18 trillion", "latency_ms": 160.09831428527832, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3141}, "6": {"k": 6, "answer": "$1.18 trillion", "latency_ms": 188.13037872314453, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3789}}}
{"qid": "-3123234845590594961", "question": "where did the race in grease take place", "gold_answers": ["Thunder Road"], "shots": {"0": {"k": 0, "answer": "1950s San Diego, California", "latency_ms": 489.7501468658447, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Los Angeles River", "latency_ms": 106.45723342895508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "Los Angeles River", "latency_ms": 197.88813591003418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1223}, "3": {"k": 3, "answer": "Los Angeles River", "latency_ms": 96.49085998535156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1871}, "4": {"k": 4, "answer": "Los Angeles River, between the First and", "latency_ms": 246.8585968017578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2557}, "5": {"k": 5, "answer": "Los Angeles River, between the First and", "latency_ms": 268.8729763031006, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3190}, "6": {"k": 6, "answer": "Los Angeles River, between the First and", "latency_ms": 405.5445194244385, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3778}}}
{"qid": "4326992806019599297", "question": "what is the maximum data rate for the 802.11a standard select one", "gold_answers": ["54 Mbit / s"], "shots": {"0": {"k": 0, "answer": "54 Mbps", "latency_ms": 131.93154335021973, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "54 Mbit/s", "latency_ms": 181.87665939331055, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 628}, "2": {"k": 2, "answer": "54 Mbit/s", "latency_ms": 195.73163986206055, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1274}, "3": {"k": 3, "answer": "54 Mbit/s", "latency_ms": 125.67448616027832, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1950}, "4": {"k": 4, "answer": "54 Mbit/s", "latency_ms": 422.81341552734375, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2643}, "5": {"k": 5, "answer": "54 Mbit/s", "latency_ms": 457.0574760437012, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3265}, "6": {"k": 6, "answer": "54 Mbit/s", "latency_ms": 405.61676025390625, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3927}}}
{"qid": "6144099837933283715", "question": "a good that can be used in place of another good", "gold_answers": ["A substitute good", "substitute good"], "shots": {"0": {"k": 0, "answer": "substitute good", "latency_ms": 92.05102920532227, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "substitute good", "latency_ms": 168.2724952697754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 577}, "2": {"k": 2, "answer": "substitute good", "latency_ms": 159.00516510009766, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1178}, "3": {"k": 3, "answer": "substitute good", "latency_ms": 130.46836853027344, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1801}, "4": {"k": 4, "answer": "substitute good", "latency_ms": 123.26550483703613, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2328}, "5": {"k": 5, "answer": "substitute good", "latency_ms": 163.62786293029785, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3033}, "6": {"k": 6, "answer": "substitute good", "latency_ms": 183.59112739562988, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3623}}}
{"qid": "-4621948696356225060", "question": "where in the bible can i find the lord's prayer", "gold_answers": ["the Gospel of Luke", "the Gospel of Matthew"], "shots": {"0": {"k": 0, "answer": "Matthew 6:9-13", "latency_ms": 348.7374782562256, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 83.5723876953125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 562}, "2": {"k": 2, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 321.014404296875, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 1152}, "3": {"k": 3, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 181.51235580444336, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 1813}, "4": {"k": 4, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 304.5387268066406, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 2417}, "5": {"k": 5, "answer": "Matthew 6:9-13 and Luke 11:2-4", "latency_ms": 361.33480072021484, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 2998}, "6": {"k": 6, "answer": "Matthew 6:9-13 and Luke 11:2-4", "latency_ms": 828.3472061157227, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 3544}}}
{"qid": "6941993254369956277", "question": "when was the last time nebraska voted democrat", "gold_answers": ["1964"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 216.0043716430664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1908", "latency_ms": 351.1326313018799, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 695}, "2": {"k": 2, "answer": "1976", "latency_ms": 252.22325325012207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1376}, "3": {"k": 3, "answer": "1976", "latency_ms": 138.6551856994629, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1948}, "4": {"k": 4, "answer": "1976", "latency_ms": 153.63812446594238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2656}, "5": {"k": 5, "answer": "2006", "latency_ms": 241.0910129547119, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3307}, "6": {"k": 6, "answer": "2006", "latency_ms": 146.23475074768066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4009}}}
{"qid": "-8955197065025093046", "question": "who become the ceo of it wipro company in 2016", "gold_answers": ["Abid Ali Neemuchwala"], "shots": {"0": {"k": 0, "answer": "S. Gopalakrishnan", "latency_ms": 112.80369758605957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Abidali Neemuchwala", "latency_ms": 365.5695915222168, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 678}, "2": {"k": 2, "answer": "Abidali Neemuchwala", "latency_ms": 196.32196426391602, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1363}, "3": {"k": 3, "answer": "Abidali Neemuchwala", "latency_ms": 438.3821487426758, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2047}, "4": {"k": 4, "answer": "Abidali Neemuchwala", "latency_ms": 488.6932373046875, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2653}, "5": {"k": 5, "answer": "Abidali Neemuchwala", "latency_ms": 374.7227191925049, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3260}, "6": {"k": 6, "answer": "Abidali Neemuchwala", "latency_ms": 494.8878288269043, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3888}}}
{"qid": "-3010425579521817304", "question": "who does eric end up with in gossip girl", "gold_answers": ["Jenny"], "shots": {"0": {"k": 0, "answer": "Serena", "latency_ms": 339.465856552124, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 104.8431396484375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 610}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 174.8952865600586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1234}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 271.5158462524414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1819}, "4": {"k": 4, "answer": "Jenny", "latency_ms": 152.44674682617188, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2445}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 184.75937843322754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3050}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 368.60156059265137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3673}}}
{"qid": "7264112453286983469", "question": "who plays unis in she's the man", "gold_answers": ["Emily Perkins"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 123.50630760192871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 100.66676139831543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 118.85356903076172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 122.09558486938477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1801}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 208.6467742919922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2369}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 164.80731964111328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2944}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 216.0956859588623, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3531}}}
{"qid": "2627695648792729859", "question": "who played booster in jingle all the way", "gold_answers": ["Curtis Armstrong"], "shots": {"0": {"k": 0, "answer": "James Caan", "latency_ms": 124.01723861694336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.86280250549316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 679}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 248.77190589904785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1342}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 168.5340404510498, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2010}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 418.4150695800781, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2699}, "5": {"k": 5, "answer": "Tim Flattery", "latency_ms": 194.83613967895508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3321}, "6": {"k": 6, "answer": "Tim Flattery", "latency_ms": 212.89825439453125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3954}}}
{"qid": "-9169976182548289414", "question": "who is recognized as the founder of islam", "gold_answers": ["the Islamic prophet Muhammad", "Muhammad"], "shots": {"0": {"k": 0, "answer": "Muhammad", "latency_ms": 83.39118957519531, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Muhammad", "latency_ms": 147.26924896240234, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 606}, "2": {"k": 2, "answer": "Muhammad", "latency_ms": 133.62836837768555, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1211}, "3": {"k": 3, "answer": "Muhammad", "latency_ms": 134.60707664489746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1837}, "4": {"k": 4, "answer": "Muhammad", "latency_ms": 198.43578338623047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2432}, "5": {"k": 5, "answer": "Muhammad", "latency_ms": 237.4265193939209, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3070}, "6": {"k": 6, "answer": "Muhammad", "latency_ms": 125.48232078552246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3674}}}
{"qid": "-6696747560626271522", "question": "who plays emma in air bud world pup", "gold_answers": ["Brittany Paige Bouck"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 127.48432159423828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 284.25073623657227, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 114.58039283752441, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 239.60471153259277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1838}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 274.66535568237305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2511}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 368.1323528289795, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3092}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 306.78296089172363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3742}}}
{"qid": "5871681964137793716", "question": "how old was sasuke when his clan died", "gold_answers": ["seven"], "shots": {"0": {"k": 0, "answer": "12 years old", "latency_ms": 92.44012832641602, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 315.8755302429199, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 594}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 213.92297744750977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1242}, "3": {"k": 3, "answer": "7", "latency_ms": 158.11777114868164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1885}, "4": {"k": 4, "answer": "7 years old", "latency_ms": 221.05121612548828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2487}, "5": {"k": 5, "answer": "7", "latency_ms": 129.26840782165527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3102}, "6": {"k": 6, "answer": "7", "latency_ms": 243.91913414001465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3730}}}
{"qid": "8552176802923865025", "question": "where do some of the natural air pollutants come from", "gold_answers": ["wildfires", "Dust", "Vegetation", "Carbon dioxide", "Volcanic activity"], "shots": {"0": {"k": 0, "answer": "Volcanoes, wildfires, and natural decay processes", "latency_ms": 198.99868965148926, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "natural sources", "latency_ms": 55.10902404785156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 718}, "2": {"k": 2, "answer": "dust storms", "latency_ms": 149.7788429260254, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1335}, "3": {"k": 3, "answer": "volcanic eruptions, dust storms", "latency_ms": 373.3851909637451, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1994}, "4": {"k": 4, "answer": "natural sources, such as dust storms", "latency_ms": 388.25345039367676, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 2656}, "5": {"k": 5, "answer": "natural sources such as dust storms, volcanic eruptions, sea spray, volcanic ash, and meteoroids", "latency_ms": 939.7275447845459, "em": 0.0, "f1": 0.13333333333333333, "containment": 1.0, "context_chars": 3341}, "6": {"k": 6, "answer": "dust storms, volcanic eruptions, natural sources like dust, pollen, sea spray, volcanic ash, meteoroids", "latency_ms": 689.1541481018066, "em": 0.0, "f1": 0.13333333333333333, "containment": 1.0, "context_chars": 4027}}}
{"qid": "-2630525064809360052", "question": "who is the pastor of new birth missionary baptist church now", "gold_answers": ["Stephen A. Davis"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 80.79767227172852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 445.0664520263672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 635}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 178.1938076019287, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1266}, "3": {"k": 3, "answer": "Jamal Bryant", "latency_ms": 160.76946258544922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1944}, "4": {"k": 4, "answer": "Jamal Bryant", "latency_ms": 199.0218162536621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2621}, "5": {"k": 5, "answer": "Jamal Bryant", "latency_ms": 291.2733554840088, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3286}, "6": {"k": 6, "answer": "Jamal Bryant", "latency_ms": 169.57330703735352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3926}}}
{"qid": "6434931911289860123", "question": "who is doing 2018 super bowl half time show", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Beyoncé", "latency_ms": 299.823522567749, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 80.3685188293457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 686}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 159.49511528015137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1311}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 155.5943489074707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1907}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 262.9437446594238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2581}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 392.60220527648926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3151}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 123.82793426513672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3756}}}
{"qid": "8869318258439148973", "question": "who is playing the halftime show for the superbowl", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Dr. Dre and Snoop Dogg", "latency_ms": 293.7450408935547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Maroon 5", "latency_ms": 85.88814735412598, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "Maroon 5 with Cardi B and Travis Scott", "latency_ms": 285.7651710510254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1302}, "3": {"k": 3, "answer": "Maroon 5 with Cardi B and Travis Scott", "latency_ms": 263.66114616394043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1954}, "4": {"k": 4, "answer": "Maroon 5", "latency_ms": 196.19369506835938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2587}, "5": {"k": 5, "answer": "Maroon 5", "latency_ms": 257.9793930053711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3223}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 237.00666427612305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3864}}}
{"qid": "-8359866474144720253", "question": "who hosted they think it's all over", "gold_answers": ["Des Lynam", "Lee Mack", "comedian Nick Hancock", "Nick Hancock"], "shots": {"0": {"k": 0, "answer": "Randy Jackson", "latency_ms": 167.95086860656738, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lee Mack", "latency_ms": 66.45774841308594, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 678}, "2": {"k": 2, "answer": "Lee Mack", "latency_ms": 172.04737663269043, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1279}, "3": {"k": 3, "answer": "Des Lynam", "latency_ms": 118.19100379943848, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1908}, "4": {"k": 4, "answer": "Nick Hancock", "latency_ms": 124.9399185180664, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2542}, "5": {"k": 5, "answer": "Nick Hancock", "latency_ms": 304.9049377441406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3181}, "6": {"k": 6, "answer": "Nick Hancock", "latency_ms": 101.48954391479492, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3827}}}
{"qid": "-1206653570097564556", "question": "where does the movie proof of life take place", "gold_answers": ["The Republic of Tecala"], "shots": {"0": {"k": 0, "answer": "India", "latency_ms": 39.275407791137695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ecuador", "latency_ms": 73.34733009338379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 640}, "2": {"k": 2, "answer": "Republic of Tecala", "latency_ms": 198.03357124328613, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1268}, "3": {"k": 3, "answer": "Republic of Tecala (fictional South American country)", "latency_ms": 516.9222354888916, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 1872}, "4": {"k": 4, "answer": "Republic of Tecala (fictional South American country), primarily set in Ecuador", "latency_ms": 695.298433303833, "em": 0.0, "f1": 0.42857142857142855, "containment": 0.0, "context_chars": 2523}, "5": {"k": 5, "answer": "Tecala", "latency_ms": 145.35832405090332, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3121}, "6": {"k": 6, "answer": "Tecala", "latency_ms": 171.51355743408203, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3756}}}
{"qid": "6144521615402262404", "question": "where does us highway 1 start and end", "gold_answers": ["Fort Kent , Maine", "to Key West , Florida", "Key West , Florida"], "shots": {"0": {"k": 0, "answer": "Start: U.S. Coast, California; End: U.S. Coast, Florida", "latency_ms": 732.7356338500977, "em": 0.0, "f1": 0.18181818181818182, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 214.50257301330566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "Starts in St. Augustine, Florida; ends in Jacksonville, Florida.", "latency_ms": 457.89384841918945, "em": 0.0, "f1": 0.16666666666666663, "containment": 0.0, "context_chars": 1273}, "3": {"k": 3, "answer": "Starts in St. Augustine, Florida; ends in Jacksonville, Florida.", "latency_ms": 283.4169864654541, "em": 0.0, "f1": 0.16666666666666663, "containment": 0.0, "context_chars": 1934}, "4": {"k": 4, "answer": "Starts in South Carolina, ends in Maine", "latency_ms": 481.33397102355957, "em": 0.0, "f1": 0.2, "containment": 0.0, "context_chars": 2548}, "5": {"k": 5, "answer": "Starts in South Carolina, ends in Florida", "latency_ms": 259.6473693847656, "em": 0.0, "f1": 0.2, "containment": 0.0, "context_chars": 3142}, "6": {"k": 6, "answer": "Starts in South Carolina, ends in Maine", "latency_ms": 497.10750579833984, "em": 0.0, "f1": 0.2, "containment": 0.0, "context_chars": 3751}}}
{"qid": "-8631190913794477985", "question": "who scored the most points in a single game in the nba", "gold_answers": ["Wilt Chamberlain"], "shots": {"0": {"k": 0, "answer": "Wilt Chamberlain", "latency_ms": 233.02483558654785, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wilt Chamberlain", "latency_ms": 90.76857566833496, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 664}, "2": {"k": 2, "answer": "Wilt Chamberlain", "latency_ms": 334.17606353759766, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1274}, "3": {"k": 3, "answer": "Wilt Chamberlain", "latency_ms": 203.0167579650879, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1896}, "4": {"k": 4, "answer": "Wilt Chamberlain", "latency_ms": 175.42552947998047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2505}, "5": {"k": 5, "answer": "Wilt Chamberlain", "latency_ms": 343.70899200439453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3102}, "6": {"k": 6, "answer": "Wilt Chamberlain", "latency_ms": 272.01342582702637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3741}}}
{"qid": "199896377614951764", "question": "where is natural gas found in the us", "gold_answers": ["Texas", "Pennsylvania", "Colorado", "New Mexico", "West Virginia", "Arkansas", "Wyoming", "Louisiana"], "shots": {"0": {"k": 0, "answer": "Texas, North Dakota, Wyoming, Utah, Pennsylvania", "latency_ms": 319.8087215423584, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "shale formations", "latency_ms": 193.07565689086914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "shale formations", "latency_ms": 209.05518531799316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1253}, "3": {"k": 3, "answer": "shale formations", "latency_ms": 307.60836601257324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1895}, "4": {"k": 4, "answer": "trapped within shale formations", "latency_ms": 165.5895709991455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2490}, "5": {"k": 5, "answer": "Ohio, Pennsylvania, West Virginia, New York, Kentucky, and the Marcellus and Utica shales", "latency_ms": 1166.1837100982666, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 3149}, "6": {"k": 6, "answer": "Barnett Shale (Texas), Antrim Shale (Michigan), Marcellus Shale (West Virginia, Pennsylvania, New York), Devonian black shales (Kentucky to New York), Utica Shale (Quebec)", "latency_ms": 921.0925102233887, "em": 0.0, "f1": 0.16, "containment": 1.0, "context_chars": 3765}}}
{"qid": "8986775680996674901", "question": "when were the first general elections held in independent india", "gold_answers": ["1951 -- 52"], "shots": {"0": {"k": 0, "answer": "15 August 1947", "latency_ms": 218.60694885253906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1951–52", "latency_ms": 304.25119400024414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 663}, "2": {"k": 2, "answer": "25 October 1951 to 27 March 1952", "latency_ms": 513.2315158843994, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 1338}, "3": {"k": 3, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 411.3273620605469, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 2001}, "4": {"k": 4, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 548.5646724700928, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 2649}, "5": {"k": 5, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 647.39990234375, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 3283}, "6": {"k": 6, "answer": "25 October 1951 to 21 February 1952", "latency_ms": 924.8464107513428, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 3944}}}
{"qid": "700604097171850168", "question": "what was the tower of london originally used for", "gold_answers": ["as a royal residence", "a royal residence"], "shots": {"0": {"k": 0, "answer": "Royal palace and fortress", "latency_ms": 128.75127792358398, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "royal residence", "latency_ms": 153.21707725524902, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "royal residence", "latency_ms": 137.01510429382324, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1176}, "3": {"k": 3, "answer": "royal residence", "latency_ms": 160.87651252746582, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1804}, "4": {"k": 4, "answer": "royal residence", "latency_ms": 122.27344512939453, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2399}, "5": {"k": 5, "answer": "royal residence", "latency_ms": 199.2964744567871, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3055}, "6": {"k": 6, "answer": "royal residence", "latency_ms": 175.2328872680664, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3701}}}
{"qid": "6745303307988470742", "question": "what type of fuel goes in a zippo", "gold_answers": ["butane", "lighter fluid"], "shots": {"0": {"k": 0, "answer": "Zippo light fluid", "latency_ms": 80.62243461608887, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 83.71853828430176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon (commonly referred to as lighter fluid or naphtha)", "latency_ms": 792.7122116088867, "em": 0.0, "f1": 0.2352941176470588, "containment": 1.0, "context_chars": 1227}, "3": {"k": 3, "answer": "lighter fluid or naphtha", "latency_ms": 339.1451835632324, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1883}, "4": {"k": 4, "answer": "lighter fluid or naphtha", "latency_ms": 151.8874168395996, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2476}, "5": {"k": 5, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon (commonly referred to as lighter fluid or naphtha)", "latency_ms": 893.89967918396, "em": 0.0, "f1": 0.2352941176470588, "containment": 1.0, "context_chars": 3114}, "6": {"k": 6, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon (commonly referred to as lighter fluid or naphtha)", "latency_ms": 680.419921875, "em": 0.0, "f1": 0.2352941176470588, "containment": 1.0, "context_chars": 3760}}}
{"qid": "-2060506905455252030", "question": "first jnanpith award was an autor of which language", "gold_answers": ["Malayalam"], "shots": {"0": {"k": 0, "answer": "Kannada", "latency_ms": 177.93965339660645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Bengali", "latency_ms": 189.81218338012695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 669}, "2": {"k": 2, "answer": "Bengali", "latency_ms": 150.45690536499023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1332}, "3": {"k": 3, "answer": "Bengali", "latency_ms": 181.66017532348633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1972}, "4": {"k": 4, "answer": "Bengali", "latency_ms": 156.47530555725098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2617}, "5": {"k": 5, "answer": "Bengali", "latency_ms": 146.56734466552734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3293}, "6": {"k": 6, "answer": "Bengali", "latency_ms": 328.3674716949463, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4018}}}
{"qid": "4738663028303377024", "question": "who played the mad hatter in the batman tv show", "gold_answers": ["Roddy McDowall", "David Wayne", "Benedict Samuel"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 247.40028381347656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 131.07609748840332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 641}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 335.8011245727539, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1279}, "3": {"k": 3, "answer": "David Wayne", "latency_ms": 85.55793762207031, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1943}, "4": {"k": 4, "answer": "David Wayne", "latency_ms": 181.09726905822754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2598}, "5": {"k": 5, "answer": "David Wayne", "latency_ms": 137.30454444885254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3250}, "6": {"k": 6, "answer": "David Wayne", "latency_ms": 182.2953224182129, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3905}}}
{"qid": "-1650946015201779846", "question": "how many cracker barrels in the united states", "gold_answers": ["639", "more than 600"], "shots": {"0": {"k": 0, "answer": "over 1,400", "latency_ms": 114.22443389892578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "645", "latency_ms": 68.41874122619629, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "645", "latency_ms": 198.79555702209473, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1280}, "3": {"k": 3, "answer": "645", "latency_ms": 175.73952674865723, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1964}, "4": {"k": 4, "answer": "645", "latency_ms": 119.05908584594727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2610}, "5": {"k": 5, "answer": "645", "latency_ms": 115.4775619506836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3282}, "6": {"k": 6, "answer": "645", "latency_ms": 302.45256423950195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3899}}}
{"qid": "-3422350952909582054", "question": "in photosynthesis the carbon in co2 is initially fixed to what molecule", "gold_answers": ["3 - phosphoglycerate"], "shots": {"0": {"k": 0, "answer": "3-phosphoglycerate (3-PGA)", "latency_ms": 274.33204650878906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 593.451738357544, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 727}, "2": {"k": 2, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 181.91218376159668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1441}, "3": {"k": 3, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 262.14051246643066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2203}, "4": {"k": 4, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 599.5409488677979, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2882}, "5": {"k": 5, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 426.84054374694824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3587}, "6": {"k": 6, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 391.68405532836914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4249}}}
{"qid": "-2704708103393811482", "question": "who voices randy in f is for family", "gold_answers": ["T.J. Miller"], "shots": {"0": {"k": 0, "answer": "John DiMaggio", "latency_ms": 79.15282249450684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Bill Burr", "latency_ms": 87.01610565185547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 582}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 103.26337814331055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1254}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 174.99065399169922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1872}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 232.63311386108398, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2535}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 211.4424705505371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3138}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 262.392520904541, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3785}}}
{"qid": "5482012214308896475", "question": "how many countries does cadbury sell its products", "gold_answers": ["more than 50", "more than 50 countries worldwide"], "shots": {"0": {"k": 0, "answer": "Over 150 countries", "latency_ms": 96.25935554504395, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "more than fifty", "latency_ms": 181.624174118042, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 702}, "2": {"k": 2, "answer": "more than fifty", "latency_ms": 102.98609733581543, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1371}, "3": {"k": 3, "answer": "more than fifty", "latency_ms": 337.2933864593506, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2079}, "4": {"k": 4, "answer": "more than fifty", "latency_ms": 417.88697242736816, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2750}, "5": {"k": 5, "answer": "more than fifty", "latency_ms": 265.37466049194336, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3449}, "6": {"k": 6, "answer": "more than fifty", "latency_ms": 229.01439666748047, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 4113}}}
{"qid": "-5737509619250561344", "question": "ethiopia flight 961 crashes in to the sea", "gold_answers": ["23 November 1996"], "shots": {"0": {"k": 0, "answer": "1996", "latency_ms": 212.10241317749023, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indian Ocean", "latency_ms": 155.49063682556152, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "Indian Ocean near Grande Comore, Comoros Islands", "latency_ms": 434.8595142364502, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1353}, "3": {"k": 3, "answer": "Indian Ocean near Grande Comore, Comoros Islands", "latency_ms": 416.672945022583, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2001}, "4": {"k": 4, "answer": "Indian Ocean near Grande Comore, Comoros Islands", "latency_ms": 577.9531002044678, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2664}, "5": {"k": 5, "answer": "no", "latency_ms": 283.40601921081543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3325}, "6": {"k": 6, "answer": "no", "latency_ms": 107.63263702392578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3991}}}
{"qid": "-7175456993713717070", "question": "what does istj mean in a personality test", "gold_answers": ["Extroverted Thinking ( Te )", "Extroverted Intuition ( Ne )", "Introverted Feeling ( Fi )", "Introverted Sensing ( Si )"], "shots": {"0": {"k": 0, "answer": "Introverted, Sensing, Thinking, Judging", "latency_ms": 375.5984306335449, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "ISTJ is a personality type in the Myers-Briggs Type Indicator.", "latency_ms": 569.6568489074707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 724}, "2": {"k": 2, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 505.6724548339844, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1385}, "3": {"k": 3, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 205.57451248168945, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2101}, "4": {"k": 4, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 456.22801780700684, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2776}, "5": {"k": 5, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 429.96811866760254, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3504}, "6": {"k": 6, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 260.6008052825928, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 4132}}}
{"qid": "4437170343606050664", "question": "who does the voice of ash in sing", "gold_answers": ["Scarlett Johansson"], "shots": {"0": {"k": 0, "answer": "John Mulaney", "latency_ms": 111.4201545715332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Veronica Taylor", "latency_ms": 151.52525901794434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 618}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 249.20010566711426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1282}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 280.58910369873047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1893}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 276.43561363220215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2528}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 165.0094985961914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3152}, "6": {"k": 6, "answer": "Scarlett Johansson", "latency_ms": 313.9910697937012, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3768}}}
{"qid": "-8773010039077552394", "question": "who scored the most points in their nba career", "gold_answers": ["Kareem Abdul - Jabbar"], "shots": {"0": {"k": 0, "answer": "Kareem Abdul-Jabbar", "latency_ms": 330.62171936035156, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 391.4318084716797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 535}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 425.05478858947754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1127}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 226.81117057800293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1680}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 183.43234062194824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2273}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 285.38966178894043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2857}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 417.1106815338135, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3414}}}
{"qid": "-3278224144622911032", "question": "with a land area of 54 314 square miles where does wisconsin rank among the 50 states", "gold_answers": ["25"], "shots": {"0": {"k": 0, "answer": "10th largest", "latency_ms": 262.9425525665283, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 100.23355484008789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 238.1134033203125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 321.8197822570801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1809}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 227.56457328796387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2388}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 371.8717098236084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3028}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 328.0506134033203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3633}}}
{"qid": "-3466298422174960041", "question": "who plays charles on have and have nots", "gold_answers": ["Nick Sager", "Nick Sagar"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 108.3066463470459, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 190.8414363861084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 650}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 218.0941104888916, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1280}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 108.76941680908203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1924}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 136.3229751586914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2547}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 175.50015449523926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3129}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 150.41542053222656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3779}}}
{"qid": "1803248910342766083", "question": "who wrote the book my country my life", "gold_answers": ["L.K. Advani"], "shots": {"0": {"k": 0, "answer": "Nelson Mandela", "latency_ms": 200.42753219604492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "L. K. Advani", "latency_ms": 228.2540798187256, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "L. K. Advani", "latency_ms": 200.64973831176758, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1158}, "3": {"k": 3, "answer": "L. K. Advani", "latency_ms": 317.9631233215332, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1767}, "4": {"k": 4, "answer": "L. K. Advani", "latency_ms": 268.3119773864746, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2408}, "5": {"k": 5, "answer": "L. K. Advani", "latency_ms": 154.04701232910156, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3040}, "6": {"k": 6, "answer": "L. K. Advani", "latency_ms": 318.8934326171875, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3673}}}
{"qid": "-1176280233636154422", "question": "when did the east india company take control of india", "gold_answers": ["1757", "1799", "1612", "in 1757"], "shots": {"0": {"k": 0, "answer": "1757", "latency_ms": 185.2738857269287, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "18th century", "latency_ms": 275.191068649292, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "1757", "latency_ms": 169.96502876281738, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1295}, "3": {"k": 3, "answer": "1757", "latency_ms": 113.09385299682617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1955}, "4": {"k": 4, "answer": "1757", "latency_ms": 431.0433864593506, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2657}, "5": {"k": 5, "answer": "1757", "latency_ms": 152.90164947509766, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3305}, "6": {"k": 6, "answer": "1757", "latency_ms": 168.6382293701172, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3960}}}
{"qid": "5911119091653953181", "question": "where does the water come from to fill lake eyre", "gold_answers": ["Warburton River"], "shots": {"0": {"k": 0, "answer": "Rainfall and inflows from surrounding rivers and groundwater.", "latency_ms": 567.6021575927734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lake Eyre basin rivers (Cooper Creek, Finke River, Georgina River, Diamantina River) and other desert rivers (Hale River, Plenty River, Todd)", "latency_ms": 1575.730323791504, "em": 0.0, "f1": 0.08695652173913042, "containment": 0.0, "context_chars": 624}, "2": {"k": 2, "answer": "Georgina River", "latency_ms": 212.64910697937012, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1212}, "3": {"k": 3, "answer": "Georgina River", "latency_ms": 279.89745140075684, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1823}, "4": {"k": 4, "answer": "Cooper Creek, Georgina River, Finke River, Diamantina River", "latency_ms": 447.49927520751953, "em": 0.0, "f1": 0.2, "containment": 0.0, "context_chars": 2441}, "5": {"k": 5, "answer": "Cooper Creek, Georgina River, Channel Country of Queensland, Finke River via Macumba River", "latency_ms": 624.1991519927979, "em": 0.0, "f1": 0.13333333333333336, "containment": 0.0, "context_chars": 3009}, "6": {"k": 6, "answer": "Cooper Creek, Georgina River, Finke River, Macumba River, Channel Country of Queensland", "latency_ms": 829.054594039917, "em": 0.0, "f1": 0.14285714285714285, "containment": 0.0, "context_chars": 3622}}}
{"qid": "5557104327881436095", "question": "who is the founder of google and when was it founded", "gold_answers": ["1998", "Sergey Brin", "Larry Page and Sergey Brin", "Larry Page"], "shots": {"0": {"k": 0, "answer": "Larry Page and Sergey Brin; 1998", "latency_ms": 229.36701774597168, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Larry Page and Sergey Brin; September 15, 1997", "latency_ms": 333.24265480041504, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 676}, "2": {"k": 2, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 521.0287570953369, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 1356}, "3": {"k": 3, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 460.41297912597656, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 2015}, "4": {"k": 4, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 239.46070671081543, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 2669}, "5": {"k": 5, "answer": "Larry Page and Sergey Brin in 1998", "latency_ms": 312.99829483032227, "em": 0.0, "f1": 0.8333333333333333, "containment": 1.0, "context_chars": 3320}, "6": {"k": 6, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 371.26731872558594, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 4002}}}
{"qid": "-1333328812971005142", "question": "who did the dominican republic gain its independence from", "gold_answers": ["Haiti"], "shots": {"0": {"k": 0, "answer": "Spain", "latency_ms": 97.25356101989746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Haiti", "latency_ms": 79.37979698181152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 673}, "2": {"k": 2, "answer": "Haiti", "latency_ms": 268.4962749481201, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1400}, "3": {"k": 3, "answer": "Haiti", "latency_ms": 123.06475639343262, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2069}, "4": {"k": 4, "answer": "Haiti", "latency_ms": 141.4642333984375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2749}, "5": {"k": 5, "answer": "Haiti", "latency_ms": 146.57211303710938, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3396}, "6": {"k": 6, "answer": "Haiti", "latency_ms": 244.16112899780273, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4091}}}
{"qid": "1122605495295045742", "question": "how did early humans make use of stones during the prehistoric period", "gold_answers": ["start fires", "hunt", "bury their dead"], "shots": {"0": {"k": 0, "answer": "Chipped and shaped for tools", "latency_ms": 114.06278610229492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "made stone tools and used jewellery", "latency_ms": 176.05090141296387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 663}, "2": {"k": 2, "answer": "made stone tools and used jewellery", "latency_ms": 409.0399742126465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1310}, "3": {"k": 3, "answer": "made stone tools and used jewellery", "latency_ms": 131.79492950439453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1995}, "4": {"k": 4, "answer": "made stone tools, used polished stone tools, and created stone circles with cosmological alignments", "latency_ms": 525.1460075378418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2649}, "5": {"k": 5, "answer": "Struck with a hammerstone to produce sharp edges for tools like choppers or scrapers", "latency_ms": 773.1974124908447, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3287}, "6": {"k": 6, "answer": "Struck with a hammerstone to produce sharp edges for tools like choppers or scrapers", "latency_ms": 529.3359756469727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3913}}}
{"qid": "7402184777947527945", "question": "who wrote the text for jeanie with the light brown hair", "gold_answers": ["Stephen Foster"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 345.95298767089844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Stephen Foster", "latency_ms": 98.25015068054199, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 632}, "2": {"k": 2, "answer": "Stephen Foster", "latency_ms": 70.71304321289062, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1222}, "3": {"k": 3, "answer": "Stephen Foster", "latency_ms": 106.25290870666504, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1839}, "4": {"k": 4, "answer": "Stephen Foster", "latency_ms": 122.92003631591797, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2458}, "5": {"k": 5, "answer": "Stephen Foster", "latency_ms": 232.2518825531006, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3103}, "6": {"k": 6, "answer": "Stephen Foster", "latency_ms": 204.47111129760742, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3752}}}
{"qid": "1656745334095178455", "question": "the basic steps of rational decision making include defining goals and", "gold_answers": ["Implementation of plans / policies"], "shots": {"0": {"k": 0, "answer": "identifying alternatives", "latency_ms": 71.48504257202148, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "defining goals and verifying the problem", "latency_ms": 358.5822582244873, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 774}, "2": {"k": 2, "answer": "defining goals and problem identification", "latency_ms": 293.9426898956299, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1428}, "3": {"k": 3, "answer": "defining goals and problem identification", "latency_ms": 180.07755279541016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2146}, "4": {"k": 4, "answer": "defining the problem, gathering information, generating alternatives, evaluating alternatives, choosing the best solution, implementing the solution, and monitoring outcomes.", "latency_ms": 955.36208152771, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2779}, "5": {"k": 5, "answer": "defining goals and information gathering", "latency_ms": 257.7705383300781, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3431}, "6": {"k": 6, "answer": "defining goals and information gathering", "latency_ms": 207.1073055267334, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4056}}}
{"qid": "-216657635823030751", "question": "when was the design period known as art deco", "gold_answers": ["1910 -- 1939"], "shots": {"0": {"k": 0, "answer": "1920s–1930s", "latency_ms": 171.28419876098633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1925", "latency_ms": 118.97635459899902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "1910 to 1914", "latency_ms": 433.2389831542969, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1299}, "3": {"k": 3, "answer": "1925", "latency_ms": 108.68453979492188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1934}, "4": {"k": 4, "answer": "1910–1925", "latency_ms": 449.631929397583, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2616}, "5": {"k": 5, "answer": "1910–1914, with the name coined in 1925", "latency_ms": 655.1554203033447, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3249}, "6": {"k": 6, "answer": "1910 to 1925", "latency_ms": 580.6393623352051, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3931}}}
{"qid": "250542332339248886", "question": "where does the bob and tom show broadcast from", "gold_answers": ["WFBQ in Indianapolis , Indiana", "Indianapolis , Indiana"], "shots": {"0": {"k": 0, "answer": "Winston-Salem, North Carolina", "latency_ms": 325.1628875732422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "WFBQ, 94.7, Indianapolis", "latency_ms": 705.4135799407959, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 583}, "2": {"k": 2, "answer": "WFBQ, 94.7, Indianapolis", "latency_ms": 452.4273872375488, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1229}, "3": {"k": 3, "answer": "WFBQ, 94.7, Indianapolis", "latency_ms": 566.1814212799072, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 434.68642234802246, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2506}, "5": {"k": 5, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 187.760591506958, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3099}, "6": {"k": 6, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 589.6360874176025, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3730}}}
{"qid": "7610758795954724809", "question": "star wars the clone wars season 3 episode 1", "gold_answers": ["Clone Cadets"], "shots": {"0": {"k": 0, "answer": "The Clone Wars: Season 3, Episode 1 is titled \"The Clone Wars: Season 3, Episode 1\" and is known as \"The Clone Wars: Season 3, Episode 1\" in the series.", "latency_ms": 1269.0584659576416, "em": 0.0, "f1": 0.07142857142857144, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 147.1116542816162, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 641}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 113.60859870910645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1338}, "3": {"k": 3, "answer": "Sphere of Influence", "latency_ms": 215.58284759521484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1978}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 158.95605087280273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2658}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 340.7735824584961, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3333}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 258.39805603027344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4014}}}
{"qid": "2997223939033016160", "question": "what part of brain is responsible for complex thinking", "gold_answers": ["the frontal lobe"], "shots": {"0": {"k": 0, "answer": "prefrontal cortex", "latency_ms": 205.5985927581787, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "neocortex", "latency_ms": 247.06530570983887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 711}, "2": {"k": 2, "answer": "neocortex", "latency_ms": 269.6247100830078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1354}, "3": {"k": 3, "answer": "neocortex", "latency_ms": 160.36701202392578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2058}, "4": {"k": 4, "answer": "neocortex", "latency_ms": 331.77781105041504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2688}, "5": {"k": 5, "answer": "neocortex", "latency_ms": 263.9937400817871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3362}, "6": {"k": 6, "answer": "cerebral cortex", "latency_ms": 313.2169246673584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3978}}}
{"qid": "2680196226239522881", "question": "who won the champions league final in 2016", "gold_answers": ["Real Madrid"], "shots": {"0": {"k": 0, "answer": "Real Madrid", "latency_ms": 182.18660354614258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Jeonbuk Hyundai Motors", "latency_ms": 310.6374740600586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 416.14723205566406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1262}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 164.28303718566895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1900}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 228.72233390808105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2526}, "5": {"k": 5, "answer": "Jeonbuk Hyundai Motors", "latency_ms": 271.76356315612793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3141}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 149.5954990386963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3792}}}
{"qid": "-8458006812258546025", "question": "who has the most gold medals in the winter olympics of all time", "gold_answers": ["Norway"], "shots": {"0": {"k": 0, "answer": "Cross-country skier, Norwegian, with multiple golds, but the most total gold medals in Winter Olympics history is held by **Anders Haugland**? No — correct answer is **Knut Hjeltnes**? No.\n\nActually, the correct answer is: **Sven Hannawald**? No.\n\nCorrect answer: **Eddie Eagan**? No — he's Summer Olympics.\n\nCorrect answer: **Knut Hjalmarson**? No.\n\nCorrect answer: **Bjørn Dæhlie** — he has 3 golds in cross-country skiing.\n\nBut the record holder for most Winter Olympic gold medals is **Bjørn Dæhlie** with 3 golds.\n\nWait — actually, **Knut Hjalmarson**? No.\n\nCorrect answer: **Anders Haugland** — no.\n\nActually, the most Winter Olympic gold medals in history is held by **Bjørn Dæhlie** with 3 golds.\n\nBut wait — **Knut Hjalmarson**? No.\n\nActually, the correct answer is: **Bjørn Dæhlie** — 3 gold medals.\n\nBut is that the most?\n\nNo — **Eddie E", "latency_ms": 5766.634225845337, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 257.19356536865234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 267.5027847290039, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1222}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 276.72600746154785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1877}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 315.859317779541, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2482}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 396.1327075958252, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3169}, "6": {"k": 6, "answer": "Ole Einar Bjørndalen", "latency_ms": 297.06454277038574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3814}}}
