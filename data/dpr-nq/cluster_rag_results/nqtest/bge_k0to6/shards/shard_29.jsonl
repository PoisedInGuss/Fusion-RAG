{"qid": "3012568508229512715", "question": "when does like cage season 2 come out", "gold_answers": ["June 22 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.03991508483887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "June 21, 2018", "latency_ms": 156.76164627075195, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "June 22, 2018", "latency_ms": 163.4061336517334, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1266}, "3": {"k": 3, "answer": "June 22, 2018", "latency_ms": 176.57208442687988, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1948}, "4": {"k": 4, "answer": "June 22, 2018", "latency_ms": 179.7471046447754, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2582}, "5": {"k": 5, "answer": "June 22, 2018", "latency_ms": 195.80316543579102, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3233}, "6": {"k": 6, "answer": "June 22, 2018", "latency_ms": 209.1989517211914, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3935}}}
{"qid": "839669511198247322", "question": "types of skiing in the winter olympics 2018", "gold_answers": ["Super-G", "Slalom -- ( SC )", "Downhill", "Giant slalom", "Slalom"], "shots": {"0": {"k": 0, "answer": "Alpine, Freestyle, Cross-country, Ski jumping, Nordic combined", "latency_ms": 193.9384937286377, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Alpine skiing", "latency_ms": 70.42670249938965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "Alpine skiing, mixed team event", "latency_ms": 127.79045104980469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Alpine skiing, mixed team event", "latency_ms": 138.80181312561035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1937}, "4": {"k": 4, "answer": "Alpine skiing, cross-country skiing", "latency_ms": 146.02208137512207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2604}, "5": {"k": 5, "answer": "Alpine skiing, cross-country skiing, ski jumping", "latency_ms": 196.7940330505371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3246}, "6": {"k": 6, "answer": "Alpine, cross-country, ski jumping, freestyle", "latency_ms": 222.78857231140137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3901}}}
{"qid": "-5881961149303861412", "question": "who played gino in a place to call home", "gold_answers": ["Aldo Mignone"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.22278213500977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 85.57510375976562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 90.85893630981445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1311}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 101.96518898010254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1949}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 121.43683433532715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2572}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 128.91125679016113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3248}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 141.49117469787598, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3944}}}
{"qid": "-9077495306870424234", "question": "who did the 49ers beat in the super bowls", "gold_answers": ["San Diego Chargers", "Miami Dolphins", "Denver Broncos", "Cincinnati Bengals"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 76.62773132324219, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "New York Giants", "latency_ms": 71.38657569885254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 552}, "2": {"k": 2, "answer": "New York Giants in Super Bowl XIX", "latency_ms": 143.0032253265381, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1199}, "3": {"k": 3, "answer": "Baltimore Ravens", "latency_ms": 90.62552452087402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1882}, "4": {"k": 4, "answer": "New York Giants in Super Bowl XIX", "latency_ms": 159.85798835754395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2523}, "5": {"k": 5, "answer": "Miami Dolphins", "latency_ms": 98.46115112304688, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3172}, "6": {"k": 6, "answer": "Miami Dolphins", "latency_ms": 114.16172981262207, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3776}}}
{"qid": "-5623229691925051533", "question": "when was the south asian association for regional co-operation (saarc) formed", "gold_answers": ["December 1985", "8 December 1985"], "shots": {"0": {"k": 0, "answer": "1985", "latency_ms": 75.6833553314209, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "8 December 1985", "latency_ms": 119.95387077331543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 727}, "2": {"k": 2, "answer": "8 December 1985", "latency_ms": 130.01251220703125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1447}, "3": {"k": 3, "answer": "8 December 1985", "latency_ms": 143.01347732543945, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2144}, "4": {"k": 4, "answer": "8 December 1985", "latency_ms": 143.62525939941406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2861}, "5": {"k": 5, "answer": "8 December 1985", "latency_ms": 158.73289108276367, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3557}, "6": {"k": 6, "answer": "8 December 1985", "latency_ms": 171.4954376220703, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4315}}}
{"qid": "8707879945343660137", "question": "what hangs from the trees in the bayou", "gold_answers": ["Spanish moss"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 75.44994354248047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "moss-covered cypress trees", "latency_ms": 107.03444480895996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 635}, "2": {"k": 2, "answer": "moss-covered cypress trees", "latency_ms": 112.37168312072754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1343}, "3": {"k": 3, "answer": "moss-covered cypress trees", "latency_ms": 127.19011306762695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1987}, "4": {"k": 4, "answer": "moss-covered cypress trees", "latency_ms": 135.4055404663086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2635}, "5": {"k": 5, "answer": "cypress knees", "latency_ms": 115.26155471801758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3264}, "6": {"k": 6, "answer": "cypress knees", "latency_ms": 128.94439697265625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3949}}}
{"qid": "-731256923033768782", "question": "new york heart association functional classification of angina", "gold_answers": ["II"], "shots": {"0": {"k": 0, "answer": "NYHA functional class", "latency_ms": 76.3394832611084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "NYHA Functional Classification", "latency_ms": 80.91282844543457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 730}, "2": {"k": 2, "answer": "NYHA Functional Classification", "latency_ms": 84.02800559997559, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1494}, "3": {"k": 3, "answer": "NYHA Functional Classification", "latency_ms": 94.40994262695312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2250}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 105.86333274841309, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2959}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 124.37772750854492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3658}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 123.50177764892578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4437}}}
{"qid": "-5350413673278085389", "question": "who wrote the song would i lie to you", "gold_answers": ["Mike Leeson", "Peter Vale"], "shots": {"0": {"k": 0, "answer": "Dolly Parton", "latency_ms": 72.42918014526367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Mike Leeson and Peter Vale", "latency_ms": 105.14616966247559, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 583}, "2": {"k": 2, "answer": "Mike Leeson and Peter Vale", "latency_ms": 113.28840255737305, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1219}, "3": {"k": 3, "answer": "Mike Leeson and Peter Vale", "latency_ms": 127.7611255645752, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1856}, "4": {"k": 4, "answer": "Mike Leeson and Peter Vale", "latency_ms": 133.7108612060547, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2455}, "5": {"k": 5, "answer": "Mike Leeson and Peter Vale", "latency_ms": 149.2166519165039, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3060}, "6": {"k": 6, "answer": "Mike Leeson and Peter Vale", "latency_ms": 162.7047061920166, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3712}}}
{"qid": "-6190173184813176768", "question": "number of films in lord of the rings", "gold_answers": ["three"], "shots": {"0": {"k": 0, "answer": "3", "latency_ms": 34.645795822143555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "3", "latency_ms": 40.14730453491211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 641}, "2": {"k": 2, "answer": "3", "latency_ms": 51.30910873413086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "3", "latency_ms": 65.3679370880127, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1882}, "4": {"k": 4, "answer": "3", "latency_ms": 73.86493682861328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2643}, "5": {"k": 5, "answer": "3", "latency_ms": 82.77201652526855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3299}, "6": {"k": 6, "answer": "3", "latency_ms": 90.15130996704102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3927}}}
{"qid": "8239463385147663167", "question": "is jaffa cakes a biscuit or a cake", "gold_answers": ["cakes", "Cake", "biscuit - sized cakes"], "shots": {"0": {"k": 0, "answer": "biscuit", "latency_ms": 60.246944427490234, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "biscuit-sized cake", "latency_ms": 92.6368236541748, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 604}, "2": {"k": 2, "answer": "biscuit-sized cake", "latency_ms": 104.36344146728516, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1184}, "3": {"k": 3, "answer": "a cake", "latency_ms": 81.1915397644043, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1765}, "4": {"k": 4, "answer": "cake", "latency_ms": 73.00400733947754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2460}, "5": {"k": 5, "answer": "cake", "latency_ms": 86.44223213195801, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3101}, "6": {"k": 6, "answer": "cake", "latency_ms": 88.61875534057617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3681}}}
{"qid": "756385103791298508", "question": "who does the voice of stewie family guy", "gold_answers": ["Seth MacFarlane"], "shots": {"0": {"k": 0, "answer": "Dan Castellaneta", "latency_ms": 76.62034034729004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Jim Reardon", "latency_ms": 63.232421875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 675}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 91.56680107116699, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1399}, "3": {"k": 3, "answer": "Rachael MacFarlane", "latency_ms": 128.56602668762207, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2065}, "4": {"k": 4, "answer": "Seth MacFarlane", "latency_ms": 121.77014350891113, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2738}, "5": {"k": 5, "answer": "Seth MacFarlane", "latency_ms": 139.23311233520508, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3407}, "6": {"k": 6, "answer": "Seth MacFarlane", "latency_ms": 150.44522285461426, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4062}}}
{"qid": "7355457342276623885", "question": "what drug does the mom use in requiem for a dream", "gold_answers": ["weight - loss amphetamines", "amphetamines"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 77.25095748901367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "amphetamines", "latency_ms": 65.98567962646484, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 609}, "2": {"k": 2, "answer": "amphetamines", "latency_ms": 76.25031471252441, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1254}, "3": {"k": 3, "answer": "amphetamines", "latency_ms": 93.18065643310547, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1831}, "4": {"k": 4, "answer": "amphetamines", "latency_ms": 95.83282470703125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2445}, "5": {"k": 5, "answer": "amphetamines", "latency_ms": 109.93337631225586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3085}, "6": {"k": 6, "answer": "amphetamines", "latency_ms": 111.46664619445801, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3708}}}
{"qid": "-251770629620567718", "question": "how long was dark side of the moon number 1", "gold_answers": ["42 : 49", "a week"], "shots": {"0": {"k": 0, "answer": "19 minutes 38 seconds", "latency_ms": 112.28251457214355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "14 years", "latency_ms": 67.59190559387207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 601}, "2": {"k": 2, "answer": "14 years", "latency_ms": 78.094482421875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1214}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 100.66461563110352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1800}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 122.2372055053711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2412}, "5": {"k": 5, "answer": "1 week", "latency_ms": 101.42135620117188, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3023}, "6": {"k": 6, "answer": "1 week", "latency_ms": 112.27536201477051, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3656}}}
{"qid": "3227997019205769806", "question": "where is the snooker world open currently held", "gold_answers": ["Hainan International Exhibition Center", "Haikou"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 75.40225982666016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 85.03317832946777, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 91.30597114562988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1286}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 106.01234436035156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1933}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 120.22542953491211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2562}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.47740745544434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3223}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 138.82780075073242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3836}}}
{"qid": "8499576329103738172", "question": "who is the actor that plays jt on the young and the restless", "gold_answers": ["Thaddeus Rowe Luckinbill"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 75.66499710083008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Thad Luckinbill", "latency_ms": 98.17147254943848, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 623}, "2": {"k": 2, "answer": "Thad Luckinbill", "latency_ms": 102.84590721130371, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Thad Luckinbill", "latency_ms": 115.55886268615723, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1898}, "4": {"k": 4, "answer": "Thad Luckinbill", "latency_ms": 134.43756103515625, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2485}, "5": {"k": 5, "answer": "Thad Luckinbill", "latency_ms": 139.76240158081055, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3103}, "6": {"k": 6, "answer": "Thad Luckinbill", "latency_ms": 152.6169776916504, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3708}}}
{"qid": "4230593664190705298", "question": "when was the first commercial cell phone released", "gold_answers": ["1983", "1973"], "shots": {"0": {"k": 0, "answer": "1983", "latency_ms": 71.35176658630371, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "early 1990s", "latency_ms": 113.555908203125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 666}, "2": {"k": 2, "answer": "1990s", "latency_ms": 99.84469413757324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1359}, "3": {"k": 3, "answer": "1973", "latency_ms": 101.85980796813965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2001}, "4": {"k": 4, "answer": "1983", "latency_ms": 108.0770492553711, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2690}, "5": {"k": 5, "answer": "1983", "latency_ms": 124.33600425720215, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3320}, "6": {"k": 6, "answer": "1983", "latency_ms": 127.18844413757324, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3903}}}
{"qid": "2241516377343726249", "question": "who sings the theme song for degrassi the next generation", "gold_answers": ["The Degrassi Junior Strings", "Jakalope"], "shots": {"0": {"k": 0, "answer": "Lori Michaels", "latency_ms": 63.77696990966797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dave Ogilvie, Anthony Valcic, Jakalope, Damhnait Doyle, in-show band", "latency_ms": 296.7519760131836, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 645}, "2": {"k": 2, "answer": "Dave Ogilvie, Anthony Valcic, Jakalope, Damhnait Doyle, Alexz Johnson", "latency_ms": 314.56899642944336, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "Dave Ogilvie, Anthony Valcic, Damhnait Doyle, Alexz Johnson, Jakalope", "latency_ms": 322.6337432861328, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 1947}, "4": {"k": 4, "answer": "Dave Ogilvie, Anthony Valcic, Damhnait Doyle, Alexz Johnson, Jim McGrath, Shobha", "latency_ms": 377.31480598449707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2598}, "5": {"k": 5, "answer": "Jakalope", "latency_ms": 108.82949829101562, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3209}, "6": {"k": 6, "answer": "Jakalope", "latency_ms": 116.0576343536377, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3935}}}
{"qid": "8179100082271836759", "question": "when did the first ice age come out", "gold_answers": ["March 15 , 2002", "2002"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.71130180358887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.37213706970215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 627}, "2": {"k": 2, "answer": "360 million years ago", "latency_ms": 117.9816722869873, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1276}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 105.10683059692383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1919}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 119.70090866088867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2576}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 125.29873847961426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3192}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 137.74585723876953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3871}}}
{"qid": "-1946900884888632776", "question": "what is a e8 in the marine corps", "gold_answers": ["Master sergeant", "First sergeant"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 77.54182815551758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 76.12800598144531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 732}, "2": {"k": 2, "answer": "E-8 in the Marine Corps is a senior enlisted rank, typically a Sergeant Major, providing technical leadership and serving as the senior enlisted member in a unit.", "latency_ms": 429.6858310699463, "em": 0.0, "f1": 0.08333333333333334, "containment": 0.0, "context_chars": 1425}, "3": {"k": 3, "answer": "E-8 is the rank of Sergeant Major in the Marine Corps.", "latency_ms": 225.30174255371094, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 2087}, "4": {"k": 4, "answer": "E-8 is the rank level of Sergeant Major in the United States Marine Corps.", "latency_ms": 268.91398429870605, "em": 0.0, "f1": 0.14285714285714285, "containment": 0.0, "context_chars": 2737}, "5": {"k": 5, "answer": "E-8 in the Marine Corps is a senior enlisted rank, typically a Sergeant Major, with responsibilities including technical leadership and assisting the commanding officer in discipline, administration, and unit morale.", "latency_ms": 527.5900363922119, "em": 0.0, "f1": 0.07142857142857144, "containment": 0.0, "context_chars": 3440}, "6": {"k": 6, "answer": "E-8 is a pay grade with two ranks: Sergeant Major and First Sergeant, serving as senior enlisted Marines providing leadership in discipline, administration, and morale.", "latency_ms": 471.56548500061035, "em": 0.0, "f1": 0.15384615384615385, "containment": 1.0, "context_chars": 4176}}}
{"qid": "8538836642175821278", "question": "who is given credit for the gnu initiative", "gold_answers": ["Richard Stallman"], "shots": {"0": {"k": 0, "answer": "Richard Stallman", "latency_ms": 59.28778648376465, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Richard Stallman", "latency_ms": 65.67072868347168, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 685}, "2": {"k": 2, "answer": "Richard Stallman", "latency_ms": 80.96766471862793, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1332}, "3": {"k": 3, "answer": "Richard Stallman", "latency_ms": 92.86856651306152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1981}, "4": {"k": 4, "answer": "Richard Stallman", "latency_ms": 96.79841995239258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2594}, "5": {"k": 5, "answer": "Richard Stallman", "latency_ms": 112.31589317321777, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3284}, "6": {"k": 6, "answer": "Richard Stallman", "latency_ms": 112.35547065734863, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3933}}}
{"qid": "-170392909557030937", "question": "who won the 10m air pistol gold medal at commonwealth shooting championship in brisbane australia", "gold_answers": ["Shahzar Rizvi"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 75.95372200012207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 82.69262313842773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 88.3491039276123, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1218}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 103.27410697937012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1828}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 110.96620559692383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2433}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 123.7783432006836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3014}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 139.5559310913086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3641}}}
{"qid": "-7137073055537137289", "question": "who is the lead singer of collective soul", "gold_answers": ["Ed Roland"], "shots": {"0": {"k": 0, "answer": "Chris Layton", "latency_ms": 63.58528137207031, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ed Roland", "latency_ms": 56.214332580566406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 693}, "2": {"k": 2, "answer": "Ed Roland", "latency_ms": 68.9535140991211, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1369}, "3": {"k": 3, "answer": "Ed Roland", "latency_ms": 77.53252983093262, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1997}, "4": {"k": 4, "answer": "Ed Roland", "latency_ms": 82.43417739868164, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2622}, "5": {"k": 5, "answer": "Ed Roland", "latency_ms": 97.09048271179199, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3207}, "6": {"k": 6, "answer": "Ed Roland", "latency_ms": 102.99181938171387, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3813}}}
{"qid": "7198381700174721586", "question": "who does dwyane wade play for right now", "gold_answers": ["the Miami Heat", "Miami Heat"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 76.54356956481934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Miami Heat", "latency_ms": 61.09881401062012, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 619}, "2": {"k": 2, "answer": "Miami Heat", "latency_ms": 65.03605842590332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1310}, "3": {"k": 3, "answer": "Miami Heat", "latency_ms": 78.47118377685547, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1908}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 105.62682151794434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2505}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 123.49987030029297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3072}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 136.854887008667, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3682}}}
{"qid": "-4036207256798544363", "question": "what is the value of the currency in paraguay", "gold_answers": ["126 PYG to 1 USD"], "shots": {"0": {"k": 0, "answer": "Paraguayan guaraní", "latency_ms": 96.89903259277344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "guarani (PYG)", "latency_ms": 122.52974510192871, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 656}, "2": {"k": 2, "answer": "guarani (PYG)", "latency_ms": 132.72476196289062, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1225}, "3": {"k": 3, "answer": "PYG", "latency_ms": 81.8021297454834, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 1823}, "4": {"k": 4, "answer": "guaraní (PYG)", "latency_ms": 160.05945205688477, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2392}, "5": {"k": 5, "answer": "PYG", "latency_ms": 112.38503456115723, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 3030}, "6": {"k": 6, "answer": "PYG", "latency_ms": 115.92578887939453, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 3667}}}
{"qid": "222526478564019881", "question": "a drug that might be used in surgery for its amnesic properties is", "gold_answers": ["benzodiazepines", "Benzodiazepines"], "shots": {"0": {"k": 0, "answer": "ketamine", "latency_ms": 48.19488525390625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "benzodiazepines", "latency_ms": 106.46986961364746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 678}, "2": {"k": 2, "answer": "benzodiazepines", "latency_ms": 118.17240715026855, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1397}, "3": {"k": 3, "answer": "benzodiazepines", "latency_ms": 128.57604026794434, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2036}, "4": {"k": 4, "answer": "benzodiazepines", "latency_ms": 133.67819786071777, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2771}, "5": {"k": 5, "answer": "benzodiazepines", "latency_ms": 145.9505558013916, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3496}, "6": {"k": 6, "answer": "benzodiazepines", "latency_ms": 149.2295265197754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4207}}}
{"qid": "-9216522514171777028", "question": "a single period of precession of earth's axis is completed in about", "gold_answers": ["26,000 years", "approximately 26,000 years"], "shots": {"0": {"k": 0, "answer": "26,000 years", "latency_ms": 108.88791084289551, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "26,000 years", "latency_ms": 122.21217155456543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 626}, "2": {"k": 2, "answer": "26,000 years", "latency_ms": 130.81789016723633, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1249}, "3": {"k": 3, "answer": "26,000 years", "latency_ms": 138.31305503845215, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1840}, "4": {"k": 4, "answer": "26,000 years", "latency_ms": 143.70083808898926, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2468}, "5": {"k": 5, "answer": "26,000 years", "latency_ms": 162.05811500549316, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3095}, "6": {"k": 6, "answer": "26,000 years", "latency_ms": 161.47804260253906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3720}}}
{"qid": "8838750614698973059", "question": "who did johnny manziel play college football for", "gold_answers": ["Texas A&M University", "Texas A&M Aggies", "Texas A&M"], "shots": {"0": {"k": 0, "answer": "Texas A&M", "latency_ms": 59.92269515991211, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Texas A&M University", "latency_ms": 75.39248466491699, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 618}, "2": {"k": 2, "answer": "Texas A&M University", "latency_ms": 93.26958656311035, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1239}, "3": {"k": 3, "answer": "Texas A&M University", "latency_ms": 104.90012168884277, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1894}, "4": {"k": 4, "answer": "Texas A&M University", "latency_ms": 107.35917091369629, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2518}, "5": {"k": 5, "answer": "Texas A&M University", "latency_ms": 120.42999267578125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3165}, "6": {"k": 6, "answer": "Texas A&M University", "latency_ms": 126.04498863220215, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3823}}}
{"qid": "7579233308152571691", "question": "natural resources can be divided into what two categories", "gold_answers": ["Biotic", "Abiotic"], "shots": {"0": {"k": 0, "answer": "Renewable and non-renewable", "latency_ms": 119.28725242614746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "renewable and non-renewable", "latency_ms": 128.49807739257812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 697}, "2": {"k": 2, "answer": "renewable and non-renewable", "latency_ms": 133.23235511779785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1403}, "3": {"k": 3, "answer": "renewable and non-renewable", "latency_ms": 143.87202262878418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2201}, "4": {"k": 4, "answer": "renewable and non-renewable", "latency_ms": 155.26413917541504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2932}, "5": {"k": 5, "answer": "renewable and non-renewable", "latency_ms": 156.38041496276855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3579}, "6": {"k": 6, "answer": "renewable and non-renewable", "latency_ms": 171.67019844055176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4356}}}
{"qid": "1106103350744797710", "question": "what is the white stork a symbol for in western culture", "gold_answers": ["bringing babies to new parents"], "shots": {"0": {"k": 0, "answer": "Good luck and prosperity", "latency_ms": 71.72060012817383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "parental devotion", "latency_ms": 69.66400146484375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 640}, "2": {"k": 2, "answer": "parental devotion", "latency_ms": 81.06136322021484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1230}, "3": {"k": 3, "answer": "purity and maternal devotion", "latency_ms": 114.30525779724121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1879}, "4": {"k": 4, "answer": "parental devotion, purity, mercy, and child-bringing", "latency_ms": 207.1828842163086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2522}, "5": {"k": 5, "answer": "child-bringing myth, purity, parental devotion, national bird, cultural presence", "latency_ms": 258.6209774017334, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3189}, "6": {"k": 6, "answer": "child-birth, purity, parental devotion, mercy, good fortune", "latency_ms": 238.1129264831543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3795}}}
{"qid": "-8361853086037854880", "question": "india south africa test series 2018 highest runs", "gold_answers": ["AB de Villiers", "286"], "shots": {"0": {"k": 0, "answer": "Rohit Sharma", "latency_ms": 75.6983757019043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 77.0866870880127, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 584}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 93.9021110534668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1203}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 102.41150856018066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1834}, "4": {"k": 4, "answer": "558", "latency_ms": 106.74786567687988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2447}, "5": {"k": 5, "answer": "558", "latency_ms": 111.9086742401123, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3084}, "6": {"k": 6, "answer": "558", "latency_ms": 127.9294490814209, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3681}}}
{"qid": "-5202207990213031495", "question": "the oligodynamic effect is a phenomenon that describes", "gold_answers": ["a biocidal effect of metals"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 77.27217674255371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "biocidal effect of metals in low concentrations", "latency_ms": 143.35179328918457, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 715}, "2": {"k": 2, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 264.0650272369385, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 1412}, "3": {"k": 3, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 276.6430377960205, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 2074}, "4": {"k": 4, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 280.04932403564453, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 2740}, "5": {"k": 5, "answer": "the biocidal effect of metals, especially heavy metals, in low concentrations", "latency_ms": 257.3211193084717, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3478}, "6": {"k": 6, "answer": "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "latency_ms": 296.9551086425781, "em": 0.0, "f1": 0.4705882352941176, "containment": 1.0, "context_chars": 4146}}}
{"qid": "6391370451004919214", "question": "who was last person to be executed in us", "gold_answers": ["Ruben Cardenas Ramirez"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 72.01170921325684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Rainey Bethea", "latency_ms": 81.27117156982422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 634}, "2": {"k": 2, "answer": "Rainey Bethea", "latency_ms": 93.08600425720215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1263}, "3": {"k": 3, "answer": "Rainey Bethea", "latency_ms": 105.01670837402344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1874}, "4": {"k": 4, "answer": "Rainey Bethea", "latency_ms": 107.65695571899414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2532}, "5": {"k": 5, "answer": "Rainey Bethea", "latency_ms": 123.63600730895996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3156}, "6": {"k": 6, "answer": "Rainey Bethea", "latency_ms": 127.91228294372559, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3776}}}
{"qid": "-8069963735453146166", "question": "when do you declare honors in contract bridge", "gold_answers": ["any time after the auction"], "shots": {"0": {"k": 0, "answer": "After the opening lead and before the first round of play.", "latency_ms": 169.09193992614746, "em": 0.0, "f1": 0.15384615384615383, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 82.54480361938477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 614}, "2": {"k": 2, "answer": "at the conclusion of play", "latency_ms": 100.9528636932373, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1166}, "3": {"k": 3, "answer": "at the conclusion of play", "latency_ms": 116.93549156188965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1736}, "4": {"k": 4, "answer": "at the conclusion of play", "latency_ms": 119.48537826538086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2342}, "5": {"k": 5, "answer": "at the conclusion of play", "latency_ms": 131.75725936889648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2924}, "6": {"k": 6, "answer": "at the conclusion of play", "latency_ms": 136.77620887756348, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3618}}}
{"qid": "-2154502691082554319", "question": "which country is the last member of saarc", "gold_answers": ["Afghanistan"], "shots": {"0": {"k": 0, "answer": "Sri Lanka", "latency_ms": 58.81643295288086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Afghanistan", "latency_ms": 68.0246353149414, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 699}, "2": {"k": 2, "answer": "Afghanistan", "latency_ms": 81.29096031188965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1416}, "3": {"k": 3, "answer": "Afghanistan", "latency_ms": 92.7734375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2145}, "4": {"k": 4, "answer": "Afghanistan", "latency_ms": 97.06807136535645, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2851}, "5": {"k": 5, "answer": "Afghanistan", "latency_ms": 111.1454963684082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3547}, "6": {"k": 6, "answer": "Afghanistan", "latency_ms": 125.10800361633301, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4271}}}
{"qid": "-8498955431733322253", "question": "describe the three phases of the normal blood clotting process", "gold_answers": ["blood coagulation", "vasoconstriction ,"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.94161415100098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "vasoconstriction, platelet plug, fibrin clot", "latency_ms": 184.15141105651855, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 684}, "2": {"k": 2, "answer": "vasoconstriction, platelet plug formation, coagulation", "latency_ms": 205.08122444152832, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": "vasoconstriction, platelet plug formation, coagulation", "latency_ms": 212.43786811828613, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 1994}, "4": {"k": 4, "answer": "vasoconstriction, platelet plug formation, coagulation", "latency_ms": 220.73721885681152, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 2663}, "5": {"k": 5, "answer": "Vasoconstriction, platelet plug formation, coagulation", "latency_ms": 234.77935791015625, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 3321}, "6": {"k": 6, "answer": "vasoconstriction, platelet plug formation, coagulation", "latency_ms": 237.80536651611328, "em": 0.0, "f1": 0.33333333333333337, "containment": 0.0, "context_chars": 4042}}}
{"qid": "7392498509104714323", "question": "who played sonny in lemony snicket's a series of unfortunate events", "gold_answers": ["Presley Smith"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 72.4935531616211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.06958389282227, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 624}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 90.12269973754883, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1324}, "3": {"k": 3, "answer": "Presley Smith", "latency_ms": 92.89765357971191, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2016}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 123.23474884033203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2700}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.3343563079834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3306}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 138.95463943481445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4000}}}
{"qid": "2911493370352980057", "question": "when was united nations convention on the rights of the child created", "gold_answers": ["20 November 1989"], "shots": {"0": {"k": 0, "answer": "1989", "latency_ms": 70.88041305541992, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "20 November 1989", "latency_ms": 132.14516639709473, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 664}, "2": {"k": 2, "answer": "20 November 1989", "latency_ms": 138.8835906982422, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1324}, "3": {"k": 3, "answer": "20 November 1989", "latency_ms": 151.60727500915527, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2012}, "4": {"k": 4, "answer": "20 November 1989", "latency_ms": 155.86423873901367, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2594}, "5": {"k": 5, "answer": "20 November 1989", "latency_ms": 173.60711097717285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3212}, "6": {"k": 6, "answer": "20 November 1989", "latency_ms": 174.92341995239258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3865}}}
{"qid": "2495228512593253768", "question": "when did we decide to leave the eu", "gold_answers": ["23 June 2016"], "shots": {"0": {"k": 0, "answer": "2016", "latency_ms": 75.68597793579102, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "29 March 2017", "latency_ms": 135.79487800598145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 623}, "2": {"k": 2, "answer": "23 June 2016", "latency_ms": 138.7479305267334, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1294}, "3": {"k": 3, "answer": "23 June 2016", "latency_ms": 153.4445285797119, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1923}, "4": {"k": 4, "answer": "23 June 2016", "latency_ms": 158.2789421081543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2504}, "5": {"k": 5, "answer": "23 June 2016", "latency_ms": 171.19860649108887, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3099}, "6": {"k": 6, "answer": "23 June 2016", "latency_ms": 177.38890647888184, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3773}}}
{"qid": "95636137517606056", "question": "tallest building in the world of all time", "gold_answers": ["Burj Khalifa"], "shots": {"0": {"k": 0, "answer": "Burj Khalifa", "latency_ms": 75.68526268005371, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Burj Khalifa", "latency_ms": 87.4629020690918, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 639}, "2": {"k": 2, "answer": "Burj Khalifa", "latency_ms": 93.22547912597656, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1320}, "3": {"k": 3, "answer": "Burj Khalifa", "latency_ms": 102.42867469787598, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2001}, "4": {"k": 4, "answer": "Burj Khalifa", "latency_ms": 107.22136497497559, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2688}, "5": {"k": 5, "answer": "Burj Khalifa", "latency_ms": 124.87387657165527, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3291}, "6": {"k": 6, "answer": "Burj Khalifa", "latency_ms": 134.83285903930664, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3982}}}
{"qid": "-4020097033891103749", "question": "who plays chummy's mother in call the midwife", "gold_answers": ["Cheryl Campbell"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 72.33023643493652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.42840385437012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 664}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 94.57874298095703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1319}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 106.6896915435791, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1940}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 110.20064353942871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2578}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 123.67939949035645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3162}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 137.39323616027832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3742}}}
{"qid": "-7690316790133138931", "question": "who created separation of powers and checks and balances", "gold_answers": ["Montesquieu in the Enlightenment"], "shots": {"0": {"k": 0, "answer": "Montesquieu", "latency_ms": 75.42300224304199, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Montesquieu", "latency_ms": 80.93428611755371, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 647}, "2": {"k": 2, "answer": "Montesquieu", "latency_ms": 89.65682983398438, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1348}, "3": {"k": 3, "answer": "Montesquieu", "latency_ms": 104.65407371520996, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1992}, "4": {"k": 4, "answer": "Montesquieu", "latency_ms": 106.93740844726562, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2714}, "5": {"k": 5, "answer": "Montesquieu", "latency_ms": 119.69661712646484, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3352}, "6": {"k": 6, "answer": "Montesquieu", "latency_ms": 126.79052352905273, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 4054}}}
{"qid": "914815550191007566", "question": "who was the nfl first draft pick 2017", "gold_answers": ["Myles Garrett"], "shots": {"0": {"k": 0, "answer": "Kyler Murray", "latency_ms": 59.59749221801758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.21025085449219, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 678}, "2": {"k": 2, "answer": "12th overall pick: Deshaun Watson", "latency_ms": 167.71793365478516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1319}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 102.02956199645996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1948}, "4": {"k": 4, "answer": "No first draft pick mentioned in context.", "latency_ms": 156.10742568969727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2585}, "5": {"k": 5, "answer": "No answer provided in context.", "latency_ms": 148.25892448425293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3213}, "6": {"k": 6, "answer": "No information provided about the first draft pick in 2017. I cannot answer.", "latency_ms": 324.664831161499, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3758}}}
{"qid": "972568347460262100", "question": "when was nepal declared a secular state in bs", "gold_answers": ["January 15 , 2007"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 71.57087326049805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 85.50620079040527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 714}, "2": {"k": 2, "answer": "2008", "latency_ms": 92.88668632507324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1412}, "3": {"k": 3, "answer": "2008", "latency_ms": 102.63729095458984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2124}, "4": {"k": 4, "answer": "2008", "latency_ms": 105.08084297180176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2877}, "5": {"k": 5, "answer": "2008", "latency_ms": 124.26090240478516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3522}, "6": {"k": 6, "answer": "2008", "latency_ms": 136.61599159240723, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4163}}}
{"qid": "-4845827823184041115", "question": "who took control of jamestown and made new rules that made everyone work", "gold_answers": ["John Smith"], "shots": {"0": {"k": 0, "answer": "Captain John Smith", "latency_ms": 63.45963478088379, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "John Smith", "latency_ms": 54.00991439819336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 623}, "2": {"k": 2, "answer": "John Smith", "latency_ms": 62.70623207092285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1261}, "3": {"k": 3, "answer": "John Smith", "latency_ms": 81.45546913146973, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1923}, "4": {"k": 4, "answer": "John Smith", "latency_ms": 85.25943756103516, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2569}, "5": {"k": 5, "answer": "John Smith", "latency_ms": 99.09605979919434, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3241}, "6": {"k": 6, "answer": "John Smith", "latency_ms": 102.45418548583984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3966}}}
{"qid": "7592863385884727299", "question": "the site of secretion of h+ and cl-", "gold_answers": ["Parietal cells"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 76.8272876739502, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "parietal cells", "latency_ms": 76.76267623901367, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 625}, "2": {"k": 2, "answer": "parietal cells", "latency_ms": 85.9825611114502, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1212}, "3": {"k": 3, "answer": "parietal cells in the stomach fundus region", "latency_ms": 178.30324172973633, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1838}, "4": {"k": 4, "answer": "parietal cells of the gastric mucosa", "latency_ms": 170.1638698577881, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2495}, "5": {"k": 5, "answer": "parietal cells in the stomach fundus region", "latency_ms": 194.8392391204834, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3126}, "6": {"k": 6, "answer": "parietal cells in the stomach fundus region", "latency_ms": 198.7159252166748, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3870}}}
{"qid": "-217684522847197793", "question": "when does body temperature tend to be lowest", "gold_answers": ["11 p.m. to 3 a.m."], "shots": {"0": {"k": 0, "answer": "between 2 and 4 a.m.", "latency_ms": 136.16609573364258, "em": 0.0, "f1": 0.20000000000000004, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "4 a.m.", "latency_ms": 79.80585098266602, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "4 a.m.", "latency_ms": 84.56730842590332, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1301}, "3": {"k": 3, "answer": "4 a.m.", "latency_ms": 94.35224533081055, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1975}, "4": {"k": 4, "answer": "4 a.m.", "latency_ms": 110.43262481689453, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2610}, "5": {"k": 5, "answer": "4 a.m.", "latency_ms": 123.58975410461426, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3264}, "6": {"k": 6, "answer": "4 a.m.", "latency_ms": 126.74975395202637, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3895}}}
{"qid": "3926678456788602613", "question": "what us state forms the western boundary of montana", "gold_answers": ["Idaho"], "shots": {"0": {"k": 0, "answer": "Idaho", "latency_ms": 47.39952087402344, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 79.99610900878906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 673}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 93.70946884155273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1332}, "3": {"k": 3, "answer": "Idaho", "latency_ms": 81.12978935241699, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1926}, "4": {"k": 4, "answer": "Idaho", "latency_ms": 84.5799446105957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2599}, "5": {"k": 5, "answer": "Idaho", "latency_ms": 96.48895263671875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3235}, "6": {"k": 6, "answer": "Idaho", "latency_ms": 98.91986846923828, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3903}}}
{"qid": "-444951262186580688", "question": "what is the pirates of the caribbean in order", "gold_answers": ["At World 's End", "Dead Men Tell No Tales", "Dead Man 's Chest", "On Stranger Tides"], "shots": {"0": {"k": 0, "answer": "1. The Curse of the Black Pearl  \n2. Dead Man's Chest  \n3. At World's End  \n4. On Stranger Tides  \n5. Pirates of the Caribbean: Dead Men Tell No Tales", "latency_ms": 538.6853218078613, "em": 0.0, "f1": 0.32258064516129037, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 77.84438133239746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 93.08671951293945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1271}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 105.31497001647949, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1835}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 106.18162155151367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2520}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 120.41592597961426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3151}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 125.23746490478516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3775}}}
{"qid": "-822810697742332337", "question": "if a piece of music is perceived to have changed key then we say the piece has", "gold_answers": ["transposed", "transposition"], "shots": {"0": {"k": 0, "answer": "modulated", "latency_ms": 48.6297607421875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "changed key", "latency_ms": 55.35006523132324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 674}, "2": {"k": 2, "answer": "changed key", "latency_ms": 58.83955955505371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1429}, "3": {"k": 3, "answer": "changed key", "latency_ms": 77.20375061035156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2096}, "4": {"k": 4, "answer": "modulated", "latency_ms": 80.42716979980469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2741}, "5": {"k": 5, "answer": "modulated", "latency_ms": 97.81384468078613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3351}, "6": {"k": 6, "answer": "modulated", "latency_ms": 99.82848167419434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3963}}}
{"qid": "-4663296615598634791", "question": "who was allowed to vote in the roman republic", "gold_answers": ["unknown", "citizens"], "shots": {"0": {"k": 0, "answer": "Roman citizens of certain property ownership classes", "latency_ms": 112.54072189331055, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Roman citizens", "latency_ms": 56.23483657836914, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 672}, "2": {"k": 2, "answer": "Full Roman citizens (excluding women, slaves, and originally those living outside of Rome)", "latency_ms": 243.99089813232422, "em": 0.0, "f1": 0.14285714285714288, "containment": 1.0, "context_chars": 1311}, "3": {"k": 3, "answer": "Adult male citizens", "latency_ms": 92.20385551452637, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1991}, "4": {"k": 4, "answer": "Adult male Roman citizens", "latency_ms": 108.43491554260254, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2685}, "5": {"k": 5, "answer": "adult male Roman citizens", "latency_ms": 122.57719039916992, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3347}, "6": {"k": 6, "answer": "Adult male Roman citizens", "latency_ms": 126.19471549987793, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 4058}}}
{"qid": "3695813920971622504", "question": "when does the next warrior book come out", "gold_answers": ["April 10 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.9141960144043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 81.48765563964844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 635}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 104.1567325592041, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1503}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 106.89926147460938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2163}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 120.03588676452637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2807}, "5": {"k": 5, "answer": "9 April 2019", "latency_ms": 162.24908828735352, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3421}, "6": {"k": 6, "answer": "9 April 2019", "latency_ms": 174.73506927490234, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 4119}}}
{"qid": "8533079479108336590", "question": "blood alcohol concentration means the parts of alcohol in the blood in relation to what", "gold_answers": ["volume of blood", "ethanol"], "shots": {"0": {"k": 0, "answer": "total blood volume", "latency_ms": 63.662052154541016, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "blood volume", "latency_ms": 52.47378349304199, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "volume of blood", "latency_ms": 69.87810134887695, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1302}, "3": {"k": 3, "answer": "volume of blood", "latency_ms": 92.6046371459961, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1899}, "4": {"k": 4, "answer": "volume of blood", "latency_ms": 94.00558471679688, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2514}, "5": {"k": 5, "answer": "volume of blood", "latency_ms": 110.90993881225586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3172}, "6": {"k": 6, "answer": "volume of blood", "latency_ms": 114.91703987121582, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3889}}}
{"qid": "-1531676396763282931", "question": "which greek god flew too close to the sun", "gold_answers": ["Icarus"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 72.27563858032227, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.29059791564941, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Phaëthon", "latency_ms": 93.18351745605469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1200}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 105.04722595214844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1736}, "4": {"k": 4, "answer": "Phaethon", "latency_ms": 107.3298454284668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2351}, "5": {"k": 5, "answer": "Phaethon", "latency_ms": 120.8047866821289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2921}, "6": {"k": 6, "answer": "Phaethon", "latency_ms": 128.0961036682129, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3539}}}
{"qid": "-3682350378604306871", "question": "when did the song rapper's delight come out", "gold_answers": ["September 16 , 1979", "1979"], "shots": {"0": {"k": 0, "answer": "1979", "latency_ms": 76.80511474609375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 16, 1979", "latency_ms": 158.37526321411133, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 615}, "2": {"k": 2, "answer": "September 16, 1979", "latency_ms": 164.4454002380371, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1195}, "3": {"k": 3, "answer": "1979", "latency_ms": 102.2343635559082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1851}, "4": {"k": 4, "answer": "1979", "latency_ms": 108.61992835998535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2464}, "5": {"k": 5, "answer": "1979", "latency_ms": 121.29902839660645, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3123}, "6": {"k": 6, "answer": "1979", "latency_ms": 123.6109733581543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3752}}}
{"qid": "-3646291394492263644", "question": "when does the stock market open in hong kong", "gold_answers": ["09 : 15"], "shots": {"0": {"k": 0, "answer": "9:30 AM Hong Kong Time", "latency_ms": 125.13399124145508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 85.13021469116211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "09:00 am", "latency_ms": 117.55967140197754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1242}, "3": {"k": 3, "answer": "09:00 am", "latency_ms": 127.35676765441895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1900}, "4": {"k": 4, "answer": "09:00 am", "latency_ms": 136.57116889953613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2548}, "5": {"k": 5, "answer": "09:00 am", "latency_ms": 150.6671905517578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3177}, "6": {"k": 6, "answer": "9:00 am", "latency_ms": 151.92699432373047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3849}}}
{"qid": "9187994979510737197", "question": "who's the model on let's make a deal", "gold_answers": ["Tiffany Adams Coyne"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 75.48737525939941, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Tiffany Coyne", "latency_ms": 86.27843856811523, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 574}, "2": {"k": 2, "answer": "Tiffany Coyne", "latency_ms": 89.41030502319336, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1202}, "3": {"k": 3, "answer": "Tiffany Coyne", "latency_ms": 102.19907760620117, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1812}, "4": {"k": 4, "answer": "Tiffany Coyne", "latency_ms": 107.86271095275879, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2398}, "5": {"k": 5, "answer": "Tiffany Coyne", "latency_ms": 126.35302543640137, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3025}, "6": {"k": 6, "answer": "Tiffany Coyne", "latency_ms": 137.6502513885498, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3639}}}
{"qid": "5078709962400196312", "question": "what mlb teams did deion sanders play for", "gold_answers": ["San Francisco Giants", "Cincinnati Reds", "Atlanta Braves", "New York Yankees"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.59709930419922, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Atlanta Braves", "latency_ms": 56.17666244506836, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 609}, "2": {"k": 2, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 200.1361846923828, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1250}, "3": {"k": 3, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 216.44020080566406, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1856}, "4": {"k": 4, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 216.25375747680664, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 2499}, "5": {"k": 5, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 233.47711563110352, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3075}, "6": {"k": 6, "answer": "New York Yankees, Atlanta Braves, Cincinnati Reds, San Francisco Giants", "latency_ms": 236.74678802490234, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3684}}}
{"qid": "-4910305402988079742", "question": "how much money did the film titanic make", "gold_answers": ["$2.18 billion", "$2.187 billion"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 76.16901397705078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "$343.4 million", "latency_ms": 116.33157730102539, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 624}, "2": {"k": 2, "answer": "$1.84 billion", "latency_ms": 117.22493171691895, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1256}, "3": {"k": 3, "answer": "$1.84 billion", "latency_ms": 127.24447250366211, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1914}, "4": {"k": 4, "answer": "$1.84 billion", "latency_ms": 129.87518310546875, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2507}, "5": {"k": 5, "answer": "$1,843,201,268", "latency_ms": 247.49159812927246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3176}, "6": {"k": 6, "answer": "$2.187 billion", "latency_ms": 176.14364624023438, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3827}}}
{"qid": "1636956235349274030", "question": "when does it's a wonderful life comes on", "gold_answers": ["Christmas season"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.77186012268066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.54451370239258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 89.45393562316895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1249}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 105.24201393127441, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1909}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 106.7514419555664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2535}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 123.09074401855469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3177}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 138.20767402648926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3837}}}
{"qid": "5608421991404219229", "question": "where does the last name hansen come from", "gold_answers": ["the Faroe Islands", "Norway", "Denmark", "Scandinavian"], "shots": {"0": {"k": 0, "answer": "Denmark/Norway", "latency_ms": 87.82339096069336, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Denmark", "latency_ms": 62.505483627319336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 686}, "2": {"k": 2, "answer": "Scandinavia", "latency_ms": 79.03480529785156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1342}, "3": {"k": 3, "answer": "Scandinavia", "latency_ms": 92.59343147277832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1993}, "4": {"k": 4, "answer": "Scandinavian patronymic surname, meaning \"son of Hans\"", "latency_ms": 244.57550048828125, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 2621}, "5": {"k": 5, "answer": "Scandinavia", "latency_ms": 113.33107948303223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3233}, "6": {"k": 6, "answer": "Scandinavia", "latency_ms": 125.35262107849121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3858}}}
{"qid": "2577140274301537828", "question": "what is the symbol for christmas in brazil", "gold_answers": ["Nativity scene", "Christmas Tree"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 72.40986824035645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Puritina", "latency_ms": 81.50720596313477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 652}, "2": {"k": 2, "answer": "Christmas Tree and Nativity scene", "latency_ms": 116.61505699157715, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1281}, "3": {"k": 3, "answer": "Christmas Tree and Nativity scene", "latency_ms": 127.55393981933594, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1949}, "4": {"k": 4, "answer": "Christmas tree and Nativity scene", "latency_ms": 130.34653663635254, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2595}, "5": {"k": 5, "answer": "Christmas tree and Nativity scene", "latency_ms": 146.51131629943848, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3270}, "6": {"k": 6, "answer": "Christmas tree and Nativity scene", "latency_ms": 153.37252616882324, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3908}}}
{"qid": "-6762654750812139940", "question": "who sings she's like the wind lyrics", "gold_answers": ["Wendy Fraser", "Patrick Swayze"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 74.87177848815918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 122.28775024414062, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 624}, "2": {"k": 2, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 127.39706039428711, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1249}, "3": {"k": 3, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 143.1131362915039, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1881}, "4": {"k": 4, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 146.64101600646973, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2475}, "5": {"k": 5, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 161.64016723632812, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3098}, "6": {"k": 6, "answer": "Patrick Swayze & Wendy Fraser", "latency_ms": 172.18685150146484, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3723}}}
{"qid": "-3640066593401557276", "question": "where does the this is us family live", "gold_answers": ["Pittsburgh"], "shots": {"0": {"k": 0, "answer": "Rhode Island", "latency_ms": 64.26167488098145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Los Angeles, New Jersey", "latency_ms": 93.52254867553711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "Los Angeles, Pittsburgh, New Jersey", "latency_ms": 128.91149520874023, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1299}, "3": {"k": 3, "answer": "Los Angeles, Pittsburgh, New Jersey", "latency_ms": 141.6330337524414, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1910}, "4": {"k": 4, "answer": "Los Angeles, New Jersey, Pittsburgh", "latency_ms": 144.4089412689209, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 2558}, "5": {"k": 5, "answer": "Los Angeles, Pittsburgh, New Jersey", "latency_ms": 157.20820426940918, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3223}, "6": {"k": 6, "answer": "Los Angeles, Pittsburgh, New Jersey", "latency_ms": 174.24488067626953, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3885}}}
{"qid": "-4546405080702371934", "question": "when was the term prime minister first used", "gold_answers": ["18th century", "1624"], "shots": {"0": {"k": 0, "answer": "1801", "latency_ms": 74.95260238647461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 87.11051940917969, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "1901", "latency_ms": 93.9493179321289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1319}, "3": {"k": 3, "answer": "1869", "latency_ms": 104.29191589355469, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1987}, "4": {"k": 4, "answer": "18th century", "latency_ms": 107.65886306762695, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2651}, "5": {"k": 5, "answer": "1727", "latency_ms": 122.52616882324219, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3319}, "6": {"k": 6, "answer": "1727", "latency_ms": 138.14210891723633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3987}}}
{"qid": "3653635084331615973", "question": "who won the mens single ice skating 2018", "gold_answers": ["Javier Fernández", "Shoma Uno", "Yuzuru Hanyu"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 72.51238822937012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Yuzuru Hanyu", "latency_ms": 105.84592819213867, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 627}, "2": {"k": 2, "answer": "Yuzuru Hanyu", "latency_ms": 118.28112602233887, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1252}, "3": {"k": 3, "answer": "Yuzuru Hanyu", "latency_ms": 127.07901000976562, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1940}, "4": {"k": 4, "answer": "Yuzuru Hanyu", "latency_ms": 133.88466835021973, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2588}, "5": {"k": 5, "answer": "Yuzuru Hanyu", "latency_ms": 148.6375331878662, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3188}, "6": {"k": 6, "answer": "Yuzuru Hanyu", "latency_ms": 164.44659233093262, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3798}}}
{"qid": "-8943432852681116531", "question": "what is cain and abel software used for", "gold_answers": ["password recovery"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 75.40655136108398, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.3802433013916, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 694}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 90.13891220092773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1385}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 101.13716125488281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2023}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 110.2137565612793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2693}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 121.32120132446289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3384}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 127.52389907836914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4055}}}
{"qid": "3562729819386821776", "question": "what is the name of governor of maharashtra", "gold_answers": ["Chennamaneni Vidyasagar Rao"], "shots": {"0": {"k": 0, "answer": "Eknath Shinde", "latency_ms": 83.42623710632324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 79.70666885375977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "C. Vidyasagar Rao", "latency_ms": 124.02105331420898, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1299}, "3": {"k": 3, "answer": "C. Vidyasagar Rao", "latency_ms": 127.64811515808105, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1953}, "4": {"k": 4, "answer": "C. Vidyasagar Rao", "latency_ms": 141.6640281677246, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2593}, "5": {"k": 5, "answer": "C. Vidyasagar Rao", "latency_ms": 158.13946723937988, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3232}, "6": {"k": 6, "answer": "C. Vidyasagar Rao", "latency_ms": 165.53330421447754, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3873}}}
{"qid": "-2040762149245803825", "question": "what channel is abc on in phoenix arizona", "gold_answers": ["15"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.31075859069824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "15", "latency_ms": 57.41119384765625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 603}, "2": {"k": 2, "answer": "15", "latency_ms": 63.291311264038086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "15", "latency_ms": 76.44796371459961, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1959}, "4": {"k": 4, "answer": "15", "latency_ms": 80.37281036376953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2713}, "5": {"k": 5, "answer": "15", "latency_ms": 95.31378746032715, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3390}, "6": {"k": 6, "answer": "15", "latency_ms": 111.55128479003906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4022}}}
{"qid": "-1378070576053000718", "question": "who played jennifer in back to the future", "gold_answers": ["Claudia Grace Wells"], "shots": {"0": {"k": 0, "answer": "Michael J. Fox", "latency_ms": 75.56629180908203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Claudia Wells", "latency_ms": 77.38208770751953, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 597}, "2": {"k": 2, "answer": "Claudia Wells", "latency_ms": 94.20275688171387, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1207}, "3": {"k": 3, "answer": "Claudia Wells", "latency_ms": 101.21965408325195, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1829}, "4": {"k": 4, "answer": "Claudia Wells", "latency_ms": 106.46533966064453, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2477}, "5": {"k": 5, "answer": "Claudia Wells", "latency_ms": 122.64108657836914, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3062}, "6": {"k": 6, "answer": "Claudia Wells", "latency_ms": 124.14360046386719, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3667}}}
{"qid": "-8946826352712324685", "question": "where was the summer olympics held in 2012", "gold_answers": ["London", "London , United Kingdom"], "shots": {"0": {"k": 0, "answer": "London", "latency_ms": 39.63303565979004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "London, United Kingdom", "latency_ms": 86.28487586975098, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 639}, "2": {"k": 2, "answer": "London, United Kingdom", "latency_ms": 90.17062187194824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1244}, "3": {"k": 3, "answer": "London, United Kingdom", "latency_ms": 107.38229751586914, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1891}, "4": {"k": 4, "answer": "London, United Kingdom", "latency_ms": 118.28064918518066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2490}, "5": {"k": 5, "answer": "London, United Kingdom", "latency_ms": 126.60336494445801, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3105}, "6": {"k": 6, "answer": "London, United Kingdom", "latency_ms": 137.76707649230957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3741}}}
{"qid": "-5578685052965147220", "question": "who cooks for the president of the united states", "gold_answers": ["Cristeta Comerford", "The White House Executive Chef", "White House Executive Chef"], "shots": {"0": {"k": 0, "answer": "The president's chef", "latency_ms": 75.08635520935059, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "White House Executive Chef", "latency_ms": 78.56392860412598, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 686}, "2": {"k": 2, "answer": "White House Executive Chef", "latency_ms": 87.40377426147461, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1369}, "3": {"k": 3, "answer": "White House Executive Chef", "latency_ms": 102.21529006958008, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2038}, "4": {"k": 4, "answer": "White House Executive Chef", "latency_ms": 109.94338989257812, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2656}, "5": {"k": 5, "answer": "White House Executive Chef", "latency_ms": 120.98050117492676, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3325}, "6": {"k": 6, "answer": "White House Executive Chef", "latency_ms": 126.30271911621094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3982}}}
{"qid": "-7323892214876884658", "question": "who was the first indian to be appointed as a judge in the international court of justice", "gold_answers": ["Sir Benegal Narsing Rau"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.79999351501465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.54904365539551, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 90.28744697570801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1248}, "3": {"k": 3, "answer": "Benegal Narsing Rau", "latency_ms": 126.60837173461914, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 1954}, "4": {"k": 4, "answer": "Benegal Narsing Rau", "latency_ms": 145.31874656677246, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 2571}, "5": {"k": 5, "answer": "Benegal Narsing Rau", "latency_ms": 151.74078941345215, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 3200}, "6": {"k": 6, "answer": "Benegal Narsing Rau", "latency_ms": 164.7806167602539, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 3846}}}
{"qid": "-7530191989994326613", "question": "where was the film coming home for christmas filmed", "gold_answers": ["British Columbia"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 71.59781455993652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "British Columbia", "latency_ms": 62.12496757507324, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 656}, "2": {"k": 2, "answer": "British Columbia", "latency_ms": 68.82214546203613, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1301}, "3": {"k": 3, "answer": "Aldergrove, British Columbia", "latency_ms": 142.72117614746094, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1986}, "4": {"k": 4, "answer": "Aldergrove, British Columbia", "latency_ms": 144.18625831604004, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2592}, "5": {"k": 5, "answer": "Aldergrove, British Columbia", "latency_ms": 160.71224212646484, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3250}, "6": {"k": 6, "answer": "Aldergrove, British Columbia", "latency_ms": 165.69924354553223, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3850}}}
{"qid": "1036801998292429298", "question": "when do the new episodes of supernatural start", "gold_answers": ["May 10 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 72.60632514953613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "October 12, 2017", "latency_ms": 160.76421737670898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "October 11, 2018", "latency_ms": 164.3199920654297, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1322}, "3": {"k": 3, "answer": "October 11, 2018", "latency_ms": 181.45203590393066, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1949}, "4": {"k": 4, "answer": "October 11, 2018", "latency_ms": 194.08893585205078, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2601}, "5": {"k": 5, "answer": "October 11, 2018", "latency_ms": 199.86224174499512, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3233}, "6": {"k": 6, "answer": "October 11, 2018", "latency_ms": 214.9810791015625, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3869}}}
{"qid": "-7296166998534064378", "question": "who played the middle sister on full house", "gold_answers": ["Jodie Sweetin"], "shots": {"0": {"k": 0, "answer": "Dana Delany", "latency_ms": 72.25584983825684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lori Loughlin", "latency_ms": 95.42632102966309, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "Jodie Sweetin", "latency_ms": 93.93620491027832, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1282}, "3": {"k": 3, "answer": "Jodie Sweetin", "latency_ms": 102.24127769470215, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1885}, "4": {"k": 4, "answer": "Jodie Sweetin", "latency_ms": 120.34034729003906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2513}, "5": {"k": 5, "answer": "Lori Loughlin", "latency_ms": 139.3284797668457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3121}, "6": {"k": 6, "answer": "Lori Loughlin", "latency_ms": 149.5535373687744, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3746}}}
{"qid": "-4173722566701480014", "question": "a 58-story skyscraper in san francisco is tilting and sinking", "gold_answers": ["Millennium Tower"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 75.15859603881836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 80.91592788696289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 614}, "2": {"k": 2, "answer": "Millennium Tower", "latency_ms": 90.1789665222168, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1266}, "3": {"k": 3, "answer": "Millennium Tower", "latency_ms": 104.61831092834473, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1867}, "4": {"k": 4, "answer": "Millennium Tower", "latency_ms": 106.10294342041016, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2553}, "5": {"k": 5, "answer": "Millennium Tower", "latency_ms": 123.02756309509277, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3248}, "6": {"k": 6, "answer": "Millennium Tower", "latency_ms": 126.32322311401367, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3905}}}
{"qid": "8741253661372783389", "question": "when was the first season of when calls the heart", "gold_answers": ["2014", "January 11 , 2014"], "shots": {"0": {"k": 0, "answer": "2014", "latency_ms": 73.10819625854492, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "2015", "latency_ms": 87.03494071960449, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 626}, "2": {"k": 2, "answer": "2014", "latency_ms": 94.62714195251465, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1221}, "3": {"k": 3, "answer": "2014", "latency_ms": 102.37431526184082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1889}, "4": {"k": 4, "answer": "January 11, 2014", "latency_ms": 181.74457550048828, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2525}, "5": {"k": 5, "answer": "January 11, 2014", "latency_ms": 198.63176345825195, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3161}, "6": {"k": 6, "answer": "January 11, 2014", "latency_ms": 212.14032173156738, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3787}}}
{"qid": "-2244798821441304446", "question": "who said one man's vulgarity is another's lyric", "gold_answers": ["Justice Harlan"], "shots": {"0": {"k": 0, "answer": "T.S. Eliot", "latency_ms": 89.61105346679688, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 82.44609832763672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 616}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 94.32435035705566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1307}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 102.53047943115234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1945}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 109.98034477233887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2559}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 120.40185928344727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3165}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 136.84511184692383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3812}}}
{"qid": "4764100575661627990", "question": "what football player is called the honey badger", "gold_answers": ["Tyrann Devine Mathieu"], "shots": {"0": {"k": 0, "answer": "Chris Berman", "latency_ms": 60.4860782623291, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Tyrann Mathieu", "latency_ms": 93.31011772155762, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "Tyrann Mathieu", "latency_ms": 101.89151763916016, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Tyrann Mathieu", "latency_ms": 117.75875091552734, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1968}, "4": {"k": 4, "answer": "Tyrann Mathieu", "latency_ms": 117.36083030700684, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2618}, "5": {"k": 5, "answer": "Tyrann Mathieu", "latency_ms": 136.0325813293457, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3236}, "6": {"k": 6, "answer": "Tyrann Mathieu", "latency_ms": 140.67363739013672, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3928}}}
{"qid": "-2273381713182307888", "question": "how many episodes of the killing on netflix", "gold_answers": ["six", "6"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 73.09412956237793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "6", "latency_ms": 43.41387748718262, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 660}, "2": {"k": 2, "answer": "6", "latency_ms": 51.748037338256836, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1318}, "3": {"k": 3, "answer": "6", "latency_ms": 64.79167938232422, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1926}, "4": {"k": 4, "answer": "6", "latency_ms": 74.4936466217041, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2633}, "5": {"k": 5, "answer": "6", "latency_ms": 81.93564414978027, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3306}, "6": {"k": 6, "answer": "6", "latency_ms": 87.85724639892578, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3923}}}
{"qid": "7214445857680379083", "question": "from whose perspective is the story of all quiet on the western front told", "gold_answers": ["Paul Baumer"], "shots": {"0": {"k": 0, "answer": "Paul Baumer's", "latency_ms": 76.74217224121094, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "soldiers' perspective", "latency_ms": 75.24490356445312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "Erich Maria Remarque", "latency_ms": 117.78497695922852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1261}, "3": {"k": 3, "answer": "Paul's perspective", "latency_ms": 93.18041801452637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1905}, "4": {"k": 4, "answer": "Erich Maria Remarque's perspective", "latency_ms": 156.95643424987793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2492}, "5": {"k": 5, "answer": "Paul's perspective", "latency_ms": 109.79008674621582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3117}, "6": {"k": 6, "answer": "Paul's perspective", "latency_ms": 114.42852020263672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3670}}}
{"qid": "4905519627710399354", "question": "who was the person who escaped from alcatraz", "gold_answers": ["John Anglin", "Clarence Anglin", "Frank Morris"], "shots": {"0": {"k": 0, "answer": "Frank Morris and the Anglin brothers", "latency_ms": 111.81282997131348, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Morris and the Anglins", "latency_ms": 116.17565155029297, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Allen West", "latency_ms": 66.2987232208252, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1175}, "3": {"k": 3, "answer": "Morris and the Anglins", "latency_ms": 143.5685157775879, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1858}, "4": {"k": 4, "answer": "John Paul Scott", "latency_ms": 97.14841842651367, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2497}, "5": {"k": 5, "answer": "John Paul Scott", "latency_ms": 107.65743255615234, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3121}, "6": {"k": 6, "answer": "John Paul Scott", "latency_ms": 114.73226547241211, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3717}}}
{"qid": "-5199162398326752157", "question": "where does trick or treat for unicef money go", "gold_answers": ["UNICEF 's global programing"], "shots": {"0": {"k": 0, "answer": "UNICEF", "latency_ms": 62.658071517944336, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "UNICEF", "latency_ms": 74.3556022644043, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 718}, "2": {"k": 2, "answer": "UNICEF delivers lifesaving therapeutic food to severely malnourished children around the world.", "latency_ms": 277.6334285736084, "em": 0.0, "f1": 0.13333333333333333, "containment": 0.0, "context_chars": 1376}, "3": {"k": 3, "answer": "lifesaving packets of therapeutic food to severely malnourished children around the world", "latency_ms": 262.59827613830566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2106}, "4": {"k": 4, "answer": "UNICEF's global programing, including lifesaving therapeutic food for severely malnourished children.", "latency_ms": 330.9366703033447, "em": 0.0, "f1": 0.26666666666666666, "containment": 0.0, "context_chars": 2800}, "5": {"k": 5, "answer": "UNICEF's global programing, including lifesaving therapeutic food for severely malnourished children.", "latency_ms": 334.644079208374, "em": 0.0, "f1": 0.26666666666666666, "containment": 0.0, "context_chars": 3488}, "6": {"k": 6, "answer": "UNICEF's global programing, including lifesaving therapeutic food for severely malnourished children.", "latency_ms": 349.58553314208984, "em": 0.0, "f1": 0.26666666666666666, "containment": 0.0, "context_chars": 4121}}}
{"qid": "9200381329776083757", "question": "where is the inscription on the statue of liberty", "gold_answers": ["The New Colossus"], "shots": {"0": {"k": 0, "answer": "Statue of Liberty's inscription is on the pedestal.", "latency_ms": 161.79537773132324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Inside the statue", "latency_ms": 67.18587875366211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "inside the pedestal of the Statue of Liberty", "latency_ms": 139.32490348815918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "Inside the pedestal of the Statue of Liberty", "latency_ms": 152.53663063049316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1864}, "4": {"k": 4, "answer": "Inside the pedestal, on a plaque engraved with lines from \"The New Colossus\" by Emma Lazarus.", "latency_ms": 339.4885063171387, "em": 0.0, "f1": 0.2666666666666667, "containment": 1.0, "context_chars": 2421}, "5": {"k": 5, "answer": "Inside the pedestal, on a plaque engraved with lines from \"The New Colossus\" by Emma Lazarus.", "latency_ms": 357.58447647094727, "em": 0.0, "f1": 0.2666666666666667, "containment": 1.0, "context_chars": 3013}, "6": {"k": 6, "answer": "Inside the pedestal, on a plaque engraved with lines from \"The New Colossus\" by Emma Lazarus.", "latency_ms": 361.19723320007324, "em": 0.0, "f1": 0.2666666666666667, "containment": 1.0, "context_chars": 3691}}}
{"qid": "9179702110514757818", "question": "where are the next two winter olympics going to be held", "gold_answers": ["Beijing", "TBD"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 74.66745376586914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 83.09197425842285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 94.13456916809082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1236}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 105.94487190246582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1838}, "4": {"k": 4, "answer": "Beijing 2022, Pyeongchang 2018", "latency_ms": 270.4145908355713, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2463}, "5": {"k": 5, "answer": "Beijing and Pyeongchang", "latency_ms": 158.06031227111816, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3064}, "6": {"k": 6, "answer": "Beijing (2022), then 2026 host to be selected on June 23, 2019", "latency_ms": 445.3866481781006, "em": 0.0, "f1": 0.15384615384615385, "containment": 1.0, "context_chars": 3726}}}
{"qid": "-5410471527866417159", "question": "when did i'm coming out come out", "gold_answers": ["August 22 , 1980"], "shots": {"0": {"k": 0, "answer": "1994", "latency_ms": 71.83146476745605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "August 22, 1980", "latency_ms": 154.84118461608887, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 631}, "2": {"k": 2, "answer": "August 22, 1980", "latency_ms": 163.9564037322998, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1256}, "3": {"k": 3, "answer": "August 22, 1980", "latency_ms": 179.84509468078613, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1892}, "4": {"k": 4, "answer": "August 22, 1980", "latency_ms": 180.05919456481934, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2519}, "5": {"k": 5, "answer": "August 22, 1980", "latency_ms": 197.3888874053955, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3135}, "6": {"k": 6, "answer": "August 22, 1980", "latency_ms": 211.70926094055176, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3702}}}
{"qid": "-3621585537083644585", "question": "what type of bridge is the charles bridge", "gold_answers": ["bow bridge", "a bow bridge", "Stone"], "shots": {"0": {"k": 0, "answer": "stone arch bridge", "latency_ms": 63.87662887573242, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "historic bridge", "latency_ms": 57.985782623291016, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 631}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 94.50745582580566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1311}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 100.84724426269531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1978}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 106.9943904876709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2679}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 123.53086471557617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3335}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 138.03339004516602, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3913}}}
{"qid": "-6934435686987287378", "question": "who makes the important government decisions in an autocracy", "gold_answers": ["one person"], "shots": {"0": {"k": 0, "answer": "The ruler or leader", "latency_ms": 76.29084587097168, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "one person or a small elite", "latency_ms": 105.15046119689941, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 674}, "2": {"k": 2, "answer": "one person or a small elite", "latency_ms": 112.152099609375, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1333}, "3": {"k": 3, "answer": "one person or a small elite", "latency_ms": 115.38815498352051, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1997}, "4": {"k": 4, "answer": "one person or a small elite", "latency_ms": 129.67777252197266, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2632}, "5": {"k": 5, "answer": "one person or a small elite", "latency_ms": 143.94235610961914, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3298}, "6": {"k": 6, "answer": "one person or a small elite", "latency_ms": 150.132417678833, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3952}}}
{"qid": "-7488727646515698670", "question": "what was the main characteristic of post-world war ii american society", "gold_answers": ["strong economic growth"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 76.55787467956543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Blue Social Model", "latency_ms": 68.11904907226562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "stable jobs, high school diploma sufficed, large labor unions, regulated prices, low competition", "latency_ms": 255.74660301208496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1368}, "3": {"k": 3, "answer": "stable jobs, high school diploma sufficed for stable career, large labor unions, low corporate competition, government regulation keeping prices down", "latency_ms": 359.76219177246094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2077}, "4": {"k": 4, "answer": "prosperity and stability", "latency_ms": 131.09946250915527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2839}, "5": {"k": 5, "answer": "prosperity, stability, and a strong work ethic", "latency_ms": 224.1652011871338, "em": 0.0, "f1": 0.2222222222222222, "containment": 0.0, "context_chars": 3538}, "6": {"k": 6, "answer": "prosperity, stability, and a strong work ethic", "latency_ms": 225.36158561706543, "em": 0.0, "f1": 0.2222222222222222, "containment": 0.0, "context_chars": 4241}}}
{"qid": "5760101138584878549", "question": "which way does the earth orbit the sun", "gold_answers": ["counter clockwise direction", "counter clockwise"], "shots": {"0": {"k": 0, "answer": "counterclockwise", "latency_ms": 71.44331932067871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "counterclockwise", "latency_ms": 82.81421661376953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 595}, "2": {"k": 2, "answer": "counterclockwise", "latency_ms": 88.45067024230957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1147}, "3": {"k": 3, "answer": "counterclockwise", "latency_ms": 107.11359977722168, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1755}, "4": {"k": 4, "answer": "counterclockwise", "latency_ms": 106.47058486938477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2362}, "5": {"k": 5, "answer": "counterclockwise", "latency_ms": 119.75646018981934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2993}, "6": {"k": 6, "answer": "counterclockwise", "latency_ms": 125.96797943115234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3595}}}
{"qid": "-7003244893871973796", "question": "what is on top of us capitol building", "gold_answers": ["The Statue of Freedom"], "shots": {"0": {"k": 0, "answer": "Dome", "latency_ms": 51.17917060852051, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "dome", "latency_ms": 57.23977088928223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 655}, "2": {"k": 2, "answer": "dome", "latency_ms": 62.93129920959473, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1312}, "3": {"k": 3, "answer": "dome", "latency_ms": 77.07548141479492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1965}, "4": {"k": 4, "answer": "dome", "latency_ms": 81.87270164489746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2548}, "5": {"k": 5, "answer": "dome", "latency_ms": 93.26958656311035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3215}, "6": {"k": 6, "answer": "bronze eagle", "latency_ms": 110.78381538391113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3811}}}
{"qid": "-3587434056219625939", "question": "who did dwight howard play for last year", "gold_answers": ["Charlotte Hornets", "Atlanta Hawks"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 75.78754425048828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 81.33769035339355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 588}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 92.64922142028809, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1220}, "3": {"k": 3, "answer": "Washington Wizards", "latency_ms": 82.76033401489258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1859}, "4": {"k": 4, "answer": "Washington Wizards", "latency_ms": 96.47440910339355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2454}, "5": {"k": 5, "answer": "Washington Wizards", "latency_ms": 97.89371490478516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3053}, "6": {"k": 6, "answer": "Washington Wizards", "latency_ms": 114.57610130310059, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3657}}}
{"qid": "-6209397953683103736", "question": "who lasted the longest in the royal rumble", "gold_answers": ["Rey Mysterio"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 76.04479789733887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Greatest Royal Rumble", "latency_ms": 95.03531455993652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "Mysterio", "latency_ms": 80.25407791137695, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1209}, "3": {"k": 3, "answer": "Mysterio", "latency_ms": 93.52922439575195, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1803}, "4": {"k": 4, "answer": "Mysterio", "latency_ms": 94.45548057556152, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2411}, "5": {"k": 5, "answer": "Mysterio", "latency_ms": 107.50722885131836, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3012}, "6": {"k": 6, "answer": "Mysterio", "latency_ms": 116.14561080932617, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3619}}}
{"qid": "-5424723408850352324", "question": "what is the largest man made lake in the us", "gold_answers": ["Lake Oahe"], "shots": {"0": {"k": 0, "answer": "Lake Mead", "latency_ms": 64.25619125366211, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lake of the Ozarks", "latency_ms": 88.83261680603027, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "Lake of the Ozarks", "latency_ms": 95.53933143615723, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "Lake of the Ozarks", "latency_ms": 117.85244941711426, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1901}, "4": {"k": 4, "answer": "Lake of the Ozarks", "latency_ms": 121.37937545776367, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2566}, "5": {"k": 5, "answer": "Lake Mead", "latency_ms": 110.80503463745117, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3145}, "6": {"k": 6, "answer": "Lake Mead", "latency_ms": 112.31327056884766, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3752}}}
{"qid": "7225785319983716635", "question": "gabbar is back is a remake of which telugu movie", "gold_answers": ["Tagore"], "shots": {"0": {"k": 0, "answer": "Gabbar Is Back", "latency_ms": 86.91668510437012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 85.6781005859375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "Tagore", "latency_ms": 66.66183471679688, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1209}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 100.67224502563477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1860}, "4": {"k": 4, "answer": "Tagore", "latency_ms": 95.61491012573242, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2571}, "5": {"k": 5, "answer": "Tagore", "latency_ms": 99.7617244720459, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3232}, "6": {"k": 6, "answer": "Tagore", "latency_ms": 115.49830436706543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3904}}}
{"qid": "8466196474705624263", "question": "who was running as vice president in 1984", "gold_answers": ["Ferraro", "Vice President George H.W. Bush", "Congresswoman Ferraro", "George H.W. Bush"], "shots": {"0": {"k": 0, "answer": "John Baker", "latency_ms": 52.08873748779297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Geraldine Ferraro", "latency_ms": 93.22643280029297, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 682}, "2": {"k": 2, "answer": "Geraldine Ferraro", "latency_ms": 102.16283798217773, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1375}, "3": {"k": 3, "answer": "Geraldine Ferraro", "latency_ms": 118.02029609680176, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2070}, "4": {"k": 4, "answer": "Geraldine Ferraro", "latency_ms": 118.50214004516602, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2787}, "5": {"k": 5, "answer": "Geraldine Ferraro", "latency_ms": 134.3395709991455, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3497}, "6": {"k": 6, "answer": "Geraldine Ferraro", "latency_ms": 150.91753005981445, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 4186}}}
{"qid": "7711594393122809900", "question": "where is the largest salt mine in the us", "gold_answers": ["Central New York", "Western New York"], "shots": {"0": {"k": 0, "answer": "Utah", "latency_ms": 47.090768814086914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Hampton Corners, Town of Groveland", "latency_ms": 154.2031764984131, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Hampton Corners, New York", "latency_ms": 129.33659553527832, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1346}, "3": {"k": 3, "answer": "Hampton Corners, New York", "latency_ms": 141.36266708374023, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1953}, "4": {"k": 4, "answer": "Hampton Corners, New York", "latency_ms": 144.07944679260254, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 2585}, "5": {"k": 5, "answer": "Hampton Corners, New York", "latency_ms": 156.98981285095215, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3201}, "6": {"k": 6, "answer": "Hampton Corners, New York", "latency_ms": 163.61117362976074, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 3790}}}
{"qid": "-7011448454774306489", "question": "who took the first steps on the moon in 1969", "gold_answers": ["Neil Armstrong"], "shots": {"0": {"k": 0, "answer": "Neil Armstrong", "latency_ms": 51.079750061035156, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Neil Armstrong", "latency_ms": 58.73751640319824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 565}, "2": {"k": 2, "answer": "Neil Armstrong", "latency_ms": 63.317298889160156, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1164}, "3": {"k": 3, "answer": "Neil Armstrong", "latency_ms": 79.73909378051758, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1754}, "4": {"k": 4, "answer": "Neil Armstrong", "latency_ms": 84.80262756347656, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2371}, "5": {"k": 5, "answer": "Neil Armstrong", "latency_ms": 96.70591354370117, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2961}, "6": {"k": 6, "answer": "Neil Armstrong", "latency_ms": 101.93705558776855, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3623}}}
{"qid": "3125559316824073534", "question": "state with highest population growth rate in india 2011", "gold_answers": ["Dadra and Nagar Haveli", "Meghalaya"], "shots": {"0": {"k": 0, "answer": "Uttar Pradesh", "latency_ms": 70.24979591369629, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Bihar", "latency_ms": 61.4771842956543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "Meghalaya", "latency_ms": 79.09989356994629, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1277}, "3": {"k": 3, "answer": "Meghalaya", "latency_ms": 92.09346771240234, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1927}, "4": {"k": 4, "answer": "Meghalaya", "latency_ms": 113.7089729309082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2569}, "5": {"k": 5, "answer": "Meghalaya", "latency_ms": 129.61053848266602, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3223}, "6": {"k": 6, "answer": "Nagaland", "latency_ms": 144.49644088745117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3870}}}
{"qid": "400438603377436284", "question": "which city is called the manchester of north india", "gold_answers": ["Ahmedabad"], "shots": {"0": {"k": 0, "answer": "Lucknow", "latency_ms": 47.90663719177246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 86.07196807861328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 626}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 94.04325485229492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1233}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 105.14450073242188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1842}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 110.37325859069824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2541}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 126.7707347869873, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3196}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 136.79122924804688, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3877}}}
