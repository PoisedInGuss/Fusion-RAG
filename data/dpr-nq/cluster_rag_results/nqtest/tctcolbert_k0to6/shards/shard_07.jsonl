{"qid": "5306300389470118812", "question": "what was the religion in persia before islam", "gold_answers": ["the Zoroastrian religion", "Zoroastrian"], "shots": {"0": {"k": 0, "answer": "Zoroastrianism", "latency_ms": 142.78674125671387, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Zoroastrianism", "latency_ms": 183.3212375640869, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 636}, "2": {"k": 2, "answer": "Zoroastrianism", "latency_ms": 110.5051040649414, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1281}, "3": {"k": 3, "answer": "Zoroastrianism", "latency_ms": 222.77498245239258, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1920}, "4": {"k": 4, "answer": "Zoroastrianism", "latency_ms": 154.54339981079102, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 2670}, "5": {"k": 5, "answer": "Zoroastrianism", "latency_ms": 176.86820030212402, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3297}, "6": {"k": 6, "answer": "Zoroastrianism", "latency_ms": 144.84786987304688, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3901}}}
{"qid": "4492088526674104172", "question": "why does kerosene oil rise up in the wick of lantern", "gold_answers": ["capillary action"], "shots": {"0": {"k": 0, "answer": "Capillary action", "latency_ms": 115.19265174865723, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "capillary action", "latency_ms": 101.74274444580078, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 586}, "2": {"k": 2, "answer": "capillary action", "latency_ms": 152.1761417388916, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1208}, "3": {"k": 3, "answer": "capillary action", "latency_ms": 99.05362129211426, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1818}, "4": {"k": 4, "answer": "capillary action", "latency_ms": 214.95723724365234, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2433}, "5": {"k": 5, "answer": "capillary action", "latency_ms": 165.44032096862793, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3010}, "6": {"k": 6, "answer": "capillary action", "latency_ms": 224.18880462646484, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3668}}}
{"qid": "-4373262132142058334", "question": "mention the chemical change that proinsulin undergo to be able to act as mature insulin", "gold_answers": ["post translational modification"], "shots": {"0": {"k": 0, "answer": "Proteolytic cleavage", "latency_ms": 214.8151397705078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Proinsulin undergoes proteolytic cleavage to form mature insulin.", "latency_ms": 301.48863792419434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 702}, "2": {"k": 2, "answer": "C-peptide is cleaved", "latency_ms": 153.80573272705078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1349}, "3": {"k": 3, "answer": "C-peptide cleavage", "latency_ms": 156.55755996704102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1986}, "4": {"k": 4, "answer": "cleavage of C-peptide", "latency_ms": 204.0877342224121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2619}, "5": {"k": 5, "answer": "C-peptide cleavage", "latency_ms": 225.33369064331055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3227}, "6": {"k": 6, "answer": "C-peptide is cleaved", "latency_ms": 351.334810256958, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3978}}}
{"qid": "-5298560410953979569", "question": "where does the sweetness of fruit come from", "gold_answers": ["Fructose , or fruit sugar", "Fructose"], "shots": {"0": {"k": 0, "answer": "sugars like fructose, glucose, and sucrose", "latency_ms": 272.85051345825195, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 130.63788414001465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 671}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 143.0814266204834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1308}, "3": {"k": 3, "answer": "sugars", "latency_ms": 217.4544334411621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2013}, "4": {"k": 4, "answer": "sugars", "latency_ms": 139.6498680114746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2676}, "5": {"k": 5, "answer": "sugars", "latency_ms": 142.7001953125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3294}, "6": {"k": 6, "answer": "sugars", "latency_ms": 134.81807708740234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3920}}}
{"qid": "4344648368832551136", "question": "an object that moves around an external axis is said to be", "gold_answers": ["orbit", "revolution or orbital revolution"], "shots": {"0": {"k": 0, "answer": "rotating", "latency_ms": 76.64942741394043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "revolving", "latency_ms": 76.02500915527344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "revolving", "latency_ms": 70.06406784057617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1174}, "3": {"k": 3, "answer": "revolving", "latency_ms": 118.93582344055176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1791}, "4": {"k": 4, "answer": "revolving", "latency_ms": 134.45544242858887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2431}, "5": {"k": 5, "answer": "revolving", "latency_ms": 155.80463409423828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3040}, "6": {"k": 6, "answer": "revolving", "latency_ms": 182.15084075927734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3652}}}
{"qid": "-724426540432437711", "question": "what was the name of the plant on little shop of horrors", "gold_answers": ["Audrey II"], "shots": {"0": {"k": 0, "answer": "Audrey II", "latency_ms": 84.20395851135254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Audrey II", "latency_ms": 98.68264198303223, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 589}, "2": {"k": 2, "answer": "Audrey", "latency_ms": 71.03419303894043, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1205}, "3": {"k": 3, "answer": "Audrey II", "latency_ms": 117.32840538024902, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1908}, "4": {"k": 4, "answer": "The plant on Little Shop of Horrors is named **Audrey**.", "latency_ms": 541.1126613616943, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 2555}, "5": {"k": 5, "answer": "The plant on Little Shop of Horrors is named \"Audrey.\"", "latency_ms": 366.35541915893555, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 3197}, "6": {"k": 6, "answer": "The name of the plant on Little Shop of Horrors is not provided in the context. I cannot answer.", "latency_ms": 644.4575786590576, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3821}}}
{"qid": "-5541610970112826260", "question": "who signed the declaration of independence for maryland", "gold_answers": ["Samuel Chase", "Charles Carroll of Carrollton", "Thomas Stone", "William Paca"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 86.68088912963867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 179.19135093688965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 726}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 194.69141960144043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1463}, "3": {"k": 3, "answer": "Maryland delegates to the Second Continental Congress", "latency_ms": 270.17951011657715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2111}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 142.48371124267578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2856}, "5": {"k": 5, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 462.1109962463379, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 3576}, "6": {"k": 6, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 475.7552146911621, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 4221}}}
{"qid": "-86997217831791339", "question": "when does wonder woman come out on blue ray", "gold_answers": ["September 19 , 2017"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 79.40077781677246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 115.14782905578613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 90.27695655822754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1238}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 173.9816665649414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1882}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 159.73162651062012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2451}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.70199775695801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3050}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 156.04496002197266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3678}}}
{"qid": "-8062812506672279762", "question": "what nba team did derrick rose play for", "gold_answers": ["New York Knicks", "Minnesota Timberwolves", "Chicago Bulls", "Cleveland Cavaliers"], "shots": {"0": {"k": 0, "answer": "Chicago Bulls", "latency_ms": 66.88618659973145, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Minnesota Timberwolves", "latency_ms": 84.96356010437012, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 630}, "2": {"k": 2, "answer": "Minnesota Timberwolves", "latency_ms": 97.15127944946289, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1285}, "3": {"k": 3, "answer": "Minnesota Timberwolves", "latency_ms": 175.0476360321045, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1918}, "4": {"k": 4, "answer": "Minnesota Timberwolves", "latency_ms": 113.47770690917969, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2514}, "5": {"k": 5, "answer": "Chicago Bulls, Minnesota Timberwolves, New York Knicks, Cleveland Cavaliers", "latency_ms": 399.19471740722656, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3136}, "6": {"k": 6, "answer": "Chicago Bulls, Minnesota Timberwolves, New York Knicks, Cleveland Cavaliers", "latency_ms": 304.5082092285156, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3766}}}
{"qid": "7933002036740390435", "question": "where did the term liberal arts come from", "gold_answers": ["the Roman Empire"], "shots": {"0": {"k": 0, "answer": "Rome", "latency_ms": 71.43139839172363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "United States", "latency_ms": 61.32864952087402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 720}, "2": {"k": 2, "answer": "European origins", "latency_ms": 87.51201629638672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1415}, "3": {"k": 3, "answer": "European origins", "latency_ms": 82.08775520324707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2128}, "4": {"k": 4, "answer": "European origins", "latency_ms": 110.20445823669434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2862}, "5": {"k": 5, "answer": "European origins", "latency_ms": 171.06127738952637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3558}, "6": {"k": 6, "answer": "Roman Empire", "latency_ms": 136.27934455871582, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4177}}}
{"qid": "-2991400700385487821", "question": "who heads the executive department of west virginia government", "gold_answers": ["the Governor of West Virginia", "the governor of West Virginia", "Jim Justice"], "shots": {"0": {"k": 0, "answer": "Governor of West Virginia", "latency_ms": 181.3673973083496, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Governor of West Virginia", "latency_ms": 116.40000343322754, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 671}, "2": {"k": 2, "answer": "Governor of West Virginia", "latency_ms": 158.6935520172119, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1358}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 193.3755874633789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2083}, "4": {"k": 4, "answer": "Governor of West Virginia", "latency_ms": 273.8769054412842, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2770}, "5": {"k": 5, "answer": "Governor of West Virginia", "latency_ms": 163.25950622558594, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3426}, "6": {"k": 6, "answer": "Governor of West Virginia", "latency_ms": 191.01619720458984, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4118}}}
{"qid": "-9186689755642837558", "question": "how long is the bridge between new brunswick and prince edward island", "gold_answers": ["12.9 - kilometre"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 162.05263137817383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 200.71768760681152, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 670}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 169.07858848571777, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1405}, "3": {"k": 3, "answer": "world's longest bridge over ice-covered", "latency_ms": 268.5739994049072, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2092}, "4": {"k": 4, "answer": "world's longest bridge over ice-covered", "latency_ms": 302.0510673522949, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2815}, "5": {"k": 5, "answer": "world's longest bridge over ice-covered", "latency_ms": 373.8391399383545, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3503}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 158.41269493103027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4178}}}
{"qid": "8029901619424338449", "question": "when did the uk and us become allies", "gold_answers": ["1940", "Since 1940"], "shots": {"0": {"k": 0, "answer": "1941", "latency_ms": 115.26751518249512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 95.72124481201172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 583}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 96.71330451965332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1195}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 215.29722213745117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1846}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 157.43017196655273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2507}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 179.22687530517578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3141}, "6": {"k": 6, "answer": "World War II", "latency_ms": 138.08727264404297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3848}}}
{"qid": "4828649525820722736", "question": "who sang the theme song for the man with the golden gun", "gold_answers": ["Lulu"], "shots": {"0": {"k": 0, "answer": "Tommy Steele", "latency_ms": 74.47981834411621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lulu", "latency_ms": 86.12799644470215, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 582}, "2": {"k": 2, "answer": "Lulu", "latency_ms": 105.24439811706543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1225}, "3": {"k": 3, "answer": "Lulu", "latency_ms": 161.57007217407227, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1818}, "4": {"k": 4, "answer": "Lulu", "latency_ms": 137.42423057556152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2431}, "5": {"k": 5, "answer": "Lulu", "latency_ms": 168.87903213500977, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3088}, "6": {"k": 6, "answer": "Lulu", "latency_ms": 185.92166900634766, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3721}}}
{"qid": "-7924663090486742985", "question": "american horror story freak show girl kidnapped by clown", "gold_answers": ["Bonnie Lipton"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 147.86887168884277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Twisty", "latency_ms": 58.121442794799805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "Twisty", "latency_ms": 98.56653213500977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1352}, "3": {"k": 3, "answer": "Sarah Paulson", "latency_ms": 130.45525550842285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1955}, "4": {"k": 4, "answer": "Twisty", "latency_ms": 88.24348449707031, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2631}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 225.04186630249023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3191}, "6": {"k": 6, "answer": "Sarah Paulson", "latency_ms": 168.47920417785645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3793}}}
{"qid": "-360675773049242516", "question": "the probability of making a type i error when retaining ho at the .05 level of significance is", "gold_answers": ["5 %"], "shots": {"0": {"k": 0, "answer": "0.05", "latency_ms": 145.91383934020996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "0.05", "latency_ms": 155.4265022277832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 613}, "2": {"k": 2, "answer": "0.05", "latency_ms": 115.29970169067383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1257}, "3": {"k": 3, "answer": "0.05", "latency_ms": 183.09378623962402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1823}, "4": {"k": 4, "answer": "0.05", "latency_ms": 140.49935340881348, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2400}, "5": {"k": 5, "answer": "0.05", "latency_ms": 159.4829559326172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2998}, "6": {"k": 6, "answer": "0.05", "latency_ms": 164.98398780822754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3579}}}
{"qid": "5819386267283467034", "question": "what year did the us hockey team won the olympics", "gold_answers": ["1960", "1980", "1960 and 1980"], "shots": {"0": {"k": 0, "answer": "1980", "latency_ms": 130.25760650634766, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1960, 1980", "latency_ms": 259.0773105621338, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 619}, "2": {"k": 2, "answer": "1960, 1980", "latency_ms": 263.72790336608887, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1280}, "3": {"k": 3, "answer": "1960, 1980", "latency_ms": 353.17325592041016, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1906}, "4": {"k": 4, "answer": "1960, 1980", "latency_ms": 375.7514953613281, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2568}, "5": {"k": 5, "answer": "1960, 1980", "latency_ms": 362.490177154541, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3185}, "6": {"k": 6, "answer": "1960, 1980", "latency_ms": 353.6388874053955, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3847}}}
{"qid": "3916394566028297311", "question": "who wrote it's a long long way to pasadena", "gold_answers": ["Harry Vanda", "John Young", "David Hemmings", "George Young"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 108.93392562866211, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 203.60374450683594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 133.18562507629395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1194}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 138.54146003723145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1867}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 212.2962474822998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2454}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.66909599304199, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3104}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 236.13309860229492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3728}}}
{"qid": "-2432292250757146771", "question": "who wrote the song i hate you i love you", "gold_answers": ["Angel Benitez", "Olivia O'Brien", "Garrett Nash"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 98.00839424133301, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "StarGate", "latency_ms": 83.43696594238281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "StarGate", "latency_ms": 98.07991981506348, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1203}, "3": {"k": 3, "answer": "StarGate", "latency_ms": 81.95757865905762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1760}, "4": {"k": 4, "answer": "StarGate", "latency_ms": 88.22274208068848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2422}, "5": {"k": 5, "answer": "StarGate", "latency_ms": 164.48688507080078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2969}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 150.90656280517578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3578}}}
{"qid": "2393389448844632496", "question": "what is loihi and where does it originate from", "gold_answers": ["from the Hawaii hotspot"], "shots": {"0": {"k": 0, "answer": "Loihi is a seamount in the Pacific Ocean, originating from the Hawaiian hotspot.", "latency_ms": 430.2535057067871, "em": 0.0, "f1": 0.30769230769230765, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 123.48556518554688, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 119.23670768737793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1260}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 109.56692695617676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1871}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 213.4101390838623, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2494}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 149.72519874572754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3103}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 189.56637382507324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3695}}}
{"qid": "-6677440871585683891", "question": "who rebuilt the temple after the babylonian captivity", "gold_answers": ["Herod"], "shots": {"0": {"k": 0, "answer": "Zerubbabel", "latency_ms": 114.06874656677246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cyrus the Great", "latency_ms": 202.69060134887695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Zerubbabel", "latency_ms": 165.97461700439453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1272}, "3": {"k": 3, "answer": "Zerubbabel", "latency_ms": 124.09114837646484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1884}, "4": {"k": 4, "answer": "Zerubbabel", "latency_ms": 158.07199478149414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2485}, "5": {"k": 5, "answer": "Zerubbabel", "latency_ms": 121.11091613769531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3103}, "6": {"k": 6, "answer": "Cyrus the Great", "latency_ms": 226.3035774230957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3700}}}
{"qid": "-6750750235985613277", "question": "who was executed for being an american spy during the revolutionary war", "gold_answers": ["Nathan Hale"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 102.84566879272461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 95.64447402954102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 602}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 258.3961486816406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1225}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 161.54766082763672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1987}, "4": {"k": 4, "answer": "Nathan Hale", "latency_ms": 99.52807426452637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2582}, "5": {"k": 5, "answer": "Nathan Hale", "latency_ms": 181.22482299804688, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3275}, "6": {"k": 6, "answer": "Nathan Hale", "latency_ms": 132.73358345031738, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3897}}}
{"qid": "8770209312170080158", "question": "who sang the song suddenly with olivia newton john", "gold_answers": ["Cliff Richard"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 74.37348365783691, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cliff Richard", "latency_ms": 145.83730697631836, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "Cliff Richard", "latency_ms": 119.50063705444336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1270}, "3": {"k": 3, "answer": "Cliff Richard", "latency_ms": 95.94511985778809, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1852}, "4": {"k": 4, "answer": "Cliff Richard", "latency_ms": 114.09235000610352, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2494}, "5": {"k": 5, "answer": "Cliff Richard", "latency_ms": 183.18581581115723, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3136}, "6": {"k": 6, "answer": "Cliff Richard", "latency_ms": 130.1271915435791, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3769}}}
{"qid": "-180549795106449014", "question": "who is command sergeant major of the army", "gold_answers": ["Daniel A. Dailey"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 162.3225212097168, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Sergeant Major of the Army", "latency_ms": 125.20432472229004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 637}, "2": {"k": 2, "answer": "Sergeant Major of the Army", "latency_ms": 227.71477699279785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1261}, "3": {"k": 3, "answer": "Sergeant Major of the Army", "latency_ms": 258.93592834472656, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1898}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 110.1992130279541, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2478}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 194.8535442352295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3130}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 143.99290084838867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3716}}}
{"qid": "7815138213728640354", "question": "who sings the original windmills of your mind", "gold_answers": ["Noel Harrison"], "shots": {"0": {"k": 0, "answer": "The Kinks", "latency_ms": 165.6191349029541, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Frank Sinatra", "latency_ms": 110.83459854125977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "The song \"The Windmills of Your Mind\" was introduced in the film *The Thomas Crown Affair* (1968), and the original version was performed by **Frank Sinatra**. However, based on the provided context, the original performer is not explicitly named.\n\nI cannot answer.", "latency_ms": 2072.105646133423, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "The Thomas Crown Affair (1968)", "latency_ms": 386.54637336730957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1873}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 146.71778678894043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2479}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 224.87497329711914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3071}, "6": {"k": 6, "answer": "Noel Harrison", "latency_ms": 130.62500953674316, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3752}}}
{"qid": "2098168902147822379", "question": "where will the next summer and winter olympics be held", "gold_answers": ["Beijing", "Tokyo"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 158.70213508605957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 126.2369155883789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 648}, "2": {"k": 2, "answer": "Abu Dhabi, United Arab Emirates (Summer); Austria (Winter)", "latency_ms": 552.4756908416748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1241}, "3": {"k": 3, "answer": "Abu Dhabi, United Arab Emirates (Summer); Beijing, China (Winter)", "latency_ms": 464.14923667907715, "em": 0.0, "f1": 0.2, "containment": 1.0, "context_chars": 1912}, "4": {"k": 4, "answer": "2028 Summer Olympics in Los Angeles, 2030 Winter Olympics in unknown location", "latency_ms": 600.6729602813721, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2495}, "5": {"k": 5, "answer": "2028 Summer Olympics in Los Angeles, 2030 Winter Olympics in unknown location", "latency_ms": 507.7240467071533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3182}, "6": {"k": 6, "answer": "2024 Paris (Summer), 2028 Los Angeles (Summer), 2030 (Winter to be announced)", "latency_ms": 839.8313522338867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3815}}}
{"qid": "-1458476052377668874", "question": "when did ole miss beat alabama in football", "gold_answers": ["September 19 , 2015", "October 3 , 1970", "October 27 , 1894", "October 13 , 2001", "October 5 , 1968", "October 4 , 2014", "September 11 , 1976", "October 8 , 1988", "October 18 , 2003", "November 5 , 1910"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 83.82153511047363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "the game where the Rebels never trailed and led by as many as twenty points and as many as nineteen in the fourth quarter.", "latency_ms": 687.1483325958252, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 588}, "2": {"k": 2, "answer": "1988, 2014", "latency_ms": 233.3080768585205, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1223}, "3": {"k": 3, "answer": "1988, 2014", "latency_ms": 264.10436630249023, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1819}, "4": {"k": 4, "answer": "2014", "latency_ms": 127.471923828125, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2456}, "5": {"k": 5, "answer": "2014", "latency_ms": 181.64610862731934, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3016}, "6": {"k": 6, "answer": "2014", "latency_ms": 188.99130821228027, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3614}}}
{"qid": "-6861734002351236912", "question": "who sang the song how far is heaven", "gold_answers": ["Los Lonely Boys"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 102.67782211303711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 109.81249809265137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 596}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 148.23222160339355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1233}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 176.1763095855713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1829}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 159.00063514709473, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2422}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 216.91536903381348, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3086}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 238.7709617614746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3751}}}
{"qid": "-1722216190521937227", "question": "where are the organ mountains in new mexico", "gold_answers": ["southern New Mexico"], "shots": {"0": {"k": 0, "answer": "Organ Mountains are located in southern New Mexico, near the border with Arizona.", "latency_ms": 364.32957649230957, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 347.8116989135742, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 606}, "2": {"k": 2, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 472.86224365234375, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1267}, "3": {"k": 3, "answer": "southern New Mexico, east of Las Cruces, in Doña Ana County", "latency_ms": 347.6090431213379, "em": 0.0, "f1": 0.42857142857142855, "containment": 1.0, "context_chars": 1924}, "4": {"k": 4, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 404.0811061859131, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 2564}, "5": {"k": 5, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 412.70995140075684, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3220}, "6": {"k": 6, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 459.0435028076172, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3939}}}
{"qid": "-2717119588239727064", "question": "where are the 10 plagues found in the bible", "gold_answers": ["Book of Exodus", "Exodus"], "shots": {"0": {"k": 0, "answer": "Exodus 7–12", "latency_ms": 195.6198215484619, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Exodus 7–12", "latency_ms": 209.86557006835938, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 666}, "2": {"k": 2, "answer": "Book of Exodus", "latency_ms": 112.78128623962402, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1313}, "3": {"k": 3, "answer": "Book of Exodus", "latency_ms": 113.5859489440918, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1947}, "4": {"k": 4, "answer": "Book of Exodus", "latency_ms": 102.59318351745605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2583}, "5": {"k": 5, "answer": "Book of Exodus", "latency_ms": 161.16976737976074, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3248}, "6": {"k": 6, "answer": "Book of Exodus", "latency_ms": 169.50178146362305, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3843}}}
{"qid": "2988284357714596500", "question": "where are red blood cells made in adults", "gold_answers": ["in the bone marrow"], "shots": {"0": {"k": 0, "answer": "bone marrow", "latency_ms": 171.00214958190918, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "bone marrow", "latency_ms": 58.808088302612305, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 614}, "2": {"k": 2, "answer": "bone marrow", "latency_ms": 132.71856307983398, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1221}, "3": {"k": 3, "answer": "bone marrow", "latency_ms": 100.57258605957031, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1821}, "4": {"k": 4, "answer": "bone marrow", "latency_ms": 91.29452705383301, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2473}, "5": {"k": 5, "answer": "bone marrow", "latency_ms": 131.26254081726074, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3069}, "6": {"k": 6, "answer": "bone marrow", "latency_ms": 125.69427490234375, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3756}}}
{"qid": "-4616596799374362422", "question": "who was the pinkerton detective agency's first female detective", "gold_answers": ["Kate Warne"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 149.86586570739746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kate Warne", "latency_ms": 113.6314868927002, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 681}, "2": {"k": 2, "answer": "Kate Warne", "latency_ms": 136.12890243530273, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1401}, "3": {"k": 3, "answer": "Kate Warne", "latency_ms": 162.02235221862793, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2065}, "4": {"k": 4, "answer": "Kate Warne", "latency_ms": 180.495023727417, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2743}, "5": {"k": 5, "answer": "Kate Warne", "latency_ms": 201.6432285308838, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3453}, "6": {"k": 6, "answer": "Kate Warne", "latency_ms": 118.72529983520508, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4064}}}
{"qid": "-3650291155113659146", "question": "how many episodes are there in modern family", "gold_answers": ["232", "201"], "shots": {"0": {"k": 0, "answer": "216 episodes", "latency_ms": 79.77914810180664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 84.99407768249512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 607}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 98.48308563232422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1278}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 130.1586627960205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1882}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 165.30585289001465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2509}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 157.8199863433838, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3155}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 168.76888275146484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3792}}}
{"qid": "3381924381590631417", "question": "who built the first temple for god in jerusalem", "gold_answers": ["Solomon"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 172.53375053405762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "King Solomon", "latency_ms": 114.79783058166504, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 634}, "2": {"k": 2, "answer": "Solomon", "latency_ms": 99.74169731140137, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1260}, "3": {"k": 3, "answer": "Solomon", "latency_ms": 103.84273529052734, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1849}, "4": {"k": 4, "answer": "Solomon", "latency_ms": 101.56750679016113, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2449}, "5": {"k": 5, "answer": "Solomon", "latency_ms": 167.44256019592285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3071}, "6": {"k": 6, "answer": "Solomon", "latency_ms": 104.33244705200195, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3680}}}
{"qid": "-154783694579651082", "question": "what is a dropped pin on google maps for", "gold_answers": ["marks locations", "marks locations in Google Maps"], "shots": {"0": {"k": 0, "answer": "A dropped pin on Google Maps typically indicates a user-created location marker, such as a place of interest, a personal location, or a point of significance.", "latency_ms": 601.7396450042725, "em": 0.0, "f1": 0.15384615384615385, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "a dropped pin on Google Maps marks a specific location on the map", "latency_ms": 303.2064437866211, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 574}, "2": {"k": 2, "answer": "to accurately mark a point on a map without obscuring the location", "latency_ms": 306.37121200561523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "to accurately mark a point on a map without obscuring the location", "latency_ms": 399.0917205810547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1937}, "4": {"k": 4, "answer": "a location marker", "latency_ms": 96.67634963989258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2531}, "5": {"k": 5, "answer": "a location marker", "latency_ms": 165.50350189208984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3165}, "6": {"k": 6, "answer": "a location marker", "latency_ms": 185.6062412261963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3760}}}
{"qid": "6915606477668963399", "question": "what do the 3 dots mean in math", "gold_answers": ["the therefore sign", "therefore sign"], "shots": {"0": {"k": 0, "answer": "ellipses", "latency_ms": 67.3379898071289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "ellipsis", "latency_ms": 119.9960708618164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 570}, "2": {"k": 2, "answer": "ellipsis", "latency_ms": 51.47981643676758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1178}, "3": {"k": 3, "answer": "Square root of 3", "latency_ms": 140.14315605163574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1837}, "4": {"k": 4, "answer": "square root of 3", "latency_ms": 169.6469783782959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2490}, "5": {"k": 5, "answer": "square root of 3", "latency_ms": 166.27120971679688, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3044}, "6": {"k": 6, "answer": "square root of 3", "latency_ms": 179.3534755706787, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3664}}}
{"qid": "-8366545547296627039", "question": "who wrote the song photograph by ringo starr", "gold_answers": ["Richard Starkey", "George Harrison", "Ringo Starr"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 92.33689308166504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ringo Starr and George Harrison", "latency_ms": 251.14989280700684, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 671}, "2": {"k": 2, "answer": "Ringo Starr and George Harrison", "latency_ms": 140.73920249938965, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1329}, "3": {"k": 3, "answer": "Ringo Starr and George Harrison", "latency_ms": 197.21007347106934, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1984}, "4": {"k": 4, "answer": "Ringo Starr and George Harrison", "latency_ms": 185.75763702392578, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2640}, "5": {"k": 5, "answer": "George Harrison", "latency_ms": 152.03046798706055, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3250}, "6": {"k": 6, "answer": "George Harrison", "latency_ms": 138.43679428100586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3865}}}
{"qid": "-5004457603684974952", "question": "who is playing the halftime show at super bowl 2016", "gold_answers": ["Beyoncé", "Coldplay", "Bruno Mars"], "shots": {"0": {"k": 0, "answer": "Bruno Mars", "latency_ms": 88.73581886291504, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Coldplay", "latency_ms": 145.19000053405762, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 634}, "2": {"k": 2, "answer": "Coldplay", "latency_ms": 86.77172660827637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1274}, "3": {"k": 3, "answer": "Coldplay", "latency_ms": 84.52987670898438, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1868}, "4": {"k": 4, "answer": "Coldplay", "latency_ms": 86.10939979553223, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2494}, "5": {"k": 5, "answer": "Coldplay", "latency_ms": 101.41873359680176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3141}, "6": {"k": 6, "answer": "Coldplay", "latency_ms": 105.2253246307373, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3774}}}
{"qid": "7420920477821898435", "question": "to aru kagaku no railgun s episode 3", "gold_answers": ["Project Radio Noise"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 79.68306541442871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 90.29221534729004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 666}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 97.78022766113281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1331}, "3": {"k": 3, "answer": "2010", "latency_ms": 112.55145072937012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1929}, "4": {"k": 4, "answer": "A Certain Scientific Railgun S episode 3", "latency_ms": 191.99800491333008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2546}, "5": {"k": 5, "answer": "A Certain Scientific Railgun S episode 3", "latency_ms": 196.75779342651367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3132}, "6": {"k": 6, "answer": "A Certain Scientific Railgun S", "latency_ms": 170.92466354370117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3782}}}
{"qid": "7478795216476346339", "question": "who won the 2017 sports personality of the year", "gold_answers": ["Mo Farah"], "shots": {"0": {"k": 0, "answer": "Tayler Hill", "latency_ms": 79.41865921020508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 90.66295623779297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 669}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 96.66681289672852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1357}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 109.87305641174316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1984}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 112.19215393066406, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2644}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 129.1210651397705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3308}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 130.81908226013184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3975}}}
{"qid": "7217222058435937287", "question": "where was the world economic forum held this year", "gold_answers": ["Davos"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 79.97608184814453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 99.30038452148438, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 89.43462371826172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1362}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 109.18927192687988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 112.7018928527832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2691}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.14290618896484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3298}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 130.0346851348877, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3999}}}
{"qid": "-143054837169120955", "question": "where are the giant redwoods located in california", "gold_answers": ["the coast of northern California", "Del Norte County", "Humboldt County"], "shots": {"0": {"k": 0, "answer": "Redwood National and State Parks", "latency_ms": 106.81724548339844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 222.426176071167, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "Kings Canyon National Park and Giant Sequoia National Monument on the western slope of California's Sierra Nevada", "latency_ms": 364.47715759277344, "em": 0.0, "f1": 0.1, "containment": 0.0, "context_chars": 1281}, "3": {"k": 3, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 270.0021266937256, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1869}, "4": {"k": 4, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 368.73459815979004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2512}, "5": {"k": 5, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 408.08629989624023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3124}, "6": {"k": 6, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 272.45473861694336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3807}}}
{"qid": "4892429640540595424", "question": "who has made the most premier league appearances", "gold_answers": ["Gareth Barry"], "shots": {"0": {"k": 0, "answer": "Alan Shearer", "latency_ms": 67.43025779724121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 127.31575965881348, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 147.77016639709473, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1203}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 175.201416015625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1794}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 183.7475299835205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2406}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 213.93203735351562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3062}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 143.20039749145508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3680}}}
{"qid": "5533906981191706877", "question": "who was the first chief minister of west bengal", "gold_answers": ["Prafulla Chandra Ghosh"], "shots": {"0": {"k": 0, "answer": "P. C. Bagchi", "latency_ms": 107.15126991271973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 107.00798034667969, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 666}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 91.63498878479004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1340}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 98.89101982116699, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1977}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 113.70706558227539, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2637}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.11524963378906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3304}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 130.21588325500488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3971}}}
{"qid": "8823076125131408628", "question": "who sings got my mind set on you", "gold_answers": ["George Harrison", "James Ray"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 195.38211822509766, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "George Harrison", "latency_ms": 58.447837829589844, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 541}, "2": {"k": 2, "answer": "George Harrison", "latency_ms": 71.99311256408691, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1098}, "3": {"k": 3, "answer": "James Ray", "latency_ms": 84.19680595397949, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1659}, "4": {"k": 4, "answer": "James Ray", "latency_ms": 90.38090705871582, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2249}, "5": {"k": 5, "answer": "James Ray", "latency_ms": 102.55718231201172, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2840}, "6": {"k": 6, "answer": "James Ray, George Harrison, Shakin' Stevens, Lee Matthews", "latency_ms": 312.5171661376953, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3427}}}
{"qid": "9168660859139861094", "question": "two main types of research methods used in psychology", "gold_answers": ["Correlational research", "Descriptive research", "quantitative data", "Quantitative psychological research", "qualitative", "quantitative", "Qualitative psychological research", "Experimental research", "qualitative data"], "shots": {"0": {"k": 0, "answer": "experimental and observational", "latency_ms": 70.47367095947266, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "qualitative, quantitative", "latency_ms": 86.07125282287598, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 735}, "2": {"k": 2, "answer": "qualitative and quantitative", "latency_ms": 147.48072624206543, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1482}, "3": {"k": 3, "answer": "qualitative and quantitative", "latency_ms": 99.06649589538574, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2166}, "4": {"k": 4, "answer": "qualitative and quantitative", "latency_ms": 128.71623039245605, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2924}, "5": {"k": 5, "answer": "qualitative and quantitative", "latency_ms": 149.43814277648926, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3624}, "6": {"k": 6, "answer": "qualitative and quantitative", "latency_ms": 160.7043743133545, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 4326}}}
{"qid": "-7073063191952855764", "question": "who plays peter in what we do in the shadows", "gold_answers": ["Ben Fransham"], "shots": {"0": {"k": 0, "answer": "Matt Berry", "latency_ms": 117.53106117248535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 223.1764793395996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 627}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 179.34846878051758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1196}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 127.79998779296875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1835}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 141.08943939208984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2493}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 204.86211776733398, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3102}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 258.91685485839844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3703}}}
{"qid": "7901746249864619718", "question": "when does the 14th season of grey's anatomy come out", "gold_answers": ["September 28 , 2017"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 243.39032173156738, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 28, 2017", "latency_ms": 199.06926155090332, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "September 28, 2017", "latency_ms": 257.1876049041748, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1364}, "3": {"k": 3, "answer": "September 28, 2017", "latency_ms": 352.0979881286621, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2007}, "4": {"k": 4, "answer": "September 28, 2017", "latency_ms": 407.4568748474121, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2695}, "5": {"k": 5, "answer": "September 28, 2017", "latency_ms": 343.1696891784668, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3363}, "6": {"k": 6, "answer": "September 28, 2017", "latency_ms": 357.50460624694824, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4005}}}
{"qid": "-896127746191949637", "question": "when was the first hunger games book published", "gold_answers": ["September 14 , 2008", "2008"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 138.53931427001953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 14, 2008", "latency_ms": 404.27279472351074, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 670}, "2": {"k": 2, "answer": "September 14, 2008", "latency_ms": 274.26815032958984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "September 14, 2008", "latency_ms": 318.61186027526855, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1949}, "4": {"k": 4, "answer": "September 14, 2008", "latency_ms": 377.05111503601074, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2609}, "5": {"k": 5, "answer": "September 14, 2008", "latency_ms": 341.2001132965088, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3258}, "6": {"k": 6, "answer": "September 14, 2008", "latency_ms": 320.65725326538086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3913}}}
{"qid": "8044909700499569711", "question": "how much is the united states in debt to china", "gold_answers": ["$1.06 trillion", "1,200.5"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 92.37551689147949, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 183.60114097595215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 642}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 165.7881736755371, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1286}, "3": {"k": 3, "answer": "10 percent of total U.S. public debt", "latency_ms": 228.11532020568848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1918}, "4": {"k": 4, "answer": "$1.16 trillion", "latency_ms": 220.69811820983887, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2566}, "5": {"k": 5, "answer": "$1.26 trillion", "latency_ms": 236.45687103271484, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3186}, "6": {"k": 6, "answer": "$1.26 trillion", "latency_ms": 235.1837158203125, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3834}}}
{"qid": "-3123234845590594961", "question": "where did the race in grease take place", "gold_answers": ["Thunder Road"], "shots": {"0": {"k": 0, "answer": "California", "latency_ms": 166.3534641265869, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Rydell High School", "latency_ms": 120.93329429626465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 662}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 126.495361328125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1222}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 176.92327499389648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1826}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 186.04755401611328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2493}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 192.57783889770508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3121}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 201.02643966674805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3685}}}
{"qid": "4326992806019599297", "question": "what is the maximum data rate for the 802.11a standard select one", "gold_answers": ["54 Mbit / s"], "shots": {"0": {"k": 0, "answer": "54 Mbps", "latency_ms": 232.17058181762695, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 108.11638832092285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 674}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 140.5317783355713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1336}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 149.5800018310547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2029}, "4": {"k": 4, "answer": "54 Mbit/s", "latency_ms": 185.84942817687988, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2659}, "5": {"k": 5, "answer": "54 Mbit/s", "latency_ms": 268.9335346221924, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3346}, "6": {"k": 6, "answer": "54 Mbit/s", "latency_ms": 227.9987335205078, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3971}}}
{"qid": "6144099837933283715", "question": "a good that can be used in place of another good", "gold_answers": ["A substitute good", "substitute good"], "shots": {"0": {"k": 0, "answer": "substitute good", "latency_ms": 88.57297897338867, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "substitute good", "latency_ms": 88.96136283874512, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 577}, "2": {"k": 2, "answer": "substitute good", "latency_ms": 99.48277473449707, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1200}, "3": {"k": 3, "answer": "substitute good", "latency_ms": 114.43400382995605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1842}, "4": {"k": 4, "answer": "substitute good", "latency_ms": 164.2751693725586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2405}, "5": {"k": 5, "answer": "substitute good", "latency_ms": 147.4597454071045, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3022}, "6": {"k": 6, "answer": "substitute good", "latency_ms": 202.89993286132812, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3651}}}
{"qid": "-4621948696356225060", "question": "where in the bible can i find the lord's prayer", "gold_answers": ["the Gospel of Luke", "the Gospel of Matthew"], "shots": {"0": {"k": 0, "answer": "Matthew 6:9-13", "latency_ms": 187.71839141845703, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 82.89337158203125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 562}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 173.3231544494629, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1230}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 141.75701141357422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1882}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 156.28838539123535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2507}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 232.36656188964844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3202}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 187.1473789215088, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3837}}}
{"qid": "6941993254369956277", "question": "when was the last time nebraska voted democrat", "gold_answers": ["1964"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 86.6098403930664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1936", "latency_ms": 108.96754264831543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 692}, "2": {"k": 2, "answer": "1936", "latency_ms": 180.6502342224121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1374}, "3": {"k": 3, "answer": "1996", "latency_ms": 156.20660781860352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2059}, "4": {"k": 4, "answer": "2008", "latency_ms": 138.82875442504883, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2761}, "5": {"k": 5, "answer": "1996", "latency_ms": 174.84068870544434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3367}, "6": {"k": 6, "answer": "1990", "latency_ms": 220.02124786376953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4046}}}
{"qid": "-8955197065025093046", "question": "who become the ceo of it wipro company in 2016", "gold_answers": ["Abid Ali Neemuchwala"], "shots": {"0": {"k": 0, "answer": "Ajay Bhatia", "latency_ms": 128.5076141357422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wipro CEO in 2016: [Name not provided in context]", "latency_ms": 423.39396476745605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 682}, "2": {"k": 2, "answer": "COO from April 2015", "latency_ms": 281.19516372680664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1322}, "3": {"k": 3, "answer": "COO from April 2015, was appointed CEO with effect from 1 February 2016.", "latency_ms": 936.5062713623047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1928}, "4": {"k": 4, "answer": "COO from April 2015, was appointed CEO with effect from 1 February 2016.", "latency_ms": 1011.0838413238525, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2593}, "5": {"k": 5, "answer": "COO from April 2015, was appointed CEO with effect from 1 February 2016.", "latency_ms": 516.0188674926758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3182}, "6": {"k": 6, "answer": "COO from April 2015, was appointed CEO with effect from 1 February 2016.", "latency_ms": 541.7120456695557, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3818}}}
{"qid": "-3010425579521817304", "question": "who does eric end up with in gossip girl", "gold_answers": ["Jenny"], "shots": {"0": {"k": 0, "answer": "Serena", "latency_ms": 53.71975898742676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 84.20014381408691, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 575}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 90.61861038208008, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1186}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 122.8632926940918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1812}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 121.43802642822266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2395}, "5": {"k": 5, "answer": "Laura", "latency_ms": 74.39351081848145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2998}, "6": {"k": 6, "answer": "Laura", "latency_ms": 88.87028694152832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3604}}}
{"qid": "7264112453286983469", "question": "who plays unis in she's the man", "gold_answers": ["Emily Perkins"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 79.97322082519531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 84.72275733947754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 613}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 96.64177894592285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "Amanda Bynes", "latency_ms": 108.98709297180176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1791}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 178.56502532958984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2417}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 127.64477729797363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2969}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 160.60853004455566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3588}}}
{"qid": "2627695648792729859", "question": "who played booster in jingle all the way", "gold_answers": ["Curtis Armstrong"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 79.96511459350586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 91.80212020874023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 652}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 97.15151786804199, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1310}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 179.3222427368164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1973}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 113.92378807067871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2606}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 129.31180000305176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3274}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 142.0004367828369, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3894}}}
{"qid": "-9169976182548289414", "question": "who is recognized as the founder of islam", "gold_answers": ["the Islamic prophet Muhammad", "Muhammad"], "shots": {"0": {"k": 0, "answer": "Muhammad", "latency_ms": 53.69448661804199, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Muhammad", "latency_ms": 58.70461463928223, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 603}, "2": {"k": 2, "answer": "Muhammad", "latency_ms": 71.55585289001465, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1211}, "3": {"k": 3, "answer": "Muhammad", "latency_ms": 134.53340530395508, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1822}, "4": {"k": 4, "answer": "Muhammad", "latency_ms": 102.51998901367188, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2455}, "5": {"k": 5, "answer": "Muhammad", "latency_ms": 99.23100471496582, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3065}, "6": {"k": 6, "answer": "Muhammad", "latency_ms": 122.82109260559082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3683}}}
{"qid": "-6696747560626271522", "question": "who plays emma in air bud world pup", "gold_answers": ["Brittany Paige Bouck"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 83.52470397949219, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 127.47001647949219, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 143.27454566955566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1155}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 159.13867950439453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1768}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 181.97178840637207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2360}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 129.48846817016602, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2974}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 146.05164527893066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3596}}}
{"qid": "5871681964137793716", "question": "how old was sasuke when his clan died", "gold_answers": ["seven"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 98.26374053955078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 88.43159675598145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "7", "latency_ms": 92.57364273071289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1229}, "3": {"k": 3, "answer": "7", "latency_ms": 70.43313980102539, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1835}, "4": {"k": 4, "answer": "7", "latency_ms": 125.05626678466797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2429}, "5": {"k": 5, "answer": "7", "latency_ms": 89.07961845397949, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3097}, "6": {"k": 6, "answer": "7", "latency_ms": 171.56696319580078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3703}}}
{"qid": "8552176802923865025", "question": "where do some of the natural air pollutants come from", "gold_answers": ["wildfires", "Dust", "Vegetation", "Carbon dioxide", "Volcanic activity"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 162.91499137878418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "volcanic eruptions", "latency_ms": 198.99487495422363, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "volcanic eruptions", "latency_ms": 138.03386688232422, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1333}, "3": {"k": 3, "answer": "volcanic eruptions", "latency_ms": 158.86449813842773, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2028}, "4": {"k": 4, "answer": "volcanic eruptions", "latency_ms": 240.45777320861816, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2728}, "5": {"k": 5, "answer": "volcanic eruptions", "latency_ms": 209.48052406311035, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3457}, "6": {"k": 6, "answer": "volcanic eruptions", "latency_ms": 160.12048721313477, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 4105}}}
{"qid": "-2630525064809360052", "question": "who is the pastor of new birth missionary baptist church now", "gold_answers": ["Stephen A. Davis"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 78.9947509765625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 113.15584182739258, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 675}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 185.93764305114746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1340}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 157.81569480895996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1976}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 171.76270484924316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2653}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 208.44221115112305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3389}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 158.47015380859375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4029}}}
{"qid": "6434931911289860123", "question": "who is doing 2018 super bowl half time show", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Mark Ronson", "latency_ms": 68.71652603149414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Mark Ronson", "latency_ms": 71.64740562438965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 613}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 119.81892585754395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1235}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 193.39370727539062, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1803}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 192.27123260498047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2408}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 209.5320224761963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2986}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 248.17514419555664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3607}}}
{"qid": "8869318258439148973", "question": "who is playing the halftime show for the superbowl", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 165.58337211608887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 113.8143539428711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 237.7171516418457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1245}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 217.51666069030762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1916}, "4": {"k": 4, "answer": "world-renowned music stars", "latency_ms": 220.80445289611816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2542}, "5": {"k": 5, "answer": "world-renowned music stars", "latency_ms": 154.56175804138184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3150}, "6": {"k": 6, "answer": "world-renowned music stars", "latency_ms": 210.8898162841797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3790}}}
{"qid": "-8359866474144720253", "question": "who hosted they think it's all over", "gold_answers": ["Des Lynam", "Lee Mack", "comedian Nick Hancock", "Nick Hancock"], "shots": {"0": {"k": 0, "answer": "They Think It's All Over was hosted by Bob Barker.", "latency_ms": 243.63303184509277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Nick Hancock", "latency_ms": 74.09477233886719, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 632}, "2": {"k": 2, "answer": "Nick Hancock", "latency_ms": 71.20466232299805, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1278}, "3": {"k": 3, "answer": "Nick Hancock", "latency_ms": 101.1500358581543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1859}, "4": {"k": 4, "answer": "Nick Hancock", "latency_ms": 137.19773292541504, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2448}, "5": {"k": 5, "answer": "Nick Hancock", "latency_ms": 145.68257331848145, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3004}, "6": {"k": 6, "answer": "Nick Hancock", "latency_ms": 143.157958984375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3602}}}
{"qid": "-1206653570097564556", "question": "where does the movie proof of life take place", "gold_answers": ["The Republic of Tecala"], "shots": {"0": {"k": 0, "answer": "India", "latency_ms": 87.7385139465332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 221.10724449157715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 192.22426414489746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1299}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 219.24257278442383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1948}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 148.1633186340332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2560}, "5": {"k": 5, "answer": "not specified in the context", "latency_ms": 207.09228515625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3194}, "6": {"k": 6, "answer": "not specified in the context", "latency_ms": 223.00982475280762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3826}}}
{"qid": "6144521615402262404", "question": "where does us highway 1 start and end", "gold_answers": ["Fort Kent , Maine", "to Key West , Florida", "Key West , Florida"], "shots": {"0": {"k": 0, "answer": "U.S. Highway 1 starts in Miami, Florida, and ends in Bar Harbor, Maine.", "latency_ms": 562.4852180480957, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 90.40331840515137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 96.43816947937012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1204}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 212.21470832824707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1859}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 185.50491333007812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2462}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 174.2079257965088, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3078}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 201.31969451904297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3648}}}
{"qid": "-8631190913794477985", "question": "who scored the most points in a single game in the nba", "gold_answers": ["Wilt Chamberlain"], "shots": {"0": {"k": 0, "answer": "Wilt Chamberlain", "latency_ms": 121.85406684875488, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 101.66382789611816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 556}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 85.69836616516113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1110}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 148.50497245788574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1661}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 139.74595069885254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2262}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 128.13830375671387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2875}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 176.30529403686523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3440}}}
{"qid": "199896377614951764", "question": "where is natural gas found in the us", "gold_answers": ["Texas", "Pennsylvania", "Colorado", "New Mexico", "West Virginia", "Arkansas", "Wyoming", "Louisiana"], "shots": {"0": {"k": 0, "answer": "Texas, North Dakota, Wyoming, Pennsylvania, and Utah", "latency_ms": 191.40243530273438, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "shale formations", "latency_ms": 107.12194442749023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "shale formations", "latency_ms": 93.5211181640625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1253}, "3": {"k": 3, "answer": "shale formations", "latency_ms": 131.64854049682617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1837}, "4": {"k": 4, "answer": "shale formations", "latency_ms": 99.98822212219238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2432}, "5": {"k": 5, "answer": "shale formations", "latency_ms": 169.91138458251953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3056}, "6": {"k": 6, "answer": "shale formations", "latency_ms": 131.37388229370117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3747}}}
{"qid": "8986775680996674901", "question": "when were the first general elections held in independent india", "gold_answers": ["1951 -- 52"], "shots": {"0": {"k": 0, "answer": "15 August 1947", "latency_ms": 318.55058670043945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1951", "latency_ms": 157.99403190612793, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 634}, "2": {"k": 2, "answer": "1951", "latency_ms": 127.49218940734863, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": "1951–52", "latency_ms": 148.48589897155762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1955}, "4": {"k": 4, "answer": "1951–52", "latency_ms": 154.04343605041504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2596}, "5": {"k": 5, "answer": "1951–52", "latency_ms": 270.44177055358887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3276}, "6": {"k": 6, "answer": "1951–52", "latency_ms": 190.6123161315918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3937}}}
{"qid": "700604097171850168", "question": "what was the tower of london originally used for", "gold_answers": ["as a royal residence", "a royal residence"], "shots": {"0": {"k": 0, "answer": "Royal palace and fortress", "latency_ms": 101.67813301086426, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "armoury", "latency_ms": 71.70677185058594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 590}, "2": {"k": 2, "answer": "armoury, treasury, menagerie, Royal Mint, public record office, home of the Crown Jewels", "latency_ms": 653.2342433929443, "em": 0.0, "f1": 0.14285714285714285, "containment": 0.0, "context_chars": 1179}, "3": {"k": 3, "answer": "royal castle", "latency_ms": 102.28204727172852, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1807}, "4": {"k": 4, "answer": "Royal castle and fortress", "latency_ms": 139.91975784301758, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2434}, "5": {"k": 5, "answer": "Royal castle and fortress built by William the Conqueror in 1078 to secure the Norman Conquest of England.", "latency_ms": 666.6686534881592, "em": 0.0, "f1": 0.1111111111111111, "containment": 0.0, "context_chars": 3004}, "6": {"k": 6, "answer": "royal residence", "latency_ms": 264.4178867340088, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3598}}}
{"qid": "6745303307988470742", "question": "what type of fuel goes in a zippo", "gold_answers": ["butane", "lighter fluid"], "shots": {"0": {"k": 0, "answer": "Zippo uses light fluid fuel.", "latency_ms": 160.39800643920898, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 100.05044937133789, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 591}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 137.9108428955078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1231}, "3": {"k": 3, "answer": "Zippo does not use fuel in the traditional sense; it uses a wick to burn a fuel, typically a blend of mineral oil or similar liquid. However, the specific type of fuel is not mentioned in the context. I cannot answer.", "latency_ms": 1302.6556968688965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1823}, "4": {"k": 4, "answer": "Zippo does not use a specific type of fuel mentioned in the context. I cannot answer.", "latency_ms": 559.9565505981445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2407}, "5": {"k": 5, "answer": "Zippo uses a wick and a fuel source typically composed of a blend of mineral oil and a small amount of alcohol, specifically designed to burn efficiently and be easily extinguished. The exact composition is proprietary, but it is generally a type of liquid fuel that is stored in the lighter and burned through the wick. The fuel is not the same as aviation or automotive fuels. The primary fuel in a Zippo is a specialized mineral oil-based fuel. \n\nAnswer: mineral oil-based fuel", "latency_ms": 2732.825756072998, "em": 0.0, "f1": 0.02777777777777778, "containment": 0.0, "context_chars": 2997}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 159.36040878295898, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3619}}}
{"qid": "-2060506905455252030", "question": "first jnanpith award was an autor of which language", "gold_answers": ["Malayalam"], "shots": {"0": {"k": 0, "answer": "Kannada", "latency_ms": 149.39641952514648, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "English", "latency_ms": 51.393985748291016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "English", "latency_ms": 155.2715301513672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1310}, "3": {"k": 3, "answer": "English", "latency_ms": 70.36137580871582, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1965}, "4": {"k": 4, "answer": "English", "latency_ms": 76.14016532897949, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2610}, "5": {"k": 5, "answer": "English", "latency_ms": 202.73399353027344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3280}, "6": {"k": 6, "answer": "Malayalam", "latency_ms": 130.3246021270752, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3951}}}
{"qid": "4738663028303377024", "question": "who played the mad hatter in the batman tv show", "gold_answers": ["Roddy McDowall", "David Wayne", "Benedict Samuel"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 143.67198944091797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 213.47308158874512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 672}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 197.57390022277832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1292}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 125.33831596374512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1901}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 147.6125717163086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2544}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 160.52842140197754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3214}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 265.7794952392578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3845}}}
{"qid": "-1650946015201779846", "question": "how many cracker barrels in the united states", "gold_answers": ["639", "more than 600"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 97.17369079589844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "645", "latency_ms": 96.2064266204834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "645", "latency_ms": 112.1225357055664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1253}, "3": {"k": 3, "answer": "645", "latency_ms": 136.50155067443848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1908}, "4": {"k": 4, "answer": "645", "latency_ms": 150.67529678344727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2528}, "5": {"k": 5, "answer": "645", "latency_ms": 161.75103187561035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3212}, "6": {"k": 6, "answer": "645", "latency_ms": 250.99468231201172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3837}}}
{"qid": "-3422350952909582054", "question": "in photosynthesis the carbon in co2 is initially fixed to what molecule", "gold_answers": ["3 - phosphoglycerate"], "shots": {"0": {"k": 0, "answer": "3-phosphoglycerate", "latency_ms": 174.79920387268066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "3-phosphoglycerate", "latency_ms": 207.34739303588867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 725}, "2": {"k": 2, "answer": "3-phosphoglycerate (3-PGA)", "latency_ms": 260.2260112762451, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1428}, "3": {"k": 3, "answer": "carbohydrate", "latency_ms": 175.5373477935791, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2154}, "4": {"k": 4, "answer": "3-phosphoglycerate (3-PGA)", "latency_ms": 440.11449813842773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2772}, "5": {"k": 5, "answer": "3-phosphoglycerate (3-PGA)", "latency_ms": 455.68275451660156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3451}, "6": {"k": 6, "answer": "3-phosphoglycerate (3-PGA)", "latency_ms": 451.12156867980957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4115}}}
{"qid": "-2704708103393811482", "question": "who voices randy in f is for family", "gold_answers": ["T.J. Miller"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 161.94796562194824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 233.22772979736328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 651}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 211.76552772521973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1289}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 172.0435619354248, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1876}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 193.53461265563965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2479}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 196.37084007263184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3082}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 246.89817428588867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3747}}}
{"qid": "5482012214308896475", "question": "how many countries does cadbury sell its products", "gold_answers": ["more than 50", "more than 50 countries worldwide"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 131.4997673034668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 167.6313877105713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 673}, "2": {"k": 2, "answer": "30", "latency_ms": 131.40249252319336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "more than fifty", "latency_ms": 115.30733108520508, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2026}, "4": {"k": 4, "answer": "more than fifty", "latency_ms": 144.46473121643066, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2731}, "5": {"k": 5, "answer": "more than fifty", "latency_ms": 113.82269859313965, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3435}, "6": {"k": 6, "answer": "more than fifty countries worldwide", "latency_ms": 283.8330268859863, "em": 0.0, "f1": 0.8000000000000002, "containment": 0.0, "context_chars": 4143}}}
{"qid": "-5737509619250561344", "question": "ethiopia flight 961 crashes in to the sea", "gold_answers": ["23 November 1996"], "shots": {"0": {"k": 0, "answer": "1996", "latency_ms": 133.00490379333496, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indian Ocean", "latency_ms": 64.62764739990234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "Indian Ocean", "latency_ms": 186.00225448608398, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1351}, "3": {"k": 3, "answer": "Indian Ocean", "latency_ms": 126.26361846923828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2014}, "4": {"k": 4, "answer": "Indian Ocean", "latency_ms": 159.65628623962402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2644}, "5": {"k": 5, "answer": "Indian Ocean", "latency_ms": 123.40164184570312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3297}, "6": {"k": 6, "answer": "Indian Ocean", "latency_ms": 135.50376892089844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3934}}}
{"qid": "-7175456993713717070", "question": "what does istj mean in a personality test", "gold_answers": ["Extroverted Thinking ( Te )", "Extroverted Intuition ( Ne )", "Introverted Feeling ( Fi )", "Introverted Sensing ( Si )"], "shots": {"0": {"k": 0, "answer": "ISTJ", "latency_ms": 100.02493858337402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 193.6788558959961, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 666}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 217.83447265625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1374}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 158.88261795043945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2130}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 180.6018352508545, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2817}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 197.91293144226074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3544}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 209.31243896484375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4225}}}
{"qid": "4437170343606050664", "question": "who does the voice of ash in sing", "gold_answers": ["Scarlett Johansson"], "shots": {"0": {"k": 0, "answer": "John Mulaney", "latency_ms": 204.4079303741455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 192.66963005065918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 130.60545921325684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1401}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 159.26241874694824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1996}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 190.74273109436035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2609}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 198.87566566467285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3228}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 195.05786895751953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3923}}}
{"qid": "-8773010039077552394", "question": "who scored the most points in their nba career", "gold_answers": ["Kareem Abdul - Jabbar"], "shots": {"0": {"k": 0, "answer": "Kareem Abdul-Jabbar", "latency_ms": 352.3745536804199, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 124.37295913696289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 552}, "2": {"k": 2, "answer": "Wilt Chamberlain", "latency_ms": 164.69311714172363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1103}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 249.88317489624023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1716}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 310.8940124511719, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2359}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 219.81453895568848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2951}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 264.4686698913574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3552}}}
{"qid": "-3278224144622911032", "question": "with a land area of 54 314 square miles where does wisconsin rank among the 50 states", "gold_answers": ["25"], "shots": {"0": {"k": 0, "answer": "10th", "latency_ms": 134.05108451843262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 209.72108840942383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 228.29246520996094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1191}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 127.81143188476562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1837}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 157.53650665283203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2414}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 192.53945350646973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3141}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 180.6638240814209, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3862}}}
{"qid": "-3466298422174960041", "question": "who plays charles on have and have nots", "gold_answers": ["Nick Sager", "Nick Sagar"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 90.17014503479004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 277.36759185791016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 175.7206916809082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1282}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 208.31012725830078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1894}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 159.90304946899414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2537}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 190.49978256225586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3169}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 216.21012687683105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3764}}}
{"qid": "1803248910342766083", "question": "who wrote the book my country my life", "gold_answers": ["L.K. Advani"], "shots": {"0": {"k": 0, "answer": "Naomi Klein", "latency_ms": 188.7977123260498, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "L. K. Advani", "latency_ms": 287.20808029174805, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "L. K. Advani", "latency_ms": 229.26807403564453, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1227}, "3": {"k": 3, "answer": "L. K. Advani", "latency_ms": 185.08505821228027, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1831}, "4": {"k": 4, "answer": "L. K. Advani", "latency_ms": 236.70125007629395, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2411}, "5": {"k": 5, "answer": "L. K. Advani", "latency_ms": 222.3818302154541, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2983}, "6": {"k": 6, "answer": "L. K. Advani", "latency_ms": 255.65552711486816, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3592}}}
{"qid": "-1176280233636154422", "question": "when did the east india company take control of india", "gold_answers": ["1757", "1799", "1612", "in 1757"], "shots": {"0": {"k": 0, "answer": "1757", "latency_ms": 135.90550422668457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 118.9718246459961, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 651}, "2": {"k": 2, "answer": "1858", "latency_ms": 179.40974235534668, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1300}, "3": {"k": 3, "answer": "1757", "latency_ms": 205.72686195373535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1970}, "4": {"k": 4, "answer": "1757", "latency_ms": 280.84731101989746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2605}, "5": {"k": 5, "answer": "1757", "latency_ms": 227.9355525970459, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3270}, "6": {"k": 6, "answer": "1757", "latency_ms": 150.70509910583496, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3932}}}
{"qid": "5911119091653953181", "question": "where does the water come from to fill lake eyre", "gold_answers": ["Warburton River"], "shots": {"0": {"k": 0, "answer": "rainfall and inflows from surrounding rivers and groundwater", "latency_ms": 181.89597129821777, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "river systems of semi-arid inland Queensland", "latency_ms": 153.37753295898438, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "river systems of semi-arid inland Queensland, roughly to the north", "latency_ms": 301.0427951812744, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 1199}, "3": {"k": 3, "answer": "river systems of semi-arid inland Queensland, roughly to the north", "latency_ms": 323.4536647796631, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 1786}, "4": {"k": 4, "answer": "river systems of semi-arid inland Queensland, roughly to the north; massive floods in the Georgina River", "latency_ms": 530.1876068115234, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 2374}, "5": {"k": 5, "answer": "rivers from the north-east part of the Lake Eyre Basin, particularly the Georgina River", "latency_ms": 534.9643230438232, "em": 0.0, "f1": 0.15384615384615385, "containment": 0.0, "context_chars": 3005}, "6": {"k": 6, "answer": "rivers from the north-east part of the Lake Eyre Basin, including the Georgina River", "latency_ms": 565.8097267150879, "em": 0.0, "f1": 0.15384615384615385, "containment": 0.0, "context_chars": 3627}}}
{"qid": "5557104327881436095", "question": "who is the founder of google and when was it founded", "gold_answers": ["1998", "Sergey Brin", "Larry Page and Sergey Brin", "Larry Page"], "shots": {"0": {"k": 0, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 220.01910209655762, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 382.2190761566162, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 678}, "2": {"k": 2, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 321.9642639160156, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 1356}, "3": {"k": 3, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 414.33143615722656, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 2023}, "4": {"k": 4, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 396.78239822387695, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 2733}, "5": {"k": 5, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 377.69341468811035, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 3373}, "6": {"k": 6, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 434.3724250793457, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 4024}}}
{"qid": "-1333328812971005142", "question": "who did the dominican republic gain its independence from", "gold_answers": ["Haiti"], "shots": {"0": {"k": 0, "answer": "Spain", "latency_ms": 69.87833976745605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Spain", "latency_ms": 51.31363868713379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "Haiti", "latency_ms": 150.39610862731934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1342}, "3": {"k": 3, "answer": "Haiti", "latency_ms": 132.18116760253906, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2022}, "4": {"k": 4, "answer": "Haiti", "latency_ms": 139.47582244873047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2736}, "5": {"k": 5, "answer": "Haiti", "latency_ms": 166.3200855255127, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3373}, "6": {"k": 6, "answer": "Haiti", "latency_ms": 167.91844367980957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4093}}}
{"qid": "1122605495295045742", "question": "how did early humans make use of stones during the prehistoric period", "gold_answers": ["start fires", "hunt", "bury their dead"], "shots": {"0": {"k": 0, "answer": "Chipped and shaped for tools", "latency_ms": 241.73736572265625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "made implements with a sharp edge, a point, or a percussion surface", "latency_ms": 356.3411235809326, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 670}, "2": {"k": 2, "answer": "made implements with a sharp edge, a point, or a percussion surface", "latency_ms": 559.664249420166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1354}, "3": {"k": 3, "answer": "made implements with a sharp edge, a point, or a percussion surface", "latency_ms": 640.6440734863281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1971}, "4": {"k": 4, "answer": "made sharp-edged, pointed, or percussion tools", "latency_ms": 553.0190467834473, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2588}, "5": {"k": 5, "answer": "made implements with an edge, a point, or a percussion surface", "latency_ms": 436.56134605407715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3234}, "6": {"k": 6, "answer": "By striking a core of hard stone (such as flint) with a hammerstone to produce sharp edges for tools like choppers or scrapers.", "latency_ms": 952.4264335632324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3872}}}
{"qid": "7402184777947527945", "question": "who wrote the text for jeanie with the light brown hair", "gold_answers": ["Stephen Foster"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 177.5045394897461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Stephen Foster", "latency_ms": 104.80690002441406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 632}, "2": {"k": 2, "answer": "Stephen Foster", "latency_ms": 99.56121444702148, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1222}, "3": {"k": 3, "answer": "Stephen Foster", "latency_ms": 83.10794830322266, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1885}, "4": {"k": 4, "answer": "Stephen Foster", "latency_ms": 121.36554718017578, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2493}, "5": {"k": 5, "answer": "Stephen Foster", "latency_ms": 135.15639305114746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3138}, "6": {"k": 6, "answer": "Stephen Foster", "latency_ms": 206.51912689208984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3757}}}
{"qid": "1656745334095178455", "question": "the basic steps of rational decision making include defining goals and", "gold_answers": ["Implementation of plans / policies"], "shots": {"0": {"k": 0, "answer": "identifying alternatives", "latency_ms": 86.62700653076172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "defining goals and identifying alternatives", "latency_ms": 218.7325954437256, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 652}, "2": {"k": 2, "answer": "defining goals and alternatives", "latency_ms": 155.84921836853027, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1345}, "3": {"k": 3, "answer": "defining goals and verifying the problem", "latency_ms": 161.8044376373291, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2121}, "4": {"k": 4, "answer": "defining goals and information gathering", "latency_ms": 179.6720027923584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2791}, "5": {"k": 5, "answer": "defining goals and information gathering", "latency_ms": 257.7846050262451, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3488}, "6": {"k": 6, "answer": "defining the problem, gathering information, evaluating alternatives, selecting a solution, implementing and monitoring", "latency_ms": 541.9776439666748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4121}}}
{"qid": "-216657635823030751", "question": "when was the design period known as art deco", "gold_answers": ["1910 -- 1939"], "shots": {"0": {"k": 0, "answer": "1920s–1930s", "latency_ms": 305.41014671325684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1925", "latency_ms": 102.2486686706543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "1925", "latency_ms": 129.16159629821777, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1343}, "3": {"k": 3, "answer": "1925", "latency_ms": 170.3794002532959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2025}, "4": {"k": 4, "answer": "just before World War I to the beginning of World War II", "latency_ms": 514.8301124572754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2658}, "5": {"k": 5, "answer": "just before World War I to the beginning of World War II", "latency_ms": 452.36945152282715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3321}, "6": {"k": 6, "answer": "1925 onwards", "latency_ms": 178.83610725402832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3983}}}
{"qid": "250542332339248886", "question": "where does the bob and tom show broadcast from", "gold_answers": ["WFBQ in Indianapolis , Indiana", "Indianapolis , Indiana"], "shots": {"0": {"k": 0, "answer": "Winston-Salem, North Carolina", "latency_ms": 187.35504150390625, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indianapolis, Indiana", "latency_ms": 108.81948471069336, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "Indianapolis, Indiana", "latency_ms": 137.95185089111328, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "Indianapolis, Indiana", "latency_ms": 160.26902198791504, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1825}, "4": {"k": 4, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 336.49682998657227, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2443}, "5": {"k": 5, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 251.9676685333252, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3092}, "6": {"k": 6, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 356.28604888916016, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3726}}}
{"qid": "7610758795954724809", "question": "star wars the clone wars season 3 episode 1", "gold_answers": ["Clone Cadets"], "shots": {"0": {"k": 0, "answer": "\"The Clone Wars: Season 3, Episode 1\"", "latency_ms": 386.5034580230713, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 142.3799991607666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 673}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 143.8124179840088, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 233.2320213317871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2030}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 286.7417335510254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2678}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 237.105131149292, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3310}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 211.0269069671631, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3953}}}
{"qid": "2997223939033016160", "question": "what part of brain is responsible for complex thinking", "gold_answers": ["the frontal lobe"], "shots": {"0": {"k": 0, "answer": "prefrontal cortex", "latency_ms": 172.73330688476562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "cerebrum", "latency_ms": 143.7513828277588, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 595}, "2": {"k": 2, "answer": "frontal lobe", "latency_ms": 166.8233871459961, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1224}, "3": {"k": 3, "answer": "frontal lobe", "latency_ms": 202.79455184936523, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1827}, "4": {"k": 4, "answer": "frontal lobe", "latency_ms": 161.4370346069336, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2454}, "5": {"k": 5, "answer": "cerebral cortex", "latency_ms": 136.29889488220215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3124}, "6": {"k": 6, "answer": "frontal lobe", "latency_ms": 217.04888343811035, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3757}}}
{"qid": "2680196226239522881", "question": "who won the champions league final in 2016", "gold_answers": ["Real Madrid"], "shots": {"0": {"k": 0, "answer": "Real Madrid", "latency_ms": 59.825897216796875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 134.90796089172363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 648}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 100.22163391113281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1274}, "3": {"k": 3, "answer": "Real Madrid", "latency_ms": 108.02197456359863, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1905}, "4": {"k": 4, "answer": "Real Madrid", "latency_ms": 128.4463405609131, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2523}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 156.33296966552734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3154}, "6": {"k": 6, "answer": "Real Madrid", "latency_ms": 129.7624111175537, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3778}}}
{"qid": "-8458006812258546025", "question": "who has the most gold medals in the winter olympics of all time", "gold_answers": ["Norway"], "shots": {"0": {"k": 0, "answer": "Cross-country skier with most gold medals in Winter Olympics of all time is not specified in the context. I cannot answer.", "latency_ms": 592.552900314331, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 170.84312438964844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Canada", "latency_ms": 58.277130126953125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1228}, "3": {"k": 3, "answer": "Canada", "latency_ms": 103.92475128173828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1853}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 165.69042205810547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2470}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 196.16246223449707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3032}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 277.94671058654785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3614}}}
