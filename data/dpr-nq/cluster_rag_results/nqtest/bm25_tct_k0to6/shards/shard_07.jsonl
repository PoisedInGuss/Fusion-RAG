{"qid": "5306300389470118812", "question": "what was the religion in persia before islam", "gold_answers": ["the Zoroastrian religion", "Zoroastrian"], "shots": {"0": {"k": 0, "answer": "Zoroastrianism", "latency_ms": 1437.5050067901611, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Zoroastrianism", "latency_ms": 1696.9926357269287, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 637}, "2": {"k": 2, "answer": "Zoroastrianism", "latency_ms": 2208.4293365478516, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1275}, "3": {"k": 3, "answer": "Zoroastrianism", "latency_ms": 3138.1630897521973, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1889}, "4": {"k": 4, "answer": "Zoroastrianism", "latency_ms": 3366.248369216919, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 2599}, "5": {"k": 5, "answer": "Zoroastrianism", "latency_ms": 3326.4148235321045, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3219}, "6": {"k": 6, "answer": "Zoroastrianism", "latency_ms": 3148.7555503845215, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3904}}}
{"qid": "4492088526674104172", "question": "why does kerosene oil rise up in the wick of lantern", "gold_answers": ["capillary action"], "shots": {"0": {"k": 0, "answer": "Capillary action", "latency_ms": 1197.3865032196045, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "capillary action", "latency_ms": 1290.3974056243896, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 586}, "2": {"k": 2, "answer": "capillary action", "latency_ms": 1785.2721214294434, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1208}, "3": {"k": 3, "answer": "capillary action", "latency_ms": 2052.0308017730713, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1818}, "4": {"k": 4, "answer": "capillary action", "latency_ms": 2331.0515880584717, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2433}, "5": {"k": 5, "answer": "capillary action", "latency_ms": 2608.6220741271973, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3010}, "6": {"k": 6, "answer": "capillary action", "latency_ms": 2799.4091510772705, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3627}}}
{"qid": "-4373262132142058334", "question": "mention the chemical change that proinsulin undergo to be able to act as mature insulin", "gold_answers": ["post translational modification"], "shots": {"0": {"k": 0, "answer": "Proteolytic cleavage", "latency_ms": 1625.591516494751, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Proinsulin undergoes proteolytic cleavage to form mature insulin.", "latency_ms": 3596.578598022461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 702}, "2": {"k": 2, "answer": "C-peptide is cleaved", "latency_ms": 2351.7894744873047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1349}, "3": {"k": 3, "answer": "C-peptide cleavage", "latency_ms": 2496.1273670196533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1986}, "4": {"k": 4, "answer": "cleavage of C-peptide", "latency_ms": 2941.1652088165283, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2619}, "5": {"k": 5, "answer": "C-peptide is cleaved", "latency_ms": 3501.378297805786, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3227}, "6": {"k": 6, "answer": "C-peptide cleavage", "latency_ms": 3724.8477935791016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3978}}}
{"qid": "-5298560410953979569", "question": "where does the sweetness of fruit come from", "gold_answers": ["Fructose , or fruit sugar", "Fructose"], "shots": {"0": {"k": 0, "answer": "sugars like fructose, glucose, and sucrose", "latency_ms": 2896.0204124450684, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "sugars", "latency_ms": 1621.546983718872, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 677}, "2": {"k": 2, "answer": "sugars", "latency_ms": 1812.4957084655762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1295}, "3": {"k": 3, "answer": "sugars", "latency_ms": 2184.2973232269287, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1902}, "4": {"k": 4, "answer": "sugars", "latency_ms": 2487.9322052001953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2558}, "5": {"k": 5, "answer": "sugars", "latency_ms": 3057.10768699646, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3191}, "6": {"k": 6, "answer": "sugars", "latency_ms": 3407.69624710083, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3801}}}
{"qid": "4344648368832551136", "question": "an object that moves around an external axis is said to be", "gold_answers": ["orbit", "revolution or orbital revolution"], "shots": {"0": {"k": 0, "answer": "rotating", "latency_ms": 832.3328495025635, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "revolving", "latency_ms": 1352.1690368652344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "revolving", "latency_ms": 1551.9294738769531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1174}, "3": {"k": 3, "answer": "revolving", "latency_ms": 1751.9919872283936, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1760}, "4": {"k": 4, "answer": "revolving", "latency_ms": 2130.1093101501465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2418}, "5": {"k": 5, "answer": "revolving", "latency_ms": 2099.491596221924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3047}, "6": {"k": 6, "answer": "revolving", "latency_ms": 2929.7549724578857, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3707}}}
{"qid": "-724426540432437711", "question": "what was the name of the plant on little shop of horrors", "gold_answers": ["Audrey II"], "shots": {"0": {"k": 0, "answer": "Audrey II", "latency_ms": 1034.5234870910645, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Audrey II", "latency_ms": 1563.1639957427979, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 589}, "2": {"k": 2, "answer": "Audrey", "latency_ms": 1573.2336044311523, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1205}, "3": {"k": 3, "answer": "Audrey II", "latency_ms": 2328.144073486328, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1908}, "4": {"k": 4, "answer": "The plant on Little Shop of Horrors is named **Audrey**.", "latency_ms": 5222.397089004517, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 2555}, "5": {"k": 5, "answer": "The plant on Little Shop of Horrors is named \"Audrey.\"", "latency_ms": 5250.139951705933, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 3197}, "6": {"k": 6, "answer": "The plant on Little Shop of Horrors is not named in the provided context. I cannot answer.", "latency_ms": 6560.534238815308, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3821}}}
{"qid": "-5541610970112826260", "question": "who signed the declaration of independence for maryland", "gold_answers": ["Samuel Chase", "Charles Carroll of Carrollton", "Thomas Stone", "William Paca"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1254.4431686401367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1913.3272171020508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 726}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1958.90212059021, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1463}, "3": {"k": 3, "answer": "Maryland delegates to the Second Continental Congress", "latency_ms": 3119.7543144226074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2111}, "4": {"k": 4, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 5098.45495223999, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 2831}, "5": {"k": 5, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 5452.012300491333, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 3523}, "6": {"k": 6, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 6132.9851150512695, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 4198}}}
{"qid": "-86997217831791339", "question": "when does wonder woman come out on blue ray", "gold_answers": ["September 19 , 2017"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1247.5080490112305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 19, 2017", "latency_ms": 3186.460256576538, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 606}, "2": {"k": 2, "answer": "September 19, 2017", "latency_ms": 3421.6837882995605, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1243}, "3": {"k": 3, "answer": "September 19, 2017", "latency_ms": 3547.672748565674, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1849}, "4": {"k": 4, "answer": "September 19, 2017", "latency_ms": 3653.686046600342, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2524}, "5": {"k": 5, "answer": "September 19, 2017", "latency_ms": 4482.364892959595, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3183}, "6": {"k": 6, "answer": "September 19, 2017", "latency_ms": 4378.232955932617, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3784}}}
{"qid": "-8062812506672279762", "question": "what nba team did derrick rose play for", "gold_answers": ["New York Knicks", "Minnesota Timberwolves", "Chicago Bulls", "Cleveland Cavaliers"], "shots": {"0": {"k": 0, "answer": "Chicago Bulls", "latency_ms": 834.2673778533936, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Minnesota Timberwolves", "latency_ms": 1794.773817062378, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 630}, "2": {"k": 2, "answer": "Minnesota Timberwolves", "latency_ms": 2018.667221069336, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1285}, "3": {"k": 3, "answer": "Minnesota Timberwolves", "latency_ms": 2456.728458404541, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1918}, "4": {"k": 4, "answer": "Minnesota Timberwolves", "latency_ms": 2523.8773822784424, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2514}, "5": {"k": 5, "answer": "Minnesota Timberwolves", "latency_ms": 3100.980043411255, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3144}, "6": {"k": 6, "answer": "Minnesota Timberwolves", "latency_ms": 2948.4822750091553, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3743}}}
{"qid": "7933002036740390435", "question": "where did the term liberal arts come from", "gold_answers": ["the Roman Empire"], "shots": {"0": {"k": 0, "answer": "Rome", "latency_ms": 890.8958435058594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "United States", "latency_ms": 953.4730911254883, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 720}, "2": {"k": 2, "answer": "European origins", "latency_ms": 1517.906904220581, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1415}, "3": {"k": 3, "answer": "European origins", "latency_ms": 1456.6502571105957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2128}, "4": {"k": 4, "answer": "European origins", "latency_ms": 2118.043899536133, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2862}, "5": {"k": 5, "answer": "Roman Empire", "latency_ms": 2385.693073272705, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3481}, "6": {"k": 6, "answer": "Roman Empire", "latency_ms": 3471.43292427063, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4195}}}
{"qid": "-2991400700385487821", "question": "who heads the executive department of west virginia government", "gold_answers": ["the Governor of West Virginia", "the governor of West Virginia", "Jim Justice"], "shots": {"0": {"k": 0, "answer": "Governor of West Virginia", "latency_ms": 1444.810390472412, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Governor of West Virginia", "latency_ms": 1555.2089214324951, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 671}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1891.9131755828857, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1396}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1866.6892051696777, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2052}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2557.0337772369385, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2773}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2668.919086456299, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3452}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3158.033847808838, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4160}}}
{"qid": "-9186689755642837558", "question": "how long is the bridge between new brunswick and prince edward island", "gold_answers": ["12.9 - kilometre"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1287.304162979126, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1782.9809188842773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 670}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2017.383337020874, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1405}, "3": {"k": 3, "answer": "world's longest bridge over ice-covered", "latency_ms": 3066.699981689453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2092}, "4": {"k": 4, "answer": "world's longest bridge over ice-covered", "latency_ms": 3269.005060195923, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2815}, "5": {"k": 5, "answer": "world's longest bridge over ice-covered", "latency_ms": 3568.430185317993, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3503}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3297.6577281951904, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4178}}}
{"qid": "8029901619424338449", "question": "when did the uk and us become allies", "gold_answers": ["1940", "Since 1940"], "shots": {"0": {"k": 0, "answer": "1941", "latency_ms": 1275.2580642700195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1516.0715579986572, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 583}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1958.4834575653076, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1223}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2271.873950958252, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1881}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2547.750473022461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2601}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2904.226064682007, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3245}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3031.6643714904785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3861}}}
{"qid": "4828649525820722736", "question": "who sang the theme song for the man with the golden gun", "gold_answers": ["Lulu"], "shots": {"0": {"k": 0, "answer": "Lulu", "latency_ms": 834.9823951721191, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lulu", "latency_ms": 1503.0295848846436, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 582}, "2": {"k": 2, "answer": "Lulu", "latency_ms": 1477.4434566497803, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1225}, "3": {"k": 3, "answer": "Lulu", "latency_ms": 1909.4302654266357, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1818}, "4": {"k": 4, "answer": "Lulu", "latency_ms": 2112.767219543457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2431}, "5": {"k": 5, "answer": "Lulu", "latency_ms": 2987.215757369995, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3088}, "6": {"k": 6, "answer": "Lulu", "latency_ms": 3474.3504524230957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3721}}}
{"qid": "-7924663090486742985", "question": "american horror story freak show girl kidnapped by clown", "gold_answers": ["Bonnie Lipton"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1272.3217010498047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Twisty", "latency_ms": 1139.2009258270264, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "Twisty", "latency_ms": 1595.1778888702393, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1352}, "3": {"k": 3, "answer": "Sarah Paulson", "latency_ms": 2018.7575817108154, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1955}, "4": {"k": 4, "answer": "Twisty", "latency_ms": 2220.294952392578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2631}, "5": {"k": 5, "answer": "Sarah Paulson", "latency_ms": 2886.8234157562256, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3309}, "6": {"k": 6, "answer": "Sarah Paulson", "latency_ms": 3344.8405265808105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3897}}}
{"qid": "-360675773049242516", "question": "the probability of making a type i error when retaining ho at the .05 level of significance is", "gold_answers": ["5 %"], "shots": {"0": {"k": 0, "answer": "0.05", "latency_ms": 1305.6044578552246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "0.05", "latency_ms": 1383.5179805755615, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 642}, "2": {"k": 2, "answer": "0.05", "latency_ms": 1987.7865314483643, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1219}, "3": {"k": 3, "answer": "0.05", "latency_ms": 2283.0944061279297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1817}, "4": {"k": 4, "answer": "0.05", "latency_ms": 2423.698663711548, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2462}, "5": {"k": 5, "answer": "0.05", "latency_ms": 2885.9572410583496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3110}, "6": {"k": 6, "answer": "0.05", "latency_ms": 2974.842071533203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3736}}}
{"qid": "5819386267283467034", "question": "what year did the us hockey team won the olympics", "gold_answers": ["1960", "1980", "1960 and 1980"], "shots": {"0": {"k": 0, "answer": "1980", "latency_ms": 1265.5858993530273, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1960, 1980", "latency_ms": 3355.2372455596924, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 659}, "2": {"k": 2, "answer": "1960", "latency_ms": 1978.5380363464355, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1276}, "3": {"k": 3, "answer": "1960", "latency_ms": 2231.724739074707, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1938}, "4": {"k": 4, "answer": "1980", "latency_ms": 2538.6645793914795, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2535}, "5": {"k": 5, "answer": "1960", "latency_ms": 3075.3211975097656, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3214}, "6": {"k": 6, "answer": "1960", "latency_ms": 2993.494749069214, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3837}}}
{"qid": "3916394566028297311", "question": "who wrote it's a long long way to pasadena", "gold_answers": ["Harry Vanda", "John Young", "David Hemmings", "George Young"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1243.83544921875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1603.4016609191895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2022.3431587219238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1257}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2402.074098587036, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1837}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2453.3519744873047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2523}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3707.092761993408, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3154}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 4097.832918167114, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3841}}}
{"qid": "-2432292250757146771", "question": "who wrote the song i hate you i love you", "gold_answers": ["Angel Benitez", "Olivia O'Brien", "Garrett Nash"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1241.3363456726074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "StarGate", "latency_ms": 980.3493022918701, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "StarGate", "latency_ms": 1581.6311836242676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1203}, "3": {"k": 3, "answer": "Gnash and Olivia O'Brien", "latency_ms": 2887.8581523895264, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1777}, "4": {"k": 4, "answer": "Gnash and Olivia O'Brien", "latency_ms": 3296.288251876831, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2379}, "5": {"k": 5, "answer": "Gnash and Olivia O'Brien", "latency_ms": 4642.3680782318115, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2979}, "6": {"k": 6, "answer": "Gnash and Olivia O'Brien", "latency_ms": 3911.2207889556885, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3554}}}
{"qid": "2393389448844632496", "question": "what is loihi and where does it originate from", "gold_answers": ["from the Hawaii hotspot"], "shots": {"0": {"k": 0, "answer": "Loihi is a seamount in the Pacific Ocean, originating from the Hawaiian hotspot.", "latency_ms": 4001.971483230591, "em": 0.0, "f1": 0.30769230769230765, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Loihi is a submarine volcano in Hawaii.", "latency_ms": 2822.503089904785, "em": 0.0, "f1": 0.2222222222222222, "containment": 0.0, "context_chars": 678}, "2": {"k": 2, "answer": "Loihi Seamount; Hawaii", "latency_ms": 2784.2798233032227, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1315}, "3": {"k": 3, "answer": "Loihi; Hawaiian volcano, submarine, Loihi Seamount", "latency_ms": 4560.68754196167, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1984}, "4": {"k": 4, "answer": "Loihi; a submarine volcano in Hawaii, originating from the Hawaiian Islands' volcanic chain.", "latency_ms": 5659.630298614502, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2628}, "5": {"k": 5, "answer": "Loihi; Hawaiian word meaning \"long\"; submarine volcano in Hawaii", "latency_ms": 4996.782064437866, "em": 0.0, "f1": 0.16666666666666663, "containment": 0.0, "context_chars": 3301}, "6": {"k": 6, "answer": "Loihi; Hawaiian volcano, submarine volcano in Hawaii", "latency_ms": 5396.578311920166, "em": 0.0, "f1": 0.2, "containment": 0.0, "context_chars": 3958}}}
{"qid": "-6677440871585683891", "question": "who rebuilt the temple after the babylonian captivity", "gold_answers": ["Herod"], "shots": {"0": {"k": 0, "answer": "Zerubbabel", "latency_ms": 1227.3273468017578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cyrus the Great", "latency_ms": 2006.9224834442139, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Ezra", "latency_ms": 1734.649896621704, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1235}, "3": {"k": 3, "answer": "Jews under Persian rule", "latency_ms": 2519.8311805725098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1832}, "4": {"k": 4, "answer": "Jews under Persian rule", "latency_ms": 2663.0425453186035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2524}, "5": {"k": 5, "answer": "Persian rulers, specifically under Cyrus the Great", "latency_ms": 4543.594837188721, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3177}, "6": {"k": 6, "answer": "Persian rulers under Cyrus", "latency_ms": 3699.857473373413, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3873}}}
{"qid": "-6750750235985613277", "question": "who was executed for being an american spy during the revolutionary war", "gold_answers": ["Nathan Hale"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1243.297815322876, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1374.4466304779053, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 602}, "2": {"k": 2, "answer": "Nathan Hale", "latency_ms": 1723.755121231079, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1197}, "3": {"k": 3, "answer": "Nathan Hale", "latency_ms": 2079.0467262268066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1891}, "4": {"k": 4, "answer": "Nathan Hale", "latency_ms": 2323.7948417663574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2549}, "5": {"k": 5, "answer": "Nathan Hale", "latency_ms": 3314.7029876708984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3210}, "6": {"k": 6, "answer": "Nathan Hale", "latency_ms": 2844.8758125305176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3862}}}
{"qid": "8770209312170080158", "question": "who sang the song suddenly with olivia newton john", "gold_answers": ["Cliff Richard"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1227.9982566833496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cliff Richard", "latency_ms": 1401.8876552581787, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "Cliff Richard", "latency_ms": 1865.8671379089355, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1270}, "3": {"k": 3, "answer": "Cliff Richard", "latency_ms": 2404.902219772339, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1903}, "4": {"k": 4, "answer": "Cliff Richard", "latency_ms": 2288.158655166626, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2550}, "5": {"k": 5, "answer": "Cliff Richard", "latency_ms": 2732.4914932250977, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3287}, "6": {"k": 6, "answer": "Cliff Richard", "latency_ms": 3597.369909286499, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3951}}}
{"qid": "-180549795106449014", "question": "who is command sergeant major of the army", "gold_answers": ["Daniel A. Dailey"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1273.2784748077393, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Sergeant Major of the Army", "latency_ms": 2369.6420192718506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 637}, "2": {"k": 2, "answer": "Sergeant Major of the Army", "latency_ms": 2651.5612602233887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1261}, "3": {"k": 3, "answer": "Sergeant Major of the Army", "latency_ms": 2575.6635665893555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1898}, "4": {"k": 4, "answer": "Sergeant Major of the Army", "latency_ms": 3120.676279067993, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2550}, "5": {"k": 5, "answer": "Sergeant Major of the Army", "latency_ms": 3778.6452770233154, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3198}, "6": {"k": 6, "answer": "Sergeant Major of the Army", "latency_ms": 3687.6871585845947, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3847}}}
{"qid": "7815138213728640354", "question": "who sings the original windmills of your mind", "gold_answers": ["Noel Harrison"], "shots": {"0": {"k": 0, "answer": "The Kinks", "latency_ms": 1052.7780055999756, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Frank Sinatra", "latency_ms": 1194.350242614746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "The song \"The Windmills of Your Mind\" was introduced in the film *The Thomas Crown Affair* (1968), and the original version was performed by **Frank Sinatra**. However, based on the provided context, it does not explicitly state who sings the original version. The context mentions a cover by Sting but does not identify the original singer.\n\nI cannot answer.", "latency_ms": 17128.170013427734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "The Thomas Crown Affair (1968)", "latency_ms": 3926.504611968994, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1873}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2798.13814163208, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2479}, "5": {"k": 5, "answer": "Noel Harrison", "latency_ms": 2782.5369834899902, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3160}, "6": {"k": 6, "answer": "Noel Harrison", "latency_ms": 3553.4772872924805, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3842}}}
{"qid": "2098168902147822379", "question": "where will the next summer and winter olympics be held", "gold_answers": ["Beijing", "Tokyo"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1222.886323928833, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Abu Dhabi, United Arab Emirates (Summer); Austria (Winter)", "latency_ms": 3811.8488788604736, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 591}, "2": {"k": 2, "answer": "2022 Winter Olympics in Beijing, China; next Summer Olympics in 2024 (not specified in context)", "latency_ms": 6476.250410079956, "em": 0.0, "f1": 0.125, "containment": 1.0, "context_chars": 1262}, "3": {"k": 3, "answer": "2028 Summer Olympics in Los Angeles, 2030 Winter Olympics in Beijing", "latency_ms": 5506.262540817261, "em": 0.0, "f1": 0.16666666666666669, "containment": 1.0, "context_chars": 1845}, "4": {"k": 4, "answer": "2028 Summer Olympics in Los Angeles, 2030 Winter Olympics in Beijing", "latency_ms": 5474.445819854736, "em": 0.0, "f1": 0.16666666666666669, "containment": 1.0, "context_chars": 2532}, "5": {"k": 5, "answer": "2028 Summer Olympics in Los Angeles, 2030 Winter Olympics in Beijing", "latency_ms": 6205.558061599731, "em": 0.0, "f1": 0.16666666666666669, "containment": 1.0, "context_chars": 3157}, "6": {"k": 6, "answer": "2028 Summer Olympics in Los Angeles, 2030 Winter Olympics in Beijing", "latency_ms": 6023.422002792358, "em": 0.0, "f1": 0.16666666666666669, "containment": 1.0, "context_chars": 3817}}}
{"qid": "-1458476052377668874", "question": "when did ole miss beat alabama in football", "gold_answers": ["September 19 , 2015", "October 3 , 1970", "October 27 , 1894", "October 13 , 2001", "October 5 , 1968", "October 4 , 2014", "September 11 , 1976", "October 8 , 1988", "October 18 , 2003", "November 5 , 1910"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1256.4175128936768, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "the game where the Rebels never trailed and led by as many as twenty points and as many as nineteen in the fourth quarter.", "latency_ms": 5603.780746459961, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 588}, "2": {"k": 2, "answer": "1988, 2014", "latency_ms": 3224.468946456909, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1223}, "3": {"k": 3, "answer": "1988, 2014", "latency_ms": 4134.723663330078, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1819}, "4": {"k": 4, "answer": "2014", "latency_ms": 2759.4144344329834, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2456}, "5": {"k": 5, "answer": "2014", "latency_ms": 3331.8278789520264, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3020}, "6": {"k": 6, "answer": "2014", "latency_ms": 3921.060562133789, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3685}}}
{"qid": "-6861734002351236912", "question": "who sang the song how far is heaven", "gold_answers": ["Los Lonely Boys"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1290.0681495666504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Justin Moore", "latency_ms": 962.7866744995117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "Justin Moore", "latency_ms": 1648.8962173461914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1196}, "3": {"k": 3, "answer": "Justin Moore", "latency_ms": 2038.3992195129395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1809}, "4": {"k": 4, "answer": "Justin Moore", "latency_ms": 2077.084541320801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2398}, "5": {"k": 5, "answer": "Justin Moore", "latency_ms": 2825.427770614624, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2987}, "6": {"k": 6, "answer": "Justin Moore", "latency_ms": 2566.659450531006, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3612}}}
{"qid": "-1722216190521937227", "question": "where are the organ mountains in new mexico", "gold_answers": ["southern New Mexico"], "shots": {"0": {"k": 0, "answer": "Organ Mountains are located in southern New Mexico, near the border with Arizona.", "latency_ms": 3509.610652923584, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 3761.255979537964, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 606}, "2": {"k": 2, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 4408.384799957275, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 1267}, "3": {"k": 3, "answer": "southern New Mexico, east of Las Cruces, in Doña Ana County", "latency_ms": 5100.292444229126, "em": 0.0, "f1": 0.42857142857142855, "containment": 1.0, "context_chars": 1924}, "4": {"k": 4, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 4945.716857910156, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 2564}, "5": {"k": 5, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 5356.25433921814, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3283}, "6": {"k": 6, "answer": "southern New Mexico, east of Las Cruces, Doña Ana County", "latency_ms": 5836.008548736572, "em": 0.0, "f1": 0.4615384615384615, "containment": 1.0, "context_chars": 3941}}}
{"qid": "-2717119588239727064", "question": "where are the 10 plagues found in the bible", "gold_answers": ["Book of Exodus", "Exodus"], "shots": {"0": {"k": 0, "answer": "Exodus 7–12", "latency_ms": 1838.2954597473145, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Exodus 7–12", "latency_ms": 2111.7782592773438, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 666}, "2": {"k": 2, "answer": "Exodus 7–12", "latency_ms": 2571.368932723999, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1376}, "3": {"k": 3, "answer": "Book of Exodus", "latency_ms": 2054.40354347229, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2002}, "4": {"k": 4, "answer": "Book of Exodus", "latency_ms": 2281.6596031188965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2677}, "5": {"k": 5, "answer": "Book of Exodus", "latency_ms": 2896.9032764434814, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3249}, "6": {"k": 6, "answer": "Book of Exodus", "latency_ms": 2769.4473266601562, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3883}}}
{"qid": "2988284357714596500", "question": "where are red blood cells made in adults", "gold_answers": ["in the bone marrow"], "shots": {"0": {"k": 0, "answer": "bone marrow", "latency_ms": 901.839017868042, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "bone marrow", "latency_ms": 1398.3168601989746, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 614}, "2": {"k": 2, "answer": "bone marrow", "latency_ms": 1627.8076171875, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1214}, "3": {"k": 3, "answer": "bone marrow", "latency_ms": 1902.8091430664062, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1810}, "4": {"k": 4, "answer": "bone marrow", "latency_ms": 2114.802360534668, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2440}, "5": {"k": 5, "answer": "bone marrow", "latency_ms": 2733.769655227661, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3102}, "6": {"k": 6, "answer": "bone marrow", "latency_ms": 2608.8380813598633, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3776}}}
{"qid": "-4616596799374362422", "question": "who was the pinkerton detective agency's first female detective", "gold_answers": ["Kate Warne"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1252.5050640106201, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kate Warne", "latency_ms": 1167.806625366211, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 681}, "2": {"k": 2, "answer": "Kate Warne", "latency_ms": 1768.378496170044, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1401}, "3": {"k": 3, "answer": "Kate Warne", "latency_ms": 2085.61110496521, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2065}, "4": {"k": 4, "answer": "Kate Warne", "latency_ms": 2220.456838607788, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2743}, "5": {"k": 5, "answer": "Kate Warne", "latency_ms": 2824.2812156677246, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3453}, "6": {"k": 6, "answer": "Kate Warne", "latency_ms": 2747.725248336792, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4064}}}
{"qid": "-3650291155113659146", "question": "how many episodes are there in modern family", "gold_answers": ["232", "201"], "shots": {"0": {"k": 0, "answer": "216 episodes", "latency_ms": 1286.78297996521, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "102", "latency_ms": 1175.8244037628174, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 654}, "2": {"k": 2, "answer": "24", "latency_ms": 2150.6474018096924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1299}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2322.6962089538574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1967}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2464.0626907348633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2586}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3038.7423038482666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3198}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3274.9340534210205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3850}}}
{"qid": "3381924381590631417", "question": "who built the first temple for god in jerusalem", "gold_answers": ["Solomon"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1264.6095752716064, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1385.9498500823975, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "King Solomon", "latency_ms": 1401.0801315307617, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1220}, "3": {"k": 3, "answer": "King Solomon", "latency_ms": 1478.9235591888428, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1829}, "4": {"k": 4, "answer": "Josiah", "latency_ms": 2098.127603530884, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2464}, "5": {"k": 5, "answer": "Josiah", "latency_ms": 2367.2115802764893, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3115}, "6": {"k": 6, "answer": "Solomon", "latency_ms": 2571.7196464538574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3768}}}
{"qid": "-154783694579651082", "question": "what is a dropped pin on google maps for", "gold_answers": ["marks locations", "marks locations in Google Maps"], "shots": {"0": {"k": 0, "answer": "A dropped pin on Google Maps typically indicates a user-created location marker, such as a point of interest, a personal location, or a saved place. It may also represent a temporary or unofficial location, like a meeting spot or a place someone has visited. Dropped pins are not officially verified or maintained by Google.", "latency_ms": 13051.315307617188, "em": 0.0, "f1": 0.0784313725490196, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "a dropped pin on Google Maps marks a specific location on the map", "latency_ms": 3172.477960586548, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 574}, "2": {"k": 2, "answer": "to accurately mark a point on a map without obscuring the location", "latency_ms": 3638.1452083587646, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "to accurately mark a point on a map without obscuring the location", "latency_ms": 3690.2287006378174, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1937}, "4": {"k": 4, "answer": "a location marker", "latency_ms": 2369.6208000183105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2531}, "5": {"k": 5, "answer": "a location marker", "latency_ms": 2303.816795349121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3165}, "6": {"k": 6, "answer": "a location marker", "latency_ms": 3026.5023708343506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3760}}}
{"qid": "6915606477668963399", "question": "what do the 3 dots mean in math", "gold_answers": ["the therefore sign", "therefore sign"], "shots": {"0": {"k": 0, "answer": "ellipses", "latency_ms": 1030.9391021728516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1600.0654697418213, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2156.0111045837402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1266}, "3": {"k": 3, "answer": "therefore", "latency_ms": 1903.9092063903809, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1917}, "4": {"k": 4, "answer": "therefore", "latency_ms": 2301.043748855591, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2568}, "5": {"k": 5, "answer": "therefore", "latency_ms": 2668.0307388305664, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3207}, "6": {"k": 6, "answer": "therefore", "latency_ms": 3298.58660697937, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3759}}}
{"qid": "-8366545547296627039", "question": "who wrote the song photograph by ringo starr", "gold_answers": ["Richard Starkey", "George Harrison", "Ringo Starr"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1238.1021976470947, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ringo Starr and George Harrison", "latency_ms": 1796.1831092834473, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 671}, "2": {"k": 2, "answer": "Ringo Starr and George Harrison", "latency_ms": 2397.7160453796387, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1329}, "3": {"k": 3, "answer": "Ringo Starr and George Harrison", "latency_ms": 2682.9934120178223, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1984}, "4": {"k": 4, "answer": "Ringo Starr and George Harrison", "latency_ms": 2863.0969524383545, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2640}, "5": {"k": 5, "answer": "George Harrison", "latency_ms": 3548.5239028930664, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3250}, "6": {"k": 6, "answer": "George Harrison", "latency_ms": 2662.775754928589, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3894}}}
{"qid": "-5004457603684974952", "question": "who is playing the halftime show at super bowl 2016", "gold_answers": ["Beyoncé", "Coldplay", "Bruno Mars"], "shots": {"0": {"k": 0, "answer": "Bruno Mars", "latency_ms": 1088.60445022583, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Coldplay", "latency_ms": 1213.1998538970947, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 634}, "2": {"k": 2, "answer": "Coldplay", "latency_ms": 1642.075777053833, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1274}, "3": {"k": 3, "answer": "Coldplay", "latency_ms": 2069.990396499634, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1921}, "4": {"k": 4, "answer": "Coldplay", "latency_ms": 2044.0473556518555, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2554}, "5": {"k": 5, "answer": "Coldplay with special guest performers Beyoncé and Bruno Mars", "latency_ms": 4625.394344329834, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 3216}, "6": {"k": 6, "answer": "Coldplay, Beyoncé, Bruno Mars", "latency_ms": 3763.624906539917, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3860}}}
{"qid": "7420920477821898435", "question": "to aru kagaku no railgun s episode 3", "gold_answers": ["Project Radio Noise"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1242.1655654907227, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1604.4461727142334, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 663}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2491.5668964385986, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1280}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 3241.335153579712, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1950}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 3381.0553550720215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2570}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3825.9193897247314, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3175}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3908.7417125701904, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3881}}}
{"qid": "7478795216476346339", "question": "who won the 2017 sports personality of the year", "gold_answers": ["Mo Farah"], "shots": {"0": {"k": 0, "answer": "Tayler Hill", "latency_ms": 1257.8814029693604, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1399.5022773742676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2048.1159687042236, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1294}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2788.6407375335693, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1954}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2518.98455619812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2606}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3033.3261489868164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3265}, "6": {"k": 6, "answer": "Lewis Hamilton", "latency_ms": 2525.2833366394043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3904}}}
{"qid": "7217222058435937287", "question": "where was the world economic forum held this year", "gold_answers": ["Davos"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1229.9673557281494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1819.706678390503, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2030.1446914672852, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1362}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2289.9556159973145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 3271.388530731201, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2734}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3287.6296043395996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3398}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3171.663284301758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4096}}}
{"qid": "-143054837169120955", "question": "where are the giant redwoods located in california", "gold_answers": ["the coast of northern California", "Del Norte County", "Humboldt County"], "shots": {"0": {"k": 0, "answer": "Redwood National and State Parks", "latency_ms": 1616.147518157959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 3109.1601848602295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "Kings Canyon National Park and Giant Sequoia National Monument on the western slope of California's Sierra Nevada", "latency_ms": 5451.814651489258, "em": 0.0, "f1": 0.1, "containment": 0.0, "context_chars": 1281}, "3": {"k": 3, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 3891.7365074157715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1924}, "4": {"k": 4, "answer": "Kings Canyon National Park and Giant Sequoia National Monument on the western slope of California's Sierra Nevada", "latency_ms": 6157.0916175842285, "em": 0.0, "f1": 0.1, "containment": 0.0, "context_chars": 2536}, "5": {"k": 5, "answer": "Kings Canyon National Park and Giant Sequoia National Monument on the western slope of California's Sierra Nevada", "latency_ms": 6750.495433807373, "em": 0.0, "f1": 0.1, "containment": 0.0, "context_chars": 3208}, "6": {"k": 6, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 4623.108863830566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3784}}}
{"qid": "4892429640540595424", "question": "who has made the most premier league appearances", "gold_answers": ["Gareth Barry"], "shots": {"0": {"k": 0, "answer": "Alan Shearer", "latency_ms": 1112.170934677124, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1791.6975021362305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 594}, "2": {"k": 2, "answer": "Gareth Barry", "latency_ms": 1834.4898223876953, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1237}, "3": {"k": 3, "answer": "Gareth Barry", "latency_ms": 2149.6899127960205, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1857}, "4": {"k": 4, "answer": "Gareth Barry", "latency_ms": 2362.776279449463, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2497}, "5": {"k": 5, "answer": "Gareth Barry", "latency_ms": 2934.0102672576904, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3119}, "6": {"k": 6, "answer": "Gareth Barry", "latency_ms": 2774.7232913970947, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3755}}}
{"qid": "5533906981191706877", "question": "who was the first chief minister of west bengal", "gold_answers": ["Prafulla Chandra Ghosh"], "shots": {"0": {"k": 0, "answer": "P. C. Bagchi", "latency_ms": 1689.7931098937988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "P. C. Bagchi", "latency_ms": 2008.9993476867676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "H. S. Suhrawardy", "latency_ms": 2971.670150756836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1325}, "3": {"k": 3, "answer": "P. C. Banerjee", "latency_ms": 3137.444019317627, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2054}, "4": {"k": 4, "answer": "H. S. Suhrawardy", "latency_ms": 3591.996908187866, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2660}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3187.4372959136963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3326}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2998.5008239746094, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3979}}}
{"qid": "8823076125131408628", "question": "who sings got my mind set on you", "gold_answers": ["George Harrison", "James Ray"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1310.6026649475098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1858.128547668457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2019.4752216339111, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1234}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2315.5860900878906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1816}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2750.1349449157715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2428}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2911.360502243042, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3000}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3005.493640899658, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3634}}}
{"qid": "9168660859139861094", "question": "two main types of research methods used in psychology", "gold_answers": ["Correlational research", "Descriptive research", "quantitative data", "Quantitative psychological research", "qualitative", "quantitative", "Qualitative psychological research", "Experimental research", "qualitative data"], "shots": {"0": {"k": 0, "answer": "experimental and observational", "latency_ms": 1068.3112144470215, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "qualitative, quantitative", "latency_ms": 1569.7381496429443, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 735}, "2": {"k": 2, "answer": "qualitative and quantitative", "latency_ms": 1941.678762435913, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1482}, "3": {"k": 3, "answer": "qualitative and quantitative", "latency_ms": 2037.8484725952148, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2166}, "4": {"k": 4, "answer": "qualitative and quantitative", "latency_ms": 3249.875783920288, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2932}, "5": {"k": 5, "answer": "qualitative and quantitative", "latency_ms": 2900.646448135376, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3695}, "6": {"k": 6, "answer": "qualitative and quantitative", "latency_ms": 3455.474615097046, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 4504}}}
{"qid": "-7073063191952855764", "question": "who plays peter in what we do in the shadows", "gold_answers": ["Ben Fransham"], "shots": {"0": {"k": 0, "answer": "Matt Berry", "latency_ms": 836.0676765441895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1375.3106594085693, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 635}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2108.858346939087, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1255}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 3059.089422225952, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1867}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2653.026819229126, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2500}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3518.329858779907, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3094}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3625.2288818359375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3730}}}
{"qid": "7901746249864619718", "question": "when does the 14th season of grey's anatomy come out", "gold_answers": ["September 28 , 2017"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1321.549892425537, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 28, 2017", "latency_ms": 3070.962905883789, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "September 28, 2017", "latency_ms": 3310.0216388702393, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1364}, "3": {"k": 3, "answer": "September 28, 2017", "latency_ms": 3765.8421993255615, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2007}, "4": {"k": 4, "answer": "September 28, 2017", "latency_ms": 4550.969362258911, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2695}, "5": {"k": 5, "answer": "September 28, 2017", "latency_ms": 4262.746810913086, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3363}, "6": {"k": 6, "answer": "September 28, 2017", "latency_ms": 4976.04775428772, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4005}}}
{"qid": "-896127746191949637", "question": "when was the first hunger games book published", "gold_answers": ["September 14 , 2008", "2008"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 1290.076732635498, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 14, 2008", "latency_ms": 2794.3084239959717, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 670}, "2": {"k": 2, "answer": "September 14, 2008", "latency_ms": 3303.593397140503, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "September 14, 2008", "latency_ms": 3764.1122341156006, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1949}, "4": {"k": 4, "answer": "September 14, 2008", "latency_ms": 4212.8589153289795, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2609}, "5": {"k": 5, "answer": "September 14, 2008", "latency_ms": 4249.872446060181, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3258}, "6": {"k": 6, "answer": "September 14, 2008", "latency_ms": 5019.931793212891, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3913}}}
{"qid": "8044909700499569711", "question": "how much is the united states in debt to china", "gold_answers": ["$1.06 trillion", "1,200.5"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1222.5677967071533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1389.7264003753662, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 642}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2034.1777801513672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1286}, "3": {"k": 3, "answer": "10 percent of total U.S. public debt", "latency_ms": 3495.9447383880615, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1918}, "4": {"k": 4, "answer": "$1.16 trillion", "latency_ms": 2814.948081970215, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2566}, "5": {"k": 5, "answer": "$1.26 trillion", "latency_ms": 4194.703102111816, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3186}, "6": {"k": 6, "answer": "$1.26 trillion", "latency_ms": 3763.843536376953, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3834}}}
{"qid": "-3123234845590594961", "question": "where did the race in grease take place", "gold_answers": ["Thunder Road"], "shots": {"0": {"k": 0, "answer": "California", "latency_ms": 691.0712718963623, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1519.5963382720947, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 602}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1884.713888168335, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1207}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2221.703290939331, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1820}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2515.0015354156494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2467}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2803.7564754486084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3102}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2990.5920028686523, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3728}}}
{"qid": "4326992806019599297", "question": "what is the maximum data rate for the 802.11a standard select one", "gold_answers": ["54 Mbit / s"], "shots": {"0": {"k": 0, "answer": "54 Mbps", "latency_ms": 1038.3625030517578, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1608.37721824646, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 674}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1946.535348892212, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1336}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 3048.1553077697754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2029}, "4": {"k": 4, "answer": "54 Mbit/s", "latency_ms": 3399.5113372802734, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2659}, "5": {"k": 5, "answer": "54 Mbit/s", "latency_ms": 4054.574966430664, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3346}, "6": {"k": 6, "answer": "54 Mbit/s", "latency_ms": 4272.230386734009, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3992}}}
{"qid": "6144099837933283715", "question": "a good that can be used in place of another good", "gold_answers": ["A substitute good", "substitute good"], "shots": {"0": {"k": 0, "answer": "substitute good", "latency_ms": 1050.9722232818604, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "grease", "latency_ms": 1362.0858192443848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 550}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1942.4808025360107, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1183}, "3": {"k": 3, "answer": "substitute good", "latency_ms": 1671.9558238983154, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1786}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2442.1722888946533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2332}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2735.9912395477295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2899}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3348.862886428833, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3473}}}
{"qid": "-4621948696356225060", "question": "where in the bible can i find the lord's prayer", "gold_answers": ["the Gospel of Luke", "the Gospel of Matthew"], "shots": {"0": {"k": 0, "answer": "Matthew 6:9-13", "latency_ms": 2036.729335784912, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1374.0267753601074, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 562}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1924.4871139526367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1230}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2086.3771438598633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1925}, "4": {"k": 4, "answer": "not in all; excluded in critical editions; absent in oldest manuscripts", "latency_ms": 4356.8902015686035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2560}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3619.511127471924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3221}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2974.578619003296, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3938}}}
{"qid": "6941993254369956277", "question": "when was the last time nebraska voted democrat", "gold_answers": ["1964"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 1256.8929195404053, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1936", "latency_ms": 1808.3908557891846, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 692}, "2": {"k": 2, "answer": "1936", "latency_ms": 2118.328094482422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1374}, "3": {"k": 3, "answer": "1996", "latency_ms": 2327.511787414551, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2059}, "4": {"k": 4, "answer": "2008", "latency_ms": 2461.5635871887207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2761}, "5": {"k": 5, "answer": "2006", "latency_ms": 3041.175127029419, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3408}, "6": {"k": 6, "answer": "2006", "latency_ms": 2974.8778343200684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4059}}}
{"qid": "-8955197065025093046", "question": "who become the ceo of it wipro company in 2016", "gold_answers": ["Abid Ali Neemuchwala"], "shots": {"0": {"k": 0, "answer": "Ajay Bhatia", "latency_ms": 1432.5780868530273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wipro CEO in 2016: [Name not provided in context]", "latency_ms": 4472.49436378479, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 682}, "2": {"k": 2, "answer": "COO from April 2015, was appointed CEO with effect from 1 February 2016.", "latency_ms": 6813.440561294556, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1322}, "3": {"k": 3, "answer": "COO from April 2015, was appointed CEO with effect from 1 February 2016.", "latency_ms": 6698.99320602417, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1928}, "4": {"k": 4, "answer": "COO from April 2015, was appointed CEO with effect from 1 February 2016.", "latency_ms": 7134.89294052124, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2593}, "5": {"k": 5, "answer": "COO from April 2015, was appointed CEO with effect from 1 February 2016.", "latency_ms": 8222.779750823975, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3182}, "6": {"k": 6, "answer": "COO from April 2015, was appointed CEO with effect from 1 February 2016.", "latency_ms": 7672.8925704956055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3818}}}
{"qid": "-3010425579521817304", "question": "who does eric end up with in gossip girl", "gold_answers": ["Jenny"], "shots": {"0": {"k": 0, "answer": "Serena", "latency_ms": 825.1299858093262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1357.5718402862549, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 609}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1971.6839790344238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1235}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1842.3035144805908, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1838}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 3236.9441986083984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2444}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2808.5989952087402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3069}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3307.8057765960693, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3665}}}
{"qid": "7264112453286983469", "question": "who plays unis in she's the man", "gold_answers": ["Emily Perkins"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1228.8055419921875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1615.7708168029785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2015.9873962402344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1206}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2915.6570434570312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1835}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2782.1288108825684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2423}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3097.2838401794434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3098}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3313.2987022399902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3734}}}
{"qid": "2627695648792729859", "question": "who played booster in jingle all the way", "gold_answers": ["Curtis Armstrong"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1220.792531967163, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1613.962173461914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 652}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2012.3984813690186, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1310}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2548.6297607421875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1973}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2450.8423805236816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2606}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3102.7979850769043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3274}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3449.4788646698, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3894}}}
{"qid": "-9169976182548289414", "question": "who is recognized as the founder of islam", "gold_answers": ["the Islamic prophet Muhammad", "Muhammad"], "shots": {"0": {"k": 0, "answer": "Muhammad", "latency_ms": 836.2629413604736, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1794.2614555358887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 679}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2004.3420791625977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1318}, "3": {"k": 3, "answer": "Muhammad", "latency_ms": 1932.755708694458, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1956}, "4": {"k": 4, "answer": "Muhammad", "latency_ms": 2135.4148387908936, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2600}, "5": {"k": 5, "answer": "Muhammad", "latency_ms": 2670.991897583008, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3198}, "6": {"k": 6, "answer": "Muhammad", "latency_ms": 3495.3744411468506, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3824}}}
{"qid": "-6696747560626271522", "question": "who plays emma in air bud world pup", "gold_answers": ["Brittany Paige Bouck"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1257.603645324707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1816.3883686065674, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1961.9309902191162, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1173}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2335.1943492889404, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1776}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2535.046339035034, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2393}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3514.8231983184814, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3000}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2979.8519611358643, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3613}}}
{"qid": "5871681964137793716", "question": "how old was sasuke when his clan died", "gold_answers": ["seven"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1243.271827697754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "seven", "latency_ms": 774.9483585357666, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 641}, "2": {"k": 2, "answer": "7", "latency_ms": 1401.569128036499, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1247}, "3": {"k": 3, "answer": "7", "latency_ms": 1709.1538906097412, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1915}, "4": {"k": 4, "answer": "7", "latency_ms": 1848.1357097625732, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2521}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3093.4810638427734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3179}, "6": {"k": 6, "answer": "7", "latency_ms": 2790.5352115631104, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3832}}}
{"qid": "8552176802923865025", "question": "where do some of the natural air pollutants come from", "gold_answers": ["wildfires", "Dust", "Vegetation", "Carbon dioxide", "Volcanic activity"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1236.59348487854, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "volcanic eruptions", "latency_ms": 1551.2115955352783, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "volcanic eruptions", "latency_ms": 2296.705722808838, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1333}, "3": {"k": 3, "answer": "volcanic eruptions", "latency_ms": 2065.937042236328, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2028}, "4": {"k": 4, "answer": "volcanic eruptions", "latency_ms": 2798.837423324585, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2715}, "5": {"k": 5, "answer": "volcanic eruptions", "latency_ms": 2652.066469192505, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3319}, "6": {"k": 6, "answer": "forest fires, volcanic eruptions, wind erosion, pollen dispersal, evaporation of organic compounds, and natural radioactivity", "latency_ms": 7365.9820556640625, "em": 0.0, "f1": 0.1176470588235294, "containment": 0.0, "context_chars": 4070}}}
{"qid": "-2630525064809360052", "question": "who is the pastor of new birth missionary baptist church now", "gold_answers": ["Stephen A. Davis"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1230.351209640503, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1607.0656776428223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 675}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1963.191270828247, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1340}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2468.7256813049316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1976}, "4": {"k": 4, "answer": "Jamal Bryant", "latency_ms": 2318.0160522460938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2654}, "5": {"k": 5, "answer": "Jamal Bryant", "latency_ms": 2737.8501892089844, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3327}, "6": {"k": 6, "answer": "Jamal Bryant", "latency_ms": 3429.044485092163, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3958}}}
{"qid": "6434931911289860123", "question": "who is doing 2018 super bowl half time show", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Mark Ronson", "latency_ms": 1044.780969619751, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1485.081672668457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 623}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1831.5529823303223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1219}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2293.5402393341064, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1855}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2545.3977584838867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2508}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3240.422487258911, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3117}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2918.7140464782715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3791}}}
{"qid": "8869318258439148973", "question": "who is playing the halftime show for the superbowl", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1261.404037475586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1363.6021614074707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1940.547227859497, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1263}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2037.874698638916, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1871}, "4": {"k": 4, "answer": "Coldplay with special guest performers Beyoncé and Bruno Mars", "latency_ms": 3920.7208156585693, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2507}, "5": {"k": 5, "answer": "Coldplay", "latency_ms": 2383.1217288970947, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3089}, "6": {"k": 6, "answer": "Coldplay", "latency_ms": 2697.171211242676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3726}}}
{"qid": "-8359866474144720253", "question": "who hosted they think it's all over", "gold_answers": ["Des Lynam", "Lee Mack", "comedian Nick Hancock", "Nick Hancock"], "shots": {"0": {"k": 0, "answer": "They Think It's All Over was hosted by Bob Barker.", "latency_ms": 2893.7747478485107, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Alex Riley", "latency_ms": 976.4084815979004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 604}, "2": {"k": 2, "answer": "Alex Riley", "latency_ms": 1620.4724311828613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1248}, "3": {"k": 3, "answer": "Alex Riley", "latency_ms": 1856.825351715088, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1904}, "4": {"k": 4, "answer": "Alex Riley", "latency_ms": 2301.3575077056885, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2549}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2986.1552715301514, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3177}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2968.4762954711914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3752}}}
{"qid": "-1206653570097564556", "question": "where does the movie proof of life take place", "gold_answers": ["The Republic of Tecala"], "shots": {"0": {"k": 0, "answer": "India", "latency_ms": 634.6297264099121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1839.0164375305176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 665}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2017.2133445739746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1332}, "3": {"k": 3, "answer": "London and Leipzig", "latency_ms": 2093.238592147827, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1974}, "4": {"k": 4, "answer": "London and Leipzig", "latency_ms": 2322.4353790283203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2684}, "5": {"k": 5, "answer": "London and Leipzig", "latency_ms": 2943.2098865509033, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3282}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3011.2621784210205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3879}}}
{"qid": "6144521615402262404", "question": "where does us highway 1 start and end", "gold_answers": ["Fort Kent , Maine", "to Key West , Florida", "Key West , Florida"], "shots": {"0": {"k": 0, "answer": "U.S. Highway 1 starts in Miami, Florida, and ends in Bar Harbor, Maine.", "latency_ms": 4467.790603637695, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1583.8706493377686, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2152.773857116699, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1328}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2341.9649600982666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1917}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2450.9341716766357, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2548}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2975.137233734131, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3132}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3836.242914199829, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3758}}}
{"qid": "-8631190913794477985", "question": "who scored the most points in a single game in the nba", "gold_answers": ["Wilt Chamberlain"], "shots": {"0": {"k": 0, "answer": "Wilt Chamberlain", "latency_ms": 1233.9136600494385, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1587.165117263794, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 552}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1990.6446933746338, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1103}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2275.9313583374023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1713}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2568.079710006714, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2281}, "5": {"k": 5, "answer": "Wilt Chamberlain", "latency_ms": 3308.54868888855, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2906}, "6": {"k": 6, "answer": "Wilt Chamberlain", "latency_ms": 3003.21626663208, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3538}}}
{"qid": "199896377614951764", "question": "where is natural gas found in the us", "gold_answers": ["Texas", "Pennsylvania", "Colorado", "New Mexico", "West Virginia", "Arkansas", "Wyoming", "Louisiana"], "shots": {"0": {"k": 0, "answer": "Texas, North Dakota, Wyoming, Pennsylvania, and Utah", "latency_ms": 2671.2276935577393, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "shale formations", "latency_ms": 1173.8104820251465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "in shale formations", "latency_ms": 1683.6047172546387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1193}, "3": {"k": 3, "answer": "shale formations, American Southwest, Southern California", "latency_ms": 3317.2366619110107, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1879}, "4": {"k": 4, "answer": "shale formations, American Southwest, Southern California", "latency_ms": 3579.315185546875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2555}, "5": {"k": 5, "answer": "Louisiana, Texas, California, and Alaska", "latency_ms": 4103.645086288452, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3187}, "6": {"k": 6, "answer": "deep underground rock formations, associated with other hydrocarbon reservoirs in coal beds, and as methane clathrates", "latency_ms": 6909.255027770996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3860}}}
{"qid": "8986775680996674901", "question": "when were the first general elections held in independent india", "gold_answers": ["1951 -- 52"], "shots": {"0": {"k": 0, "answer": "15 August 1947", "latency_ms": 2112.3569011688232, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1951", "latency_ms": 1596.0047245025635, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 634}, "2": {"k": 2, "answer": "1951–52", "latency_ms": 2646.169900894165, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1299}, "3": {"k": 3, "answer": "1951–52", "latency_ms": 3129.917621612549, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1940}, "4": {"k": 4, "answer": "1951–52", "latency_ms": 3071.765184402466, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2601}, "5": {"k": 5, "answer": "1951–52", "latency_ms": 3933.0368041992188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3302}, "6": {"k": 6, "answer": "1951–52", "latency_ms": 3894.303560256958, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3999}}}
{"qid": "700604097171850168", "question": "what was the tower of london originally used for", "gold_answers": ["as a royal residence", "a royal residence"], "shots": {"0": {"k": 0, "answer": "Royal palace and fortress", "latency_ms": 1250.3535747528076, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Military fortress and royal palace", "latency_ms": 1580.221176147461, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 568}, "2": {"k": 2, "answer": "A fortress and royal palace", "latency_ms": 2149.4152545928955, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1166}, "3": {"k": 3, "answer": "A fortress and royal palace", "latency_ms": 2096.47536277771, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1773}, "4": {"k": 4, "answer": "A fortress and royal palace", "latency_ms": 2684.011220932007, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2406}, "5": {"k": 5, "answer": "storage and as a fortress", "latency_ms": 3405.7552814483643, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2990}, "6": {"k": 6, "answer": "castle and storage", "latency_ms": 2805.0479888916016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3646}}}
{"qid": "6745303307988470742", "question": "what type of fuel goes in a zippo", "gold_answers": ["butane", "lighter fluid"], "shots": {"0": {"k": 0, "answer": "Zippo uses lighter fluid or white gas.", "latency_ms": 2487.168312072754, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Zippo uses a wick and a fuel that is typically a blend of mineral spirits or a similar flammable liquid. The exact composition is proprietary, but it is commonly known as \"Zippo fuel\" or \"Zippo lighter fluid.\" \n\nAnswer: mineral spirits", "latency_ms": 11239.315509796143, "em": 0.0, "f1": 0.1081081081081081, "containment": 1.0, "context_chars": 590}, "2": {"k": 2, "answer": "Zippo lighter fluid", "latency_ms": 2028.425693511963, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1218}, "3": {"k": 3, "answer": "butane", "latency_ms": 1867.8028583526611, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1815}, "4": {"k": 4, "answer": "butane", "latency_ms": 2957.563638687134, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2385}, "5": {"k": 5, "answer": "butane", "latency_ms": 2623.260498046875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3011}, "6": {"k": 6, "answer": "butane", "latency_ms": 2557.3577880859375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3676}}}
{"qid": "-2060506905455252030", "question": "first jnanpith award was an autor of which language", "gold_answers": ["Malayalam"], "shots": {"0": {"k": 0, "answer": "Kannada", "latency_ms": 1046.262502670288, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "English", "latency_ms": 991.668701171875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "English", "latency_ms": 1468.987226486206, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1310}, "3": {"k": 3, "answer": "English", "latency_ms": 1695.098876953125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1965}, "4": {"k": 4, "answer": "English", "latency_ms": 2453.2556533813477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2610}, "5": {"k": 5, "answer": "English", "latency_ms": 2496.9818592071533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3280}, "6": {"k": 6, "answer": "Malayalam", "latency_ms": 3045.414924621582, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3951}}}
{"qid": "4738663028303377024", "question": "who played the mad hatter in the batman tv show", "gold_answers": ["Roddy McDowall", "David Wayne", "Benedict Samuel"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1243.8068389892578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1401.4732837677002, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 672}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1875.5342960357666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1292}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2303.2712936401367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1901}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2505.847930908203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2544}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3428.203344345093, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3214}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3520.1523303985596, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3845}}}
{"qid": "-1650946015201779846", "question": "how many cracker barrels in the united states", "gold_answers": ["639", "more than 600"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1283.3383083343506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "645", "latency_ms": 1168.5254573822021, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "645", "latency_ms": 1856.7721843719482, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1253}, "3": {"k": 3, "answer": "645", "latency_ms": 2276.019811630249, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1908}, "4": {"k": 4, "answer": "645", "latency_ms": 2291.722059249878, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2592}, "5": {"k": 5, "answer": "645", "latency_ms": 2958.0395221710205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3248}, "6": {"k": 6, "answer": "645", "latency_ms": 2883.056879043579, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3892}}}
{"qid": "-3422350952909582054", "question": "in photosynthesis the carbon in co2 is initially fixed to what molecule", "gold_answers": ["3 - phosphoglycerate"], "shots": {"0": {"k": 0, "answer": "3-phosphoglycerate", "latency_ms": 1699.587106704712, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "3-phosphoglycerate", "latency_ms": 1967.8494930267334, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 724}, "2": {"k": 2, "answer": "glucose", "latency_ms": 1476.4094352722168, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1403}, "3": {"k": 3, "answer": "carbohydrates", "latency_ms": 2158.3316326141357, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2067}, "4": {"k": 4, "answer": "3-phosphoglycerate", "latency_ms": 2918.7967777252197, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2871}, "5": {"k": 5, "answer": "glucose", "latency_ms": 2828.5138607025146, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3552}, "6": {"k": 6, "answer": "3-phosphoglycerate (3-PGA)", "latency_ms": 4414.352893829346, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4261}}}
{"qid": "-2704708103393811482", "question": "who voices randy in f is for family", "gold_answers": ["T.J. Miller"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1257.5583457946777, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1388.7810707092285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 567}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2012.4974250793457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1195}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2240.1862144470215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1851}, "4": {"k": 4, "answer": "April Stewart", "latency_ms": 2133.287191390991, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2509}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3136.4636421203613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3138}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3260.967016220093, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3769}}}
{"qid": "5482012214308896475", "question": "how many countries does cadbury sell its products", "gold_answers": ["more than 50", "more than 50 countries worldwide"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1225.9433269500732, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1593.8069820404053, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 673}, "2": {"k": 2, "answer": "30", "latency_ms": 1536.7815494537354, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "more than fifty", "latency_ms": 2115.1533126831055, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2026}, "4": {"k": 4, "answer": "more than fifty", "latency_ms": 2517.230749130249, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2730}, "5": {"k": 5, "answer": "more than fifty", "latency_ms": 2847.0942974090576, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3438}, "6": {"k": 6, "answer": "more than fifty", "latency_ms": 2768.3255672454834, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 4084}}}
{"qid": "-5737509619250561344", "question": "ethiopia flight 961 crashes in to the sea", "gold_answers": ["23 November 1996"], "shots": {"0": {"k": 0, "answer": "1996", "latency_ms": 1240.9613132476807, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indian Ocean near Grande Comore, Comoros Islands", "latency_ms": 3067.7084922790527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "Indian Ocean near Grande Comore, Comoros Islands", "latency_ms": 3267.451286315918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1359}, "3": {"k": 3, "answer": "no", "latency_ms": 1845.5302715301514, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1996}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2855.114698410034, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2657}, "5": {"k": 5, "answer": "no", "latency_ms": 3168.0054664611816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3311}, "6": {"k": 6, "answer": "no", "latency_ms": 3214.060068130493, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3968}}}
{"qid": "-7175456993713717070", "question": "what does istj mean in a personality test", "gold_answers": ["Extroverted Thinking ( Te )", "Extroverted Intuition ( Ne )", "Introverted Feeling ( Fi )", "Introverted Sensing ( Si )"], "shots": {"0": {"k": 0, "answer": "ISTJ", "latency_ms": 836.4851474761963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "ISTJ", "latency_ms": 962.435245513916, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 709}, "2": {"k": 2, "answer": "ISTJ means \"Introverted, Sensing, Thinking, Judging\" in a personality test.", "latency_ms": 5398.211717605591, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1337}, "3": {"k": 3, "answer": "ISTJ is an abbreviation for one of sixteen personality types in the Myers–Briggs Type Indicator (MBTI).", "latency_ms": 6243.680953979492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2063}, "4": {"k": 4, "answer": "ISTJ means Introversion, Sensing, Thinking, Judgment in the Myers-Briggs Type Indicator personality test.", "latency_ms": 6334.337949752808, "em": 0.0, "f1": 0.13333333333333333, "containment": 0.0, "context_chars": 2706}, "5": {"k": 5, "answer": "ISTJ means Introversion, Sensing, Thinking, Judgment in the Myers–Briggs Type Indicator (MBTI).", "latency_ms": 7643.414735794067, "em": 0.0, "f1": 0.14285714285714288, "containment": 0.0, "context_chars": 3367}, "6": {"k": 6, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 3959.991693496704, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 4054}}}
{"qid": "4437170343606050664", "question": "who does the voice of ash in sing", "gold_answers": ["Scarlett Johansson"], "shots": {"0": {"k": 0, "answer": "John Mulaney", "latency_ms": 1142.514705657959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1889.7483348846436, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2082.4532508850098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1185}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2317.48366355896, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1813}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2787.210702896118, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2412}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2781.229257583618, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3011}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2977.637767791748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3606}}}
{"qid": "-8773010039077552394", "question": "who scored the most points in their nba career", "gold_answers": ["Kareem Abdul - Jabbar"], "shots": {"0": {"k": 0, "answer": "Kareem Abdul-Jabbar", "latency_ms": 1852.595567703247, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1794.5866584777832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 552}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2045.785903930664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1144}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2613.7568950653076, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1730}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 3126.3961791992188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2335}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2954.6523094177246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2900}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3753.399610519409, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3496}}}
{"qid": "-3278224144622911032", "question": "with a land area of 54 314 square miles where does wisconsin rank among the 50 states", "gold_answers": ["25"], "shots": {"0": {"k": 0, "answer": "10th", "latency_ms": 1070.2741146087646, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1632.9131126403809, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 597}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2274.0023136138916, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1191}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2476.421356201172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1784}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 3535.376787185669, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2396}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3867.4278259277344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2983}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 4121.108055114746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3540}}}
{"qid": "-3466298422174960041", "question": "who plays charles on have and have nots", "gold_answers": ["Nick Sager", "Nick Sagar"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1233.3390712738037, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1383.2604885101318, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2038.461446762085, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1270}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2594.035863876343, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1891}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2494.833469390869, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2489}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3243.9489364624023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3119}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3300.2686500549316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3756}}}
{"qid": "1803248910342766083", "question": "who wrote the book my country my life", "gold_answers": ["L.K. Advani"], "shots": {"0": {"k": 0, "answer": "Nelson Mandela", "latency_ms": 1076.3673782348633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "L. K. Advani", "latency_ms": 2109.4746589660645, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "L. K. Advani", "latency_ms": 2499.1579055786133, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1158}, "3": {"k": 3, "answer": "L. K. Advani", "latency_ms": 2687.0338916778564, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1794}, "4": {"k": 4, "answer": "L. K. Advani", "latency_ms": 3715.900659561157, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2480}, "5": {"k": 5, "answer": "L. K. Advani", "latency_ms": 3523.3218669891357, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3080}, "6": {"k": 6, "answer": "L. K. Advani", "latency_ms": 3413.4364128112793, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3738}}}
{"qid": "-1176280233636154422", "question": "when did the east india company take control of india", "gold_answers": ["1757", "1799", "1612", "in 1757"], "shots": {"0": {"k": 0, "answer": "1757", "latency_ms": 1249.46928024292, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1757", "latency_ms": 1380.4011344909668, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 647}, "2": {"k": 2, "answer": "1757", "latency_ms": 2047.2702980041504, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1317}, "3": {"k": 3, "answer": "1757", "latency_ms": 2669.5168018341064, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1952}, "4": {"k": 4, "answer": "1757", "latency_ms": 2855.0450801849365, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2584}, "5": {"k": 5, "answer": "1757", "latency_ms": 3189.511775970459, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3244}, "6": {"k": 6, "answer": "1757", "latency_ms": 2966.7041301727295, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3884}}}
{"qid": "5911119091653953181", "question": "where does the water come from to fill lake eyre", "gold_answers": ["Warburton River"], "shots": {"0": {"k": 0, "answer": "rainfall and inflows from surrounding rivers and groundwater", "latency_ms": 2464.857578277588, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "river systems of semi-arid inland Queensland", "latency_ms": 2200.535535812378, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "river systems of semi-arid inland Queensland, roughly to the north", "latency_ms": 3735.644817352295, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 1199}, "3": {"k": 3, "answer": "river systems of semi-arid inland Queensland, roughly to the north", "latency_ms": 4091.1128520965576, "em": 0.0, "f1": 0.1818181818181818, "containment": 0.0, "context_chars": 1786}, "4": {"k": 4, "answer": "river systems of semi-arid inland Queensland, roughly to the north; massive floods in the Georgina River", "latency_ms": 5986.0804080963135, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 2374}, "5": {"k": 5, "answer": "rivers from the north-east part of the Lake Eyre Basin, particularly the Georgina River", "latency_ms": 6026.562690734863, "em": 0.0, "f1": 0.15384615384615385, "containment": 0.0, "context_chars": 3005}, "6": {"k": 6, "answer": "rivers from the north-east part of the Lake Eyre Basin, including the Georgina River", "latency_ms": 6035.454511642456, "em": 0.0, "f1": 0.15384615384615385, "containment": 0.0, "context_chars": 3627}}}
{"qid": "5557104327881436095", "question": "who is the founder of google and when was it founded", "gold_answers": ["1998", "Sergey Brin", "Larry Page and Sergey Brin", "Larry Page"], "shots": {"0": {"k": 0, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 2850.7163524627686, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Larry Page and Sergey Brin; September 15, 1997", "latency_ms": 4288.53440284729, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 676}, "2": {"k": 2, "answer": "Larry Page and Sergey Brin; September 15, 1997", "latency_ms": 4643.482685089111, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 1292}, "3": {"k": 3, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 4017.6966190338135, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 1916}, "4": {"k": 4, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 5032.810926437378, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 2551}, "5": {"k": 5, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 4696.338176727295, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 3186}, "6": {"k": 6, "answer": "Larry Page and Sergey Brin, 1999", "latency_ms": 4935.271263122559, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 3830}}}
{"qid": "-1333328812971005142", "question": "who did the dominican republic gain its independence from", "gold_answers": ["Haiti"], "shots": {"0": {"k": 0, "answer": "Spain", "latency_ms": 649.3287086486816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Spain", "latency_ms": 950.6516456604004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "Haiti", "latency_ms": 1707.465410232544, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1342}, "3": {"k": 3, "answer": "Haiti", "latency_ms": 2208.186388015747, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2022}, "4": {"k": 4, "answer": "Haiti", "latency_ms": 2292.7610874176025, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2736}, "5": {"k": 5, "answer": "Haiti", "latency_ms": 3107.97381401062, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3373}, "6": {"k": 6, "answer": "Haiti", "latency_ms": 3070.430278778076, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4093}}}
{"qid": "1122605495295045742", "question": "how did early humans make use of stones during the prehistoric period", "gold_answers": ["start fires", "hunt", "bury their dead"], "shots": {"0": {"k": 0, "answer": "Chipped and shaped for tools", "latency_ms": 1703.0692100524902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "made implements with a sharp edge, a point, or a percussion surface", "latency_ms": 3568.9022541046143, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 670}, "2": {"k": 2, "answer": "made implements with a sharp edge, a point, or a percussion surface", "latency_ms": 4015.324592590332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1316}, "3": {"k": 3, "answer": "made implements with a sharp edge, a point, or a percussion surface", "latency_ms": 4438.1396770477295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1972}, "4": {"k": 4, "answer": "made implements with a sharp edge, a point, or a percussion surface", "latency_ms": 4543.87092590332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2626}, "5": {"k": 5, "answer": "made implements with a sharp edge, a point, or a percussion surface", "latency_ms": 4956.981897354126, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3326}, "6": {"k": 6, "answer": "made implements with a sharp edge, a point, or a percussion surface", "latency_ms": 5019.673585891724, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3975}}}
{"qid": "7402184777947527945", "question": "who wrote the text for jeanie with the light brown hair", "gold_answers": ["Stephen Foster"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 1226.9420623779297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Stephen Foster", "latency_ms": 1418.2982444763184, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 632}, "2": {"k": 2, "answer": "Stephen Foster", "latency_ms": 1668.4694290161133, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1222}, "3": {"k": 3, "answer": "Stephen Foster", "latency_ms": 1952.845573425293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1885}, "4": {"k": 4, "answer": "Stephen Foster", "latency_ms": 2150.951147079468, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2493}, "5": {"k": 5, "answer": "Stephen Foster", "latency_ms": 2570.8813667297363, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3138}, "6": {"k": 6, "answer": "Stephen Foster", "latency_ms": 2562.3245239257812, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3757}}}
{"qid": "1656745334095178455", "question": "the basic steps of rational decision making include defining goals and", "gold_answers": ["Implementation of plans / policies"], "shots": {"0": {"k": 0, "answer": "identifying alternatives", "latency_ms": 1038.2251739501953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "defining goals and identifying alternatives", "latency_ms": 1757.7176094055176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 652}, "2": {"k": 2, "answer": "defining goals and alternatives", "latency_ms": 2157.600164413452, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1345}, "3": {"k": 3, "answer": "defining goals and verifying the problem", "latency_ms": 3180.574655532837, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2121}, "4": {"k": 4, "answer": "defining the problem, information gathering, analyzing solutions, implementing, monitoring, modifying", "latency_ms": 4992.883920669556, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2754}, "5": {"k": 5, "answer": "defining goals and identifying problems", "latency_ms": 2849.698543548584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3406}, "6": {"k": 6, "answer": "defining goals and identifying problems", "latency_ms": 3920.1889038085938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4094}}}
{"qid": "-216657635823030751", "question": "when was the design period known as art deco", "gold_answers": ["1910 -- 1939"], "shots": {"0": {"k": 0, "answer": "1920s–1930s", "latency_ms": 2654.116630554199, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1925", "latency_ms": 1382.443904876709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "1925", "latency_ms": 1994.9264526367188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1343}, "3": {"k": 3, "answer": "1925", "latency_ms": 2433.929920196533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2025}, "4": {"k": 4, "answer": "1925", "latency_ms": 2552.475929260254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2688}, "5": {"k": 5, "answer": "1925", "latency_ms": 3013.1890773773193, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3372}, "6": {"k": 6, "answer": "1925", "latency_ms": 3067.073345184326, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4018}}}
{"qid": "250542332339248886", "question": "where does the bob and tom show broadcast from", "gold_answers": ["WFBQ in Indianapolis , Indiana", "Indianapolis , Indiana"], "shots": {"0": {"k": 0, "answer": "Winston-Salem, North Carolina", "latency_ms": 1843.0838584899902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indianapolis, Indiana", "latency_ms": 1843.8730239868164, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "Indianapolis, Indiana", "latency_ms": 2075.404167175293, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "Indianapolis, Indiana", "latency_ms": 2307.7306747436523, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1825}, "4": {"k": 4, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 3238.1432056427, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2443}, "5": {"k": 5, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 4031.435489654541, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3077}, "6": {"k": 6, "answer": "WFBQ in Indianapolis, Indiana", "latency_ms": 3937.3788833618164, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3685}}}
{"qid": "7610758795954724809", "question": "star wars the clone wars season 3 episode 1", "gold_answers": ["Clone Cadets"], "shots": {"0": {"k": 0, "answer": "\"The Clone Wars\" Season 3, Episode 1: \"The Clone Wars\"", "latency_ms": 4092.949628829956, "em": 0.0, "f1": 0.2, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1885.5786323547363, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 673}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2018.1529521942139, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2559.6792697906494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2030}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2820.3682899475098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2678}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3079.3933868408203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3310}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 4266.382694244385, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3953}}}
{"qid": "2997223939033016160", "question": "what part of brain is responsible for complex thinking", "gold_answers": ["the frontal lobe"], "shots": {"0": {"k": 0, "answer": "prefrontal cortex", "latency_ms": 1237.8225326538086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "prefrontal cortex", "latency_ms": 1386.9576454162598, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "cerebral cortex", "latency_ms": 2058.3415031433105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1260}, "3": {"k": 3, "answer": "cerebral cortex", "latency_ms": 2057.6555728912354, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1876}, "4": {"k": 4, "answer": "cerebral cortex", "latency_ms": 2760.4503631591797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2560}, "5": {"k": 5, "answer": "cerebral cortex", "latency_ms": 2456.5486907958984, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3177}, "6": {"k": 6, "answer": "cerebral cortex", "latency_ms": 3750.041723251343, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3843}}}
{"qid": "2680196226239522881", "question": "who won the champions league final in 2016", "gold_answers": ["Real Madrid"], "shots": {"0": {"k": 0, "answer": "Real Madrid", "latency_ms": 841.3023948669434, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1590.1215076446533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 648}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2074.8884677886963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1274}, "3": {"k": 3, "answer": "Real Madrid", "latency_ms": 1975.8718013763428, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1905}, "4": {"k": 4, "answer": "Real Madrid", "latency_ms": 2802.2243976593018, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2523}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3843.8549041748047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3154}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 4195.718288421631, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3744}}}
{"qid": "-8458006812258546025", "question": "who has the most gold medals in the winter olympics of all time", "gold_answers": ["Norway"], "shots": {"0": {"k": 0, "answer": "Cross-country skier, Norwegian, 1928–1952", "latency_ms": 3951.667070388794, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1605.238437652588, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Canada", "latency_ms": 1479.0773391723633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1228}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2288.3975505828857, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1845}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2518.4366703033447, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2407}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2791.012763977051, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3015}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2978.0001640319824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3626}}}
