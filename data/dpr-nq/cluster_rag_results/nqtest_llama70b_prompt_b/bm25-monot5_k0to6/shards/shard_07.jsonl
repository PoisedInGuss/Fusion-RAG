{"qid": "5306300389470118812", "question": "what was the religion in persia before islam", "gold_answers": ["the Zoroastrian religion", "Zoroastrian"], "shots": {"0": {"k": 0, "answer": "Zoroastrianism", "latency_ms": 2728.0311584472656, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Zoroastrian", "latency_ms": 2123.1000423431396, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 637}, "2": {"k": 2, "answer": "Zoroastrian", "latency_ms": 2985.9116077423096, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1275}, "3": {"k": 3, "answer": "Zoroastrian", "latency_ms": 2366.159200668335, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1985}, "4": {"k": 4, "answer": "Zoroastrian", "latency_ms": 2251.5177726745605, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2650}, "5": {"k": 5, "answer": "Zoroastrian", "latency_ms": 2433.181047439575, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3335}, "6": {"k": 6, "answer": "Zoroastrian", "latency_ms": 2008.176326751709, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3996}}}
{"qid": "4492088526674104172", "question": "why does kerosene oil rise up in the wick of lantern", "gold_answers": ["capillary action"], "shots": {"0": {"k": 0, "answer": "Capillary action.", "latency_ms": 3125.549793243408, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2448.814630508423, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 653}, "2": {"k": 2, "answer": "Capillary action", "latency_ms": 2014.9459838867188, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1241}, "3": {"k": 3, "answer": "Capillary action", "latency_ms": 2214.4110202789307, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1876}, "4": {"k": 4, "answer": "Capillary action", "latency_ms": 1047.4696159362793, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2493}, "5": {"k": 5, "answer": "Capillary action", "latency_ms": 1617.1677112579346, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3112}, "6": {"k": 6, "answer": "Capillary action", "latency_ms": 1849.841594696045, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3689}}}
{"qid": "-4373262132142058334", "question": "mention the chemical change that proinsulin undergo to be able to act as mature insulin", "gold_answers": ["post translational modification"], "shots": {"0": {"k": 0, "answer": "Proteolytic cleavage", "latency_ms": 3129.1513442993164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cleavage of C-peptide", "latency_ms": 4337.716102600098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "Cleavage", "latency_ms": 2448.624610900879, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1282}, "3": {"k": 3, "answer": "Cleavage", "latency_ms": 1551.011323928833, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1986}, "4": {"k": 4, "answer": "Cleavage of C-peptide", "latency_ms": 2014.6005153656006, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2594}, "5": {"k": 5, "answer": "Cleavage of C-peptide", "latency_ms": 4341.146230697632, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3306}, "6": {"k": 6, "answer": "Cleavage of C-peptide", "latency_ms": 2137.4566555023193, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3855}}}
{"qid": "-5298560410953979569", "question": "where does the sweetness of fruit come from", "gold_answers": ["Fructose , or fruit sugar", "Fructose"], "shots": {"0": {"k": 0, "answer": "Fructose", "latency_ms": 1302.8039932250977, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2425.0242710113525, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 677}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2628.809690475464, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1340}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2419.349431991577, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1938}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1631.894588470459, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2653}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1863.1854057312012, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3309}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2310.7876777648926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3894}}}
{"qid": "4344648368832551136", "question": "an object that moves around an external axis is said to be", "gold_answers": ["orbit", "revolution or orbital revolution"], "shots": {"0": {"k": 0, "answer": "rotating", "latency_ms": 1435.0707530975342, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "revolving", "latency_ms": 1536.4220142364502, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "in a revolution or orbital revolution", "latency_ms": 2930.966377258301, "em": 0.0, "f1": 0.888888888888889, "containment": 1.0, "context_chars": 1206}, "3": {"k": 3, "answer": "in revolution or orbital revolution", "latency_ms": 1967.9460525512695, "em": 0.0, "f1": 0.888888888888889, "containment": 1.0, "context_chars": 1864}, "4": {"k": 4, "answer": "in a revolution or orbital revolution", "latency_ms": 1828.4485340118408, "em": 0.0, "f1": 0.888888888888889, "containment": 1.0, "context_chars": 2543}, "5": {"k": 5, "answer": "in revolution or orbital revolution", "latency_ms": 2722.9013442993164, "em": 0.0, "f1": 0.888888888888889, "containment": 1.0, "context_chars": 3112}, "6": {"k": 6, "answer": "in revolution or orbital revolution", "latency_ms": 2162.484645843506, "em": 0.0, "f1": 0.888888888888889, "containment": 1.0, "context_chars": 3772}}}
{"qid": "-724426540432437711", "question": "what was the name of the plant on little shop of horrors", "gold_answers": ["Audrey II"], "shots": {"0": {"k": 0, "answer": "Audrey II", "latency_ms": 1586.6129398345947, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Audrey II", "latency_ms": 1549.537181854248, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 615}, "2": {"k": 2, "answer": "Audrey II", "latency_ms": 1673.1736660003662, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1244}, "3": {"k": 3, "answer": "Audrey II", "latency_ms": 3103.8267612457275, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1914}, "4": {"k": 4, "answer": "Audrey II", "latency_ms": 1097.2356796264648, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2538}, "5": {"k": 5, "answer": "Audrey II", "latency_ms": 1354.686975479126, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3191}, "6": {"k": 6, "answer": "Audrey II", "latency_ms": 2197.044610977173, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3833}}}
{"qid": "-5541610970112826260", "question": "who signed the declaration of independence for maryland", "gold_answers": ["Samuel Chase", "Charles Carroll of Carrollton", "Thomas Stone", "William Paca"], "shots": {"0": {"k": 0, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll.", "latency_ms": 6563.025951385498, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 5196.614742279053, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 673}, "2": {"k": 2, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 5653.765439987183, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 1263}, "3": {"k": 3, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 6421.4067459106445, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 1883}, "4": {"k": 4, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 6360.89563369751, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 2575}, "5": {"k": 5, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton", "latency_ms": 7563.729524612427, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 3263}, "6": {"k": 6, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll of Carrollton.", "latency_ms": 6829.094171524048, "em": 0.0, "f1": 0.5333333333333333, "containment": 1.0, "context_chars": 3926}}}
{"qid": "-86997217831791339", "question": "when does wonder woman come out on blue ray", "gold_answers": ["September 19 , 2017"], "shots": {"0": {"k": 0, "answer": "August 29, 2017", "latency_ms": 3892.6186561584473, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 19, 2017", "latency_ms": 3633.850336074829, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 606}, "2": {"k": 2, "answer": "September 19, 2017", "latency_ms": 2855.201244354248, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1212}, "3": {"k": 3, "answer": "September 19, 2017", "latency_ms": 3495.7430362701416, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1872}, "4": {"k": 4, "answer": "September 19, 2017", "latency_ms": 3191.9665336608887, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2523}, "5": {"k": 5, "answer": "September 19, 2017", "latency_ms": 4049.204111099243, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3144}, "6": {"k": 6, "answer": "September 19, 2017", "latency_ms": 2827.5699615478516, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3795}}}
{"qid": "-8062812506672279762", "question": "what nba team did derrick rose play for", "gold_answers": ["New York Knicks", "Minnesota Timberwolves", "Chicago Bulls", "Cleveland Cavaliers"], "shots": {"0": {"k": 0, "answer": "Chicago Bulls, Cleveland Cavaliers, Minnesota Timberwolves, Detroit Pistons, New York Knicks.", "latency_ms": 8036.57341003418, "em": 0.0, "f1": 0.42857142857142855, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Chicago Bulls, Minnesota Timberwolves", "latency_ms": 3545.360326766968, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 630}, "2": {"k": 2, "answer": "Minnesota Timberwolves, Chicago Bulls, Cleveland Cavaliers, Boston Celtics", "latency_ms": 4772.058963775635, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1238}, "3": {"k": 3, "answer": "Minnesota Timberwolves, Chicago Bulls, Cleveland Cavaliers, Boston Celtics", "latency_ms": 6712.737083435059, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1808}, "4": {"k": 4, "answer": "Minnesota Timberwolves, Chicago Bulls, Cleveland Cavaliers", "latency_ms": 4334.464073181152, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2463}, "5": {"k": 5, "answer": "Chicago Bulls, Minnesota Timberwolves, Cleveland Cavaliers, New York Knicks", "latency_ms": 4985.748767852783, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3066}, "6": {"k": 6, "answer": "Chicago Bulls, Minnesota Timberwolves, Cleveland Cavaliers, New York Knicks", "latency_ms": 5747.160911560059, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3667}}}
{"qid": "7933002036740390435", "question": "where did the term liberal arts come from", "gold_answers": ["the Roman Empire"], "shots": {"0": {"k": 0, "answer": "Ancient Rome", "latency_ms": 1429.0530681610107, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Roman Empire", "latency_ms": 1498.262643814087, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Roman Empire", "latency_ms": 1813.1883144378662, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1351}, "3": {"k": 3, "answer": "Roman Empire", "latency_ms": 1304.3262958526611, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2046}, "4": {"k": 4, "answer": "Roman Empire", "latency_ms": 2003.7167072296143, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2760}, "5": {"k": 5, "answer": "Roman Empire", "latency_ms": 1791.402816772461, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3467}, "6": {"k": 6, "answer": "Roman Empire", "latency_ms": 1806.321382522583, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4188}}}
{"qid": "-2991400700385487821", "question": "who heads the executive department of west virginia government", "gold_answers": ["the Governor of West Virginia", "the governor of West Virginia", "Jim Justice"], "shots": {"0": {"k": 0, "answer": "The Governor", "latency_ms": 1547.3978519439697, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "The governor", "latency_ms": 1526.153326034546, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "The Governor of West Virginia", "latency_ms": 2491.398572921753, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1309}, "3": {"k": 3, "answer": "The Governor of West Virginia", "latency_ms": 2785.116672515869, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1911}, "4": {"k": 4, "answer": "The Governor of West Virginia", "latency_ms": 2693.485736846924, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2616}, "5": {"k": 5, "answer": "The Governor of West Virginia", "latency_ms": 2518.5773372650146, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3308}, "6": {"k": 6, "answer": "The Governor of West Virginia", "latency_ms": 2031.2085151672363, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4002}}}
{"qid": "-9186689755642837558", "question": "how long is the bridge between new brunswick and prince edward island", "gold_answers": ["12.9 - kilometre"], "shots": {"0": {"k": 0, "answer": "13 kilometers", "latency_ms": 1733.067512512207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2190.3369426727295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 686}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1608.2806587219238, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1345}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2489.7255897521973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2032}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1638.2808685302734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2691}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2488.5756969451904, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3279}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3010.943651199341, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3940}}}
{"qid": "8029901619424338449", "question": "when did the uk and us become allies", "gold_answers": ["1940", "Since 1940"], "shots": {"0": {"k": 0, "answer": "1778, then 1814, and fully in 1941", "latency_ms": 4674.592971801758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "After the end of the Second World War in 1945", "latency_ms": 5927.725076675415, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 583}, "2": {"k": 2, "answer": "After the end of the Second World War in 1945", "latency_ms": 4139.368295669556, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1223}, "3": {"k": 3, "answer": "After the end of the Second World War in 1945", "latency_ms": 5383.249282836914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1841}, "4": {"k": 4, "answer": "After the end of the Second World War in 1945", "latency_ms": 5188.055515289307, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2452}, "5": {"k": 5, "answer": "After the end of the Second World War in 1945", "latency_ms": 4764.092683792114, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3132}, "6": {"k": 6, "answer": "After the end of the Second World War in 1945", "latency_ms": 4788.783311843872, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3748}}}
{"qid": "4828649525820722736", "question": "who sang the theme song for the man with the golden gun", "gold_answers": ["Lulu"], "shots": {"0": {"k": 0, "answer": "Lulu", "latency_ms": 1231.9989204406738, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lulu", "latency_ms": 1837.2647762298584, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 582}, "2": {"k": 2, "answer": "Lulu", "latency_ms": 1488.4724617004395, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1225}, "3": {"k": 3, "answer": "Lulu", "latency_ms": 1325.324296951294, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1810}, "4": {"k": 4, "answer": "Lulu", "latency_ms": 1722.473382949829, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2423}, "5": {"k": 5, "answer": "Lulu", "latency_ms": 2781.233072280884, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3016}, "6": {"k": 6, "answer": "Lulu", "latency_ms": 1059.5476627349854, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3686}}}
{"qid": "-7924663090486742985", "question": "american horror story freak show girl kidnapped by clown", "gold_answers": ["Bonnie Lipton"], "shots": {"0": {"k": 0, "answer": "Pennywise is not correct, the answer is: Twisty.", "latency_ms": 6517.586708068848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2655.1899909973145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "Twisty", "latency_ms": 2100.790023803711, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1271}, "3": {"k": 3, "answer": "Twisty", "latency_ms": 1816.5409564971924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1883}, "4": {"k": 4, "answer": "Twisty", "latency_ms": 1821.570873260498, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2582}, "5": {"k": 5, "answer": "the girl", "latency_ms": 1781.5351486206055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3202}, "6": {"k": 6, "answer": "Twisty", "latency_ms": 998.528242111206, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3823}}}
{"qid": "-360675773049242516", "question": "the probability of making a type i error when retaining ho at the .05 level of significance is", "gold_answers": ["5 %"], "shots": {"0": {"k": 0, "answer": "0.05", "latency_ms": 1408.5962772369385, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "0.05", "latency_ms": 1804.4183254241943, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 575}, "2": {"k": 2, "answer": "0.05", "latency_ms": 1584.7623348236084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1151}, "3": {"k": 3, "answer": "0.05", "latency_ms": 2366.7643070220947, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1786}, "4": {"k": 4, "answer": "0.05", "latency_ms": 2551.241874694824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2354}, "5": {"k": 5, "answer": "0.05", "latency_ms": 1838.442087173462, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2946}, "6": {"k": 6, "answer": "0.05", "latency_ms": 2363.278865814209, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3538}}}
{"qid": "5819386267283467034", "question": "what year did the us hockey team won the olympics", "gold_answers": ["1960", "1980", "1960 and 1980"], "shots": {"0": {"k": 0, "answer": "1980", "latency_ms": 727.323055267334, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1960, 1980", "latency_ms": 2326.362371444702, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 659}, "2": {"k": 2, "answer": "1960, 1980", "latency_ms": 3454.397439956665, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1338}, "3": {"k": 3, "answer": "1960, 1980", "latency_ms": 3464.463949203491, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1935}, "4": {"k": 4, "answer": "1960, 1980", "latency_ms": 2823.7321376800537, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2558}, "5": {"k": 5, "answer": "1960, 1980", "latency_ms": 1831.266164779663, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3219}, "6": {"k": 6, "answer": "1960, 1980", "latency_ms": 2707.6003551483154, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3843}}}
{"qid": "3916394566028297311", "question": "who wrote it's a long long way to pasadena", "gold_answers": ["Harry Vanda", "John Young", "David Hemmings", "George Young"], "shots": {"0": {"k": 0, "answer": "Albert Von Tilzer and Edgar Leslie", "latency_ms": 2614.269971847534, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3073.502540588379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 925.3785610198975, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1313}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2895.730972290039, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1948}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2329.6780586242676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2572}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1942.7814483642578, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3201}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2739.795684814453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3837}}}
{"qid": "-2432292250757146771", "question": "who wrote the song i hate you i love you", "gold_answers": ["Angel Benitez", "Olivia O'Brien", "Garrett Nash"], "shots": {"0": {"k": 0, "answer": "Gnash and Olivia O'Brien", "latency_ms": 2587.6801013946533, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ne-Yo and StarGate", "latency_ms": 3194.9992179870605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "Ne-Yo and StarGate", "latency_ms": 3177.3862838745117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "Ne-Yo and StarGate", "latency_ms": 3129.856586456299, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1803}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2122.6065158843994, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2431}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2392.3161029815674, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3072}, "6": {"k": 6, "answer": "Celine Dion and others", "latency_ms": 2761.850357055664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3660}}}
{"qid": "2393389448844632496", "question": "what is loihi and where does it originate from", "gold_answers": ["from the Hawaii hotspot"], "shots": {"0": {"k": 0, "answer": "Loihi, Hawaii, USA", "latency_ms": 2824.000358581543, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Intel's self-learning neuromorphic chip, Hawaiian seamount Loihi", "latency_ms": 7493.530511856079, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 723}, "2": {"k": 2, "answer": "Hawaiian seamount Loihi", "latency_ms": 4015.5932903289795, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1403}, "3": {"k": 3, "answer": "Hawaiian seamount Loihi", "latency_ms": 4191.278696060181, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2072}, "4": {"k": 4, "answer": "Hawaiian seamount Loihi", "latency_ms": 3312.8881454467773, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2750}, "5": {"k": 5, "answer": "Hawaiian seamount Loihi", "latency_ms": 3624.2220401763916, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3404}, "6": {"k": 6, "answer": "Hawaiian seamount Loihi", "latency_ms": 4234.405994415283, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4077}}}
{"qid": "-6677440871585683891", "question": "who rebuilt the temple after the babylonian captivity", "gold_answers": ["Herod"], "shots": {"0": {"k": 0, "answer": "Zerubbabel", "latency_ms": 2174.504518508911, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2604.9439907073975, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 575}, "2": {"k": 2, "answer": "the Jews", "latency_ms": 1514.8394107818604, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1171}, "3": {"k": 3, "answer": "the Jews", "latency_ms": 765.6865119934082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1822}, "4": {"k": 4, "answer": "Jews who had returned", "latency_ms": 2896.5225219726562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2468}, "5": {"k": 5, "answer": "Jews who had returned from the Babylonian captivity", "latency_ms": 4949.727296829224, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3096}, "6": {"k": 6, "answer": "Jews who had returned from the Babylonian captivity", "latency_ms": 4759.247779846191, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3694}}}
{"qid": "-6750750235985613277", "question": "who was executed for being an american spy during the revolutionary war", "gold_answers": ["Nathan Hale"], "shots": {"0": {"k": 0, "answer": "Nathan Hale", "latency_ms": 2623.5594749450684, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Nathan Hale", "latency_ms": 2121.0923194885254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 617}, "2": {"k": 2, "answer": "Nathan Hale", "latency_ms": 1666.2120819091797, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1212}, "3": {"k": 3, "answer": "Nathan Hale", "latency_ms": 2156.87894821167, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1884}, "4": {"k": 4, "answer": "Nathan Hale", "latency_ms": 1808.9234828948975, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2528}, "5": {"k": 5, "answer": "Nathan Hale", "latency_ms": 1887.9094123840332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3222}, "6": {"k": 6, "answer": "Nathan Hale", "latency_ms": 2320.7666873931885, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3833}}}
{"qid": "8770209312170080158", "question": "who sang the song suddenly with olivia newton john", "gold_answers": ["Cliff Richard"], "shots": {"0": {"k": 0, "answer": "Cliff Richard", "latency_ms": 2174.5505332946777, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cliff Richard", "latency_ms": 2622.279405593872, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "Cliff Richard", "latency_ms": 2637.924909591675, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1270}, "3": {"k": 3, "answer": "Cliff Richard", "latency_ms": 1869.2643642425537, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1966}, "4": {"k": 4, "answer": "Cliff Richard", "latency_ms": 1948.9998817443848, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2542}, "5": {"k": 5, "answer": "Cliff Richard", "latency_ms": 1858.7334156036377, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3148}, "6": {"k": 6, "answer": "Cliff Richard", "latency_ms": 2121.147394180298, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3808}}}
{"qid": "-180549795106449014", "question": "who is command sergeant major of the army", "gold_answers": ["Daniel A. Dailey"], "shots": {"0": {"k": 0, "answer": "Michael A. Grinston", "latency_ms": 2639.089822769165, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Daniel A. Dailey", "latency_ms": 2832.287073135376, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 569}, "2": {"k": 2, "answer": "Daniel A. Dailey", "latency_ms": 2503.929615020752, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1137}, "3": {"k": 3, "answer": "Daniel A. Dailey", "latency_ms": 3561.1894130706787, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1843}, "4": {"k": 4, "answer": "Daniel A. Dailey", "latency_ms": 2532.179594039917, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2520}, "5": {"k": 5, "answer": "Daniel A. Dailey", "latency_ms": 2556.0708045959473, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3166}, "6": {"k": 6, "answer": "Daniel A. Dailey", "latency_ms": 2956.730842590332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3911}}}
{"qid": "7815138213728640354", "question": "who sings the original windmills of your mind", "gold_answers": ["Noel Harrison"], "shots": {"0": {"k": 0, "answer": "Noel Harrison", "latency_ms": 1836.580753326416, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Noel Harrison", "latency_ms": 1572.2274780273438, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 603}, "2": {"k": 2, "answer": "No singer mentioned", "latency_ms": 2582.880973815918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "Noel Harrison", "latency_ms": 1583.3029747009277, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1918}, "4": {"k": 4, "answer": "Noel Harrison", "latency_ms": 1676.572561264038, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2570}, "5": {"k": 5, "answer": "Noel Harrison", "latency_ms": 2105.8056354522705, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3206}, "6": {"k": 6, "answer": "Noel Harrison", "latency_ms": 2110.2912425994873, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3812}}}
{"qid": "2098168902147822379", "question": "where will the next summer and winter olympics be held", "gold_answers": ["Beijing", "Tokyo"], "shots": {"0": {"k": 0, "answer": "Paris 2024 and Milan-Cortina 2026.", "latency_ms": 5918.632745742798, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Beijing", "latency_ms": 1815.1967525482178, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 623}, "2": {"k": 2, "answer": "Beijing", "latency_ms": 1297.3606586456299, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1216}, "3": {"k": 3, "answer": "Beijing", "latency_ms": 2024.3217945098877, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1855}, "4": {"k": 4, "answer": "Beijing and Abu Dhabi", "latency_ms": 2515.155553817749, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2457}, "5": {"k": 5, "answer": "Tokyo, Beijing", "latency_ms": 2576.575756072998, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3128}, "6": {"k": 6, "answer": "Tokyo, Japan and Beijing, China", "latency_ms": 3711.2841606140137, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3804}}}
{"qid": "-1458476052377668874", "question": "when did ole miss beat alabama in football", "gold_answers": ["September 19 , 2015", "October 3 , 1970", "October 27 , 1894", "October 13 , 2001", "October 5 , 1968", "October 4 , 2014", "September 11 , 1976", "October 8 , 1988", "October 18 , 2003", "November 5 , 1910"], "shots": {"0": {"k": 0, "answer": "October 4, 2014", "latency_ms": 3139.5528316497803, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2102.3781299591064, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "1988", "latency_ms": 2305.2139282226562, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1190}, "3": {"k": 3, "answer": "1988", "latency_ms": 1726.6607284545898, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1825}, "4": {"k": 4, "answer": "1988", "latency_ms": 1816.7328834533691, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2438}, "5": {"k": 5, "answer": "1988, 2014, 2015", "latency_ms": 4369.713544845581, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3073}, "6": {"k": 6, "answer": "1988, 2014, 1968, 1910", "latency_ms": 5895.678758621216, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3701}}}
{"qid": "-6861734002351236912", "question": "who sang the song how far is heaven", "gold_answers": ["Los Lonely Boys"], "shots": {"0": {"k": 0, "answer": "Los Lonely Boys", "latency_ms": 2128.9584636688232, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Justin Moore", "latency_ms": 978.1272411346436, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "Justin Moore", "latency_ms": 2014.6102905273438, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1196}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2460.0706100463867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1785}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1391.874074935913, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2374}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2927.2773265838623, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2973}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2794.4490909576416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3590}}}
{"qid": "-1722216190521937227", "question": "where are the organ mountains in new mexico", "gold_answers": ["southern New Mexico"], "shots": {"0": {"k": 0, "answer": "Las Cruces", "latency_ms": 1926.0096549987793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "southern New Mexico", "latency_ms": 2501.5039443969727, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 606}, "2": {"k": 2, "answer": "southern New Mexico", "latency_ms": 2601.2086868286133, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1325}, "3": {"k": 3, "answer": "southern New Mexico", "latency_ms": 2370.577335357666, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1986}, "4": {"k": 4, "answer": "southern New Mexico", "latency_ms": 2839.1330242156982, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2643}, "5": {"k": 5, "answer": "southern New Mexico", "latency_ms": 2942.526340484619, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3235}, "6": {"k": 6, "answer": "southern New Mexico", "latency_ms": 2506.227493286133, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3856}}}
{"qid": "-2717119588239727064", "question": "where are the 10 plagues found in the bible", "gold_answers": ["Book of Exodus", "Exodus"], "shots": {"0": {"k": 0, "answer": "Exodus 7-12", "latency_ms": 2558.377981185913, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1830.5346965789795, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 570}, "2": {"k": 2, "answer": "the Hebrew Bible", "latency_ms": 2229.5918464660645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1238}, "3": {"k": 3, "answer": "Hebrew Bible", "latency_ms": 1807.0323467254639, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1861}, "4": {"k": 4, "answer": "Hebrew Bible", "latency_ms": 2087.9297256469727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2465}, "5": {"k": 5, "answer": "Hebrew Bible", "latency_ms": 1328.8636207580566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3091}, "6": {"k": 6, "answer": "Hebrew Bible", "latency_ms": 2307.0013523101807, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3739}}}
{"qid": "2988284357714596500", "question": "where are red blood cells made in adults", "gold_answers": ["in the bone marrow"], "shots": {"0": {"k": 0, "answer": "Bone marrow", "latency_ms": 1586.3690376281738, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "bone marrow", "latency_ms": 1011.4345550537109, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 614}, "2": {"k": 2, "answer": "bone marrow", "latency_ms": 1776.9417762756348, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1238}, "3": {"k": 3, "answer": "the larger bones", "latency_ms": 1612.5359535217285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1911}, "4": {"k": 4, "answer": "the larger bones", "latency_ms": 1407.8340530395508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2601}, "5": {"k": 5, "answer": "bone marrow", "latency_ms": 1327.2731304168701, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3245}, "6": {"k": 6, "answer": "bone marrow", "latency_ms": 1261.6584300994873, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3887}}}
{"qid": "-4616596799374362422", "question": "who was the pinkerton detective agency's first female detective", "gold_answers": ["Kate Warne"], "shots": {"0": {"k": 0, "answer": "Kate Warne", "latency_ms": 2121.532440185547, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kate Warne", "latency_ms": 1814.4817352294922, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 609}, "2": {"k": 2, "answer": "Kate Warne", "latency_ms": 2416.7263507843018, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1235}, "3": {"k": 3, "answer": "Kate Warne", "latency_ms": 2150.8095264434814, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1918}, "4": {"k": 4, "answer": "Kate Warne", "latency_ms": 1880.523681640625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2548}, "5": {"k": 5, "answer": "Kate Warne", "latency_ms": 1871.992588043213, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3173}, "6": {"k": 6, "answer": "Kate Warne", "latency_ms": 1894.103765487671, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3846}}}
{"qid": "-3650291155113659146", "question": "how many episodes are there in modern family", "gold_answers": ["232", "201"], "shots": {"0": {"k": 0, "answer": "250 episodes", "latency_ms": 1279.9792289733887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "24", "latency_ms": 1241.5876388549805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 643}, "2": {"k": 2, "answer": "At least 141", "latency_ms": 2482.2773933410645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1260}, "3": {"k": 3, "answer": "At least 141", "latency_ms": 2676.706552505493, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1898}, "4": {"k": 4, "answer": "At least 141", "latency_ms": 1984.2774868011475, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2543}, "5": {"k": 5, "answer": "At least 141", "latency_ms": 2510.199546813965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3161}, "6": {"k": 6, "answer": "At least 141", "latency_ms": 2077.162027359009, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3768}}}
{"qid": "3381924381590631417", "question": "who built the first temple for god in jerusalem", "gold_answers": ["Solomon"], "shots": {"0": {"k": 0, "answer": "King Solomon", "latency_ms": 1046.738624572754, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "King Solomon", "latency_ms": 1808.638334274292, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 643}, "2": {"k": 2, "answer": "King Solomon", "latency_ms": 751.1036396026611, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1276}, "3": {"k": 3, "answer": "King Solomon", "latency_ms": 1493.3693408966064, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1911}, "4": {"k": 4, "answer": "King Solomon", "latency_ms": 1774.7304439544678, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2471}, "5": {"k": 5, "answer": "King Solomon", "latency_ms": 968.5146808624268, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3155}, "6": {"k": 6, "answer": "King Solomon", "latency_ms": 2355.4599285125732, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3777}}}
{"qid": "-154783694579651082", "question": "what is a dropped pin on google maps for", "gold_answers": ["marks locations", "marks locations in Google Maps"], "shots": {"0": {"k": 0, "answer": "Saving a location.", "latency_ms": 963.0084037780762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "marking locations", "latency_ms": 1879.2974948883057, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "to mark a point on a map", "latency_ms": 2571.5689659118652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "to mark a point on a map", "latency_ms": 3087.7156257629395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1851}, "4": {"k": 4, "answer": "to mark a point on a map", "latency_ms": 3843.1427478790283, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2445}, "5": {"k": 5, "answer": "to mark a point on a map", "latency_ms": 2874.7456073760986, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3039}, "6": {"k": 6, "answer": "to mark a point on a map", "latency_ms": 3166.425943374634, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3715}}}
{"qid": "6915606477668963399", "question": "what do the 3 dots mean in math", "gold_answers": ["the therefore sign", "therefore sign"], "shots": {"0": {"k": 0, "answer": "Ellipsis", "latency_ms": 1382.7035427093506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2185.3113174438477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "∧", "latency_ms": 1177.1337985992432, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1308}, "3": {"k": 3, "answer": "Therefore", "latency_ms": 1198.6842155456543, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1959}, "4": {"k": 4, "answer": "Therefore sign", "latency_ms": 2008.436918258667, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2565}, "5": {"k": 5, "answer": "Therefore sign", "latency_ms": 2055.6321144104004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3281}, "6": {"k": 6, "answer": "Therefore sign", "latency_ms": 1830.631971359253, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3870}}}
{"qid": "-8366545547296627039", "question": "who wrote the song photograph by ringo starr", "gold_answers": ["Richard Starkey", "George Harrison", "Ringo Starr"], "shots": {"0": {"k": 0, "answer": "Ringo Starr and George Harrison.", "latency_ms": 1880.850076675415, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ringo Starr and George Harrison", "latency_ms": 3263.1168365478516, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 671}, "2": {"k": 2, "answer": "Ringo Starr and George Harrison", "latency_ms": 2538.8472080230713, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1329}, "3": {"k": 3, "answer": "Ringo Starr and George Harrison", "latency_ms": 3918.504238128662, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1957}, "4": {"k": 4, "answer": "Ringo Starr and George Harrison", "latency_ms": 2810.6069564819336, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2652}, "5": {"k": 5, "answer": "Ringo Starr and George Harrison", "latency_ms": 3601.983070373535, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3307}, "6": {"k": 6, "answer": "Ringo Starr and George Harrison", "latency_ms": 2687.342882156372, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3963}}}
{"qid": "-5004457603684974952", "question": "who is playing the halftime show at super bowl 2016", "gold_answers": ["Beyoncé", "Coldplay", "Bruno Mars"], "shots": {"0": {"k": 0, "answer": "Coldplay", "latency_ms": 953.4022808074951, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Coldplay, Beyoncé and Bruno Mars", "latency_ms": 3366.563081741333, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 634}, "2": {"k": 2, "answer": "Coldplay, Beyoncé, and Bruno Mars.", "latency_ms": 3843.629837036133, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "Coldplay, Beyoncé, and Bruno Mars.", "latency_ms": 4568.444490432739, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1924}, "4": {"k": 4, "answer": "Coldplay, Beyoncé, and Bruno Mars", "latency_ms": 3937.21604347229, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2580}, "5": {"k": 5, "answer": "Coldplay", "latency_ms": 1781.0592651367188, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3240}, "6": {"k": 6, "answer": "Coldplay, Beyoncé, and Bruno Mars.", "latency_ms": 4389.867544174194, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3884}}}
{"qid": "7420920477821898435", "question": "to aru kagaku no railgun s episode 3", "gold_answers": ["Project Radio Noise"], "shots": {"0": {"k": 0, "answer": "Ability and Choice.", "latency_ms": 1767.6594257354736, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2192.121982574463, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 662}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2402.2419452667236, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1389}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2938.8673305511475, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1980}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2103.4276485443115, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2650}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2598.6204147338867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3267}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2916.7425632476807, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3872}}}
{"qid": "7478795216476346339", "question": "who won the 2017 sports personality of the year", "gold_answers": ["Mo Farah"], "shots": {"0": {"k": 0, "answer": "Mo Farah", "latency_ms": 1368.4501647949219, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2134.892463684082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1910.2261066436768, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1649.6829986572266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1917}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2674.0951538085938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2542}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2682.3549270629883, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3122}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2474.6479988098145, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3761}}}
{"qid": "7217222058435937287", "question": "where was the world economic forum held this year", "gold_answers": ["Davos"], "shots": {"0": {"k": 0, "answer": "Davos, Switzerland", "latency_ms": 2056.0498237609863, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Baku", "latency_ms": 1275.3334045410156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 653}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2171.189308166504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1344}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1420.1440811157227, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1971}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2667.4370765686035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2591}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2309.61012840271, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3262}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2610.9697818756104, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3935}}}
{"qid": "-143054837169120955", "question": "where are the giant redwoods located in california", "gold_answers": ["the coast of northern California", "Del Norte County", "Humboldt County"], "shots": {"0": {"k": 0, "answer": "Northern California.", "latency_ms": 1606.6200733184814, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 5222.727060317993, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "Kings Canyon National Park and Sierra Nevada", "latency_ms": 3766.4761543273926, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1259}, "3": {"k": 3, "answer": "Kings Canyon National Park and Sierra Nevada", "latency_ms": 3826.7054557800293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1871}, "4": {"k": 4, "answer": "Sierra Nevada, Santa Cruz County, Humboldt County, Fresno County", "latency_ms": 5514.554977416992, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 2592}, "5": {"k": 5, "answer": "Kings Canyon National Park, Sierra Nevada, Santa Cruz County, Humboldt County", "latency_ms": 5897.878885269165, "em": 0.0, "f1": 0.3076923076923077, "containment": 1.0, "context_chars": 3189}, "6": {"k": 6, "answer": "Sierra Nevada, Santa Cruz County, Humboldt County", "latency_ms": 3285.1409912109375, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 3832}}}
{"qid": "4892429640540595424", "question": "who has made the most premier league appearances", "gold_answers": ["Gareth Barry"], "shots": {"0": {"k": 0, "answer": "Gareth Barry", "latency_ms": 2711.3912105560303, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Gareth Barry", "latency_ms": 2366.333246231079, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 641}, "2": {"k": 2, "answer": "Gareth Barry", "latency_ms": 1593.09983253479, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1261}, "3": {"k": 3, "answer": "Gareth Barry", "latency_ms": 2130.1958560943604, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1881}, "4": {"k": 4, "answer": "Gareth Barry", "latency_ms": 1875.4417896270752, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2542}, "5": {"k": 5, "answer": "Gareth Barry", "latency_ms": 2104.388475418091, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3203}, "6": {"k": 6, "answer": "Gareth Barry", "latency_ms": 2328.1891345977783, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3858}}}
{"qid": "5533906981191706877", "question": "who was the first chief minister of west bengal", "gold_answers": ["Prafulla Chandra Ghosh"], "shots": {"0": {"k": 0, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3991.9042587280273, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3803.6820888519287, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 693}, "2": {"k": 2, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3485.1341247558594, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1408}, "3": {"k": 3, "answer": "Prafulla Chandra Ghosh", "latency_ms": 4073.6770629882812, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2054}, "4": {"k": 4, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3942.8060054779053, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2656}, "5": {"k": 5, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3872.8082180023193, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3283}, "6": {"k": 6, "answer": "Prafulla Chandra Ghosh", "latency_ms": 4665.794134140015, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4012}}}
{"qid": "8823076125131408628", "question": "who sings got my mind set on you", "gold_answers": ["George Harrison", "James Ray"], "shots": {"0": {"k": 0, "answer": "George Harrison", "latency_ms": 1835.7477188110352, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3036.2188816070557, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 610}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2182.6651096343994, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1192}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2327.125310897827, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1784}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2912.097930908203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2318}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2754.579544067383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2971}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2912.8572940826416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3545}}}
{"qid": "9168660859139861094", "question": "two main types of research methods used in psychology", "gold_answers": ["Correlational research", "Descriptive research", "quantitative data", "Quantitative psychological research", "qualitative", "quantitative", "Qualitative psychological research", "Experimental research", "qualitative data"], "shots": {"0": {"k": 0, "answer": "Qualitative and Quantitative.", "latency_ms": 3450.1307010650635, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Qualitative and quantitative", "latency_ms": 1771.0742950439453, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 807}, "2": {"k": 2, "answer": "Qualitative or quantitative", "latency_ms": 2328.1099796295166, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1554}, "3": {"k": 3, "answer": "Qualitative or quantitative", "latency_ms": 2238.409996032715, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2378}, "4": {"k": 4, "answer": "Qualitative or quantitative", "latency_ms": 1978.0645370483398, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3120}, "5": {"k": 5, "answer": "Qualitative or quantitative", "latency_ms": 2182.501792907715, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3923}, "6": {"k": 6, "answer": "Qualitative or quantitative", "latency_ms": 3431.377410888672, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 4660}}}
{"qid": "-7073063191952855764", "question": "who plays peter in what we do in the shadows", "gold_answers": ["Ben Fransham"], "shots": {"0": {"k": 0, "answer": "Kayvan Novak", "latency_ms": 1470.0613021850586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2457.911491394043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 654}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1844.7530269622803, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1290}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2762.2718811035156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1932}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2048.1464862823486, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2572}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2479.2187213897705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3192}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2124.803304672241, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3795}}}
{"qid": "7901746249864619718", "question": "when does the 14th season of grey's anatomy come out", "gold_answers": ["September 28 , 2017"], "shots": {"0": {"k": 0, "answer": "September 28, 2017", "latency_ms": 2400.424003601074, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 28, 2017", "latency_ms": 3126.6961097717285, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "September 28, 2017", "latency_ms": 3373.481273651123, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "September 28, 2017", "latency_ms": 2835.1588249206543, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "September 28, 2017", "latency_ms": 3144.2878246307373, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2701}, "5": {"k": 5, "answer": "September 28, 2017", "latency_ms": 3393.421411514282, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3389}, "6": {"k": 6, "answer": "September 28, 2017", "latency_ms": 3107.945442199707, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4057}}}
{"qid": "-896127746191949637", "question": "when was the first hunger games book published", "gold_answers": ["September 14 , 2008", "2008"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 1797.50394821167, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 14, 2008", "latency_ms": 3680.0568103790283, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 670}, "2": {"k": 2, "answer": "September 14, 2008", "latency_ms": 3489.1719818115234, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1322}, "3": {"k": 3, "answer": "September 14, 2008", "latency_ms": 3272.9411125183105, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1996}, "4": {"k": 4, "answer": "September 14, 2008", "latency_ms": 3871.602773666382, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2679}, "5": {"k": 5, "answer": "September 14, 2008", "latency_ms": 3136.43741607666, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3304}, "6": {"k": 6, "answer": "September 14, 2008", "latency_ms": 3541.8448448181152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3917}}}
{"qid": "8044909700499569711", "question": "how much is the united states in debt to china", "gold_answers": ["$1.06 trillion", "1,200.5"], "shots": {"0": {"k": 0, "answer": "$1.08 trillion", "latency_ms": 1917.5255298614502, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "21 percent", "latency_ms": 2019.2017555236816, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "$1.18 trillion", "latency_ms": 2723.4086990356445, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "$1.18 trillion", "latency_ms": 2989.5310401916504, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1931}, "4": {"k": 4, "answer": "$1.18 trillion", "latency_ms": 2933.6211681365967, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2579}, "5": {"k": 5, "answer": "$1.18 trillion", "latency_ms": 1959.981918334961, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3199}, "6": {"k": 6, "answer": "$1.18 trillion", "latency_ms": 1944.6802139282227, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3843}}}
{"qid": "-3123234845590594961", "question": "where did the race in grease take place", "gold_answers": ["Thunder Road"], "shots": {"0": {"k": 0, "answer": "The Thunder Road Race", "latency_ms": 1914.4492149353027, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Los Angeles River", "latency_ms": 2562.889814376831, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "Los Angeles River", "latency_ms": 1402.7867317199707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1219}, "3": {"k": 3, "answer": "Los Angeles River", "latency_ms": 2595.3450202941895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1905}, "4": {"k": 4, "answer": "Los Angeles River", "latency_ms": 1873.5709190368652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2543}, "5": {"k": 5, "answer": "Los Angeles River", "latency_ms": 1343.2581424713135, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3141}, "6": {"k": 6, "answer": "Los Angeles River", "latency_ms": 2313.002586364746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3754}}}
{"qid": "4326992806019599297", "question": "what is the maximum data rate for the 802.11a standard select one", "gold_answers": ["54 Mbit / s"], "shots": {"0": {"k": 0, "answer": "54 Mbps", "latency_ms": 984.4908714294434, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "54 Mbit/s", "latency_ms": 2641.9661045074463, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 628}, "2": {"k": 2, "answer": "54 Mbit/s", "latency_ms": 1861.6411685943604, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1321}, "3": {"k": 3, "answer": "54 Mbit/s", "latency_ms": 1979.4566631317139, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1997}, "4": {"k": 4, "answer": "54 Mbit/s", "latency_ms": 2417.074203491211, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2619}, "5": {"k": 5, "answer": "54 Mbit/s", "latency_ms": 2667.851209640503, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3265}, "6": {"k": 6, "answer": "54 Mbit/s", "latency_ms": 3188.4617805480957, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3927}}}
{"qid": "6144099837933283715", "question": "a good that can be used in place of another good", "gold_answers": ["A substitute good", "substitute good"], "shots": {"0": {"k": 0, "answer": "Substitute", "latency_ms": 1242.7492141723633, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Pickling salt", "latency_ms": 1842.503547668457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 537}, "2": {"k": 2, "answer": "Pickling salt", "latency_ms": 2514.629125595093, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1157}, "3": {"k": 3, "answer": "Pickling salt", "latency_ms": 2290.727376937866, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1814}, "4": {"k": 4, "answer": "Pickling salt", "latency_ms": 1856.5263748168945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2368}, "5": {"k": 5, "answer": "Pickling salt", "latency_ms": 2270.456075668335, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3048}, "6": {"k": 6, "answer": "Pickling salt", "latency_ms": 2054.572105407715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3677}}}
{"qid": "-4621948696356225060", "question": "where in the bible can i find the lord's prayer", "gold_answers": ["the Gospel of Luke", "the Gospel of Matthew"], "shots": {"0": {"k": 0, "answer": "Matthew 6:9-13, Luke 11:2-4", "latency_ms": 5164.689540863037, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1749.7549057006836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 562}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2128.183126449585, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1223}, "3": {"k": 3, "answer": "Matthew 6:9", "latency_ms": 3526.944398880005, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1769}, "4": {"k": 4, "answer": "Matthew 6:9-13", "latency_ms": 3923.342227935791, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2350}, "5": {"k": 5, "answer": "Matthew 6:9-13", "latency_ms": 4163.565397262573, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2985}, "6": {"k": 6, "answer": "Matthew 6:9-13", "latency_ms": 2711.314916610718, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3623}}}
{"qid": "6941993254369956277", "question": "when was the last time nebraska voted democrat", "gold_answers": ["1964"], "shots": {"0": {"k": 0, "answer": "1964", "latency_ms": 725.8243560791016, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1908", "latency_ms": 2594.395637512207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 695}, "2": {"k": 2, "answer": "1908", "latency_ms": 1544.5172786712646, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1397}, "3": {"k": 3, "answer": "1994", "latency_ms": 1752.3963451385498, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2129}, "4": {"k": 4, "answer": "1994", "latency_ms": 1756.2603950500488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2814}, "5": {"k": 5, "answer": "1908", "latency_ms": 2287.959337234497, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3495}, "6": {"k": 6, "answer": "1994", "latency_ms": 1030.9333801269531, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4146}}}
{"qid": "-8955197065025093046", "question": "who become the ceo of it wipro company in 2016", "gold_answers": ["Abid Ali Neemuchwala"], "shots": {"0": {"k": 0, "answer": "Abidali Neemuchwala", "latency_ms": 3454.512119293213, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Abidali Neemuchwala", "latency_ms": 3994.9870109558105, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 678}, "2": {"k": 2, "answer": "Abidali Neemuchwala", "latency_ms": 4164.62254524231, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1363}, "3": {"k": 3, "answer": "Abidali Neemuchwala", "latency_ms": 3492.892265319824, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2047}, "4": {"k": 4, "answer": "Abidali Neemuchwala", "latency_ms": 3940.1912689208984, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2675}, "5": {"k": 5, "answer": "Abidali Neemuchwala", "latency_ms": 3494.4303035736084, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3341}, "6": {"k": 6, "answer": "Abidali Neemuchwala", "latency_ms": 3865.518093109131, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3981}}}
{"qid": "-3010425579521817304", "question": "who does eric end up with in gossip girl", "gold_answers": ["Jenny"], "shots": {"0": {"k": 0, "answer": "Vanessa, then ultimately Serena", "latency_ms": 2745.704174041748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2133.654832839966, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1617.321252822876, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1245}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1786.6599559783936, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1871}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2627.8929710388184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2483}, "5": {"k": 5, "answer": "Jenny", "latency_ms": 1973.6545085906982, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3068}, "6": {"k": 6, "answer": "Jenny", "latency_ms": 1537.6944541931152, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3658}}}
{"qid": "7264112453286983469", "question": "who plays unis in she's the man", "gold_answers": ["Emily Perkins"], "shots": {"0": {"k": 0, "answer": "Amanda Bynes", "latency_ms": 1664.8454666137695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1425.6207942962646, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 694}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1860.7349395751953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1282}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2493.7593936920166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1890}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2185.680150985718, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2531}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1621.1392879486084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3167}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2663.8190746307373, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3767}}}
{"qid": "2627695648792729859", "question": "who played booster in jingle all the way", "gold_answers": ["Curtis Armstrong"], "shots": {"0": {"k": 0, "answer": "Eric Lloyd", "latency_ms": 1275.9809494018555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2189.7497177124023, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 679}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1912.6858711242676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1342}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1935.8067512512207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2010}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2407.834529876709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2699}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2427.050828933716, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3332}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1905.383825302124, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3986}}}
{"qid": "-9169976182548289414", "question": "who is recognized as the founder of islam", "gold_answers": ["the Islamic prophet Muhammad", "Muhammad"], "shots": {"0": {"k": 0, "answer": "Muhammad", "latency_ms": 1364.1393184661865, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Urwah ibn Zubayr", "latency_ms": 3180.7022094726562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 624}, "2": {"k": 2, "answer": "Muhammad", "latency_ms": 1288.8493537902832, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1262}, "3": {"k": 3, "answer": "Muhammad", "latency_ms": 1243.814468383789, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1925}, "4": {"k": 4, "answer": "Muhammad", "latency_ms": 2241.881847381592, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2567}, "5": {"k": 5, "answer": "Muhammad", "latency_ms": 1246.5882301330566, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3193}, "6": {"k": 6, "answer": "Muhammad", "latency_ms": 1553.6470413208008, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3813}}}
{"qid": "-6696747560626271522", "question": "who plays emma in air bud world pup", "gold_answers": ["Brittany Paige Bouck"], "shots": {"0": {"k": 0, "answer": "Brittany Paige Bouck", "latency_ms": 1935.0266456604004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Brittany Paige Bouck", "latency_ms": 3216.63498878479, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1923.5508441925049, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2492.9492473602295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1871}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2106.91499710083, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2452}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1722.3191261291504, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3169}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2927.649736404419, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3794}}}
{"qid": "5871681964137793716", "question": "how old was sasuke when his clan died", "gold_answers": ["seven"], "shots": {"0": {"k": 0, "answer": "7 years old", "latency_ms": 1844.954252243042, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Seven", "latency_ms": 919.6851253509521, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 641}, "2": {"k": 2, "answer": "Seven", "latency_ms": 1983.8101863861084, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1247}, "3": {"k": 3, "answer": "Seven", "latency_ms": 478.6639213562012, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1875}, "4": {"k": 4, "answer": "Seven", "latency_ms": 1745.4097270965576, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2543}, "5": {"k": 5, "answer": "Seven", "latency_ms": 1181.1671257019043, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3145}, "6": {"k": 6, "answer": "Seven", "latency_ms": 1506.4237117767334, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3760}}}
{"qid": "8552176802923865025", "question": "where do some of the natural air pollutants come from", "gold_answers": ["wildfires", "Dust", "Vegetation", "Carbon dioxide", "Volcanic activity"], "shots": {"0": {"k": 0, "answer": "Volcanic eruptions.", "latency_ms": 1489.7809028625488, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2740.4274940490723, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 718}, "2": {"k": 2, "answer": "Volcanic eruption", "latency_ms": 1897.6082801818848, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1377}, "3": {"k": 3, "answer": "Volcanic eruption", "latency_ms": 862.7994060516357, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2025}, "4": {"k": 4, "answer": "forest fires, volcanic eruptions", "latency_ms": 3086.611747741699, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2776}, "5": {"k": 5, "answer": "forest fires, volcanic eruptions, wind erosion, pollen dispersal, evaporation of organic compounds, and natural radioactivity.", "latency_ms": 8149.201393127441, "em": 0.0, "f1": 0.1176470588235294, "containment": 0.0, "context_chars": 3506}, "6": {"k": 6, "answer": "forest fires, volcanic eruptions, wind erosion, pollen dispersal, evaporation of organic compounds, and natural radioactivity.", "latency_ms": 9497.369527816772, "em": 0.0, "f1": 0.1176470588235294, "containment": 0.0, "context_chars": 4123}}}
{"qid": "-2630525064809360052", "question": "who is the pastor of new birth missionary baptist church now", "gold_answers": ["Stephen A. Davis"], "shots": {"0": {"k": 0, "answer": "Jamal Harrison Bryant", "latency_ms": 1406.526803970337, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Jamal Bryant", "latency_ms": 1897.9086875915527, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 676}, "2": {"k": 2, "answer": "Jamal Bryant", "latency_ms": 1647.1009254455566, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1353}, "3": {"k": 3, "answer": "Jamal Bryant", "latency_ms": 2428.4472465515137, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1990}, "4": {"k": 4, "answer": "Jamal Bryant", "latency_ms": 1830.418586730957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2621}, "5": {"k": 5, "answer": "Jamal Bryant", "latency_ms": 1410.4785919189453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3286}, "6": {"k": 6, "answer": "Jamal Bryant", "latency_ms": 1594.2838191986084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3959}}}
{"qid": "6434931911289860123", "question": "who is doing 2018 super bowl half time show", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Justin Timberlake", "latency_ms": 1107.9764366149902, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Justin Timberlake", "latency_ms": 2286.2067222595215, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 658}, "2": {"k": 2, "answer": "Justin Timberlake", "latency_ms": 2070.706844329834, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1290}, "3": {"k": 3, "answer": "Justin Timberlake", "latency_ms": 2048.6373901367188, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1899}, "4": {"k": 4, "answer": "Justin Timberlake", "latency_ms": 2317.385196685791, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2587}, "5": {"k": 5, "answer": "Justin Timberlake", "latency_ms": 2113.1277084350586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3249}, "6": {"k": 6, "answer": "Justin Timberlake", "latency_ms": 1628.8836002349854, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3874}}}
{"qid": "8869318258439148973", "question": "who is playing the halftime show for the superbowl", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Rihanna", "latency_ms": 1771.9433307647705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Coldplay, Beyoncé and Bruno Mars", "latency_ms": 3649.310827255249, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 634}, "2": {"k": 2, "answer": "Coldplay, Beyoncé, and Bruno Mars.", "latency_ms": 3563.5650157928467, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1267}, "3": {"k": 3, "answer": "Coldplay, Beyoncé, and Bruno Mars.", "latency_ms": 3299.621343612671, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1892}, "4": {"k": 4, "answer": "Coldplay, Beyoncé, Bruno Mars", "latency_ms": 2896.5976238250732, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2526}, "5": {"k": 5, "answer": "Maroon 5", "latency_ms": 2217.472553253174, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3158}, "6": {"k": 6, "answer": "Maroon 5", "latency_ms": 2183.6419105529785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3819}}}
{"qid": "-8359866474144720253", "question": "who hosted they think it's all over", "gold_answers": ["Des Lynam", "Lee Mack", "comedian Nick Hancock", "Nick Hancock"], "shots": {"0": {"k": 0, "answer": "Nick Hancock", "latency_ms": 1072.1349716186523, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2084.181547164917, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 604}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1855.6840419769287, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1211}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2387.216567993164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1867}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2579.79416847229, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2473}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2120.31626701355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3117}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2111.956834793091, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3703}}}
{"qid": "-1206653570097564556", "question": "where does the movie proof of life take place", "gold_answers": ["The Republic of Tecala"], "shots": {"0": {"k": 0, "answer": "South America", "latency_ms": 1329.805612564087, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1885.2007389068604, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 596}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1685.0395202636719, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1238}, "3": {"k": 3, "answer": "London and Leipzig", "latency_ms": 1812.072992324829, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1858}, "4": {"k": 4, "answer": "The Republic of Tecala", "latency_ms": 2474.475145339966, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2486}, "5": {"k": 5, "answer": "The Republic of Tecala", "latency_ms": 3574.434518814087, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3083}, "6": {"k": 6, "answer": "The Republic of Tecala", "latency_ms": 2748.4939098358154, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3728}}}
{"qid": "6144521615402262404", "question": "where does us highway 1 start and end", "gold_answers": ["Fort Kent , Maine", "to Key West , Florida", "Key West , Florida"], "shots": {"0": {"k": 0, "answer": "Fort Kent, Maine to Key West, Florida", "latency_ms": 4140.976428985596, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1640.066385269165, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 624}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2657.737970352173, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1217}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2424.011707305908, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1811}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2680.915117263794, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2442}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2226.983070373535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3103}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1880.014181137085, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3823}}}
{"qid": "-8631190913794477985", "question": "who scored the most points in a single game in the nba", "gold_answers": ["Wilt Chamberlain"], "shots": {"0": {"k": 0, "answer": "Wilt Chamberlain, 100 points", "latency_ms": 3120.4617023468018, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wilt Chamberlain", "latency_ms": 3434.7286224365234, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 664}, "2": {"k": 2, "answer": "Wilt Chamberlain", "latency_ms": 2168.910503387451, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1310}, "3": {"k": 3, "answer": "Wilt Chamberlain", "latency_ms": 3004.934310913086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1892}, "4": {"k": 4, "answer": "Wilt Chamberlain", "latency_ms": 1592.439889907837, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2531}, "5": {"k": 5, "answer": "Wilt Chamberlain", "latency_ms": 1587.571144104004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3083}, "6": {"k": 6, "answer": "Wilt Chamberlain", "latency_ms": 2450.202226638794, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3637}}}
{"qid": "199896377614951764", "question": "where is natural gas found in the us", "gold_answers": ["Texas", "Pennsylvania", "Colorado", "New Mexico", "West Virginia", "Arkansas", "Wyoming", "Louisiana"], "shots": {"0": {"k": 0, "answer": "Texas, Oklahoma, Louisiana, Alaska, and Pennsylvania.", "latency_ms": 3181.711196899414, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Louisiana, Texas, California, and Alaska.", "latency_ms": 4372.080326080322, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 630}, "2": {"k": 2, "answer": "Louisiana, Texas, California, and Alaska", "latency_ms": 4671.109676361084, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1225}, "3": {"k": 3, "answer": "Louisiana, Texas, California, Alaska, shale formations", "latency_ms": 5433.426856994629, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 1825}, "4": {"k": 4, "answer": "Louisiana, Texas, California, Alaska", "latency_ms": 3448.4431743621826, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2501}, "5": {"k": 5, "answer": "Louisiana, Texas, California, Alaska", "latency_ms": 3776.6456604003906, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3174}, "6": {"k": 6, "answer": "Louisiana, Texas, California, Alaska", "latency_ms": 3921.5104579925537, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3889}}}
{"qid": "8986775680996674901", "question": "when were the first general elections held in independent india", "gold_answers": ["1951 -- 52"], "shots": {"0": {"k": 0, "answer": "1951-52", "latency_ms": 2306.537389755249, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1952", "latency_ms": 793.0991649627686, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "1952", "latency_ms": 1601.1497974395752, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1312}, "3": {"k": 3, "answer": "1952", "latency_ms": 1475.032091140747, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1977}, "4": {"k": 4, "answer": "25 October 1951 and 21 February 1952", "latency_ms": 4717.673778533936, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 2625}, "5": {"k": 5, "answer": "January 1952", "latency_ms": 2128.437280654907, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3250}, "6": {"k": 6, "answer": "January 1952", "latency_ms": 2356.771230697632, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3884}}}
{"qid": "700604097171850168", "question": "what was the tower of london originally used for", "gold_answers": ["as a royal residence", "a royal residence"], "shots": {"0": {"k": 0, "answer": "Royal Palace and Fortress", "latency_ms": 1401.0565280914307, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "holding prisoners was incidental", "latency_ms": 1959.1515064239502, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 654}, "2": {"k": 2, "answer": "incidental role of holding prisoners", "latency_ms": 2672.151565551758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1238}, "3": {"k": 3, "answer": "incidental role of holding prisoners as would have been the case for any castle", "latency_ms": 6129.7407150268555, "em": 0.0, "f1": 0.125, "containment": 0.0, "context_chars": 1909}, "4": {"k": 4, "answer": "incidental role of holding prisoners", "latency_ms": 2081.2246799468994, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2527}, "5": {"k": 5, "answer": "incidental role of holding prisoners", "latency_ms": 3452.6262283325195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3097}, "6": {"k": 6, "answer": "incidental role of holding prisoners", "latency_ms": 2748.0454444885254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3720}}}
{"qid": "6745303307988470742", "question": "what type of fuel goes in a zippo", "gold_answers": ["butane", "lighter fluid"], "shots": {"0": {"k": 0, "answer": "Lighter fluid", "latency_ms": 2144.6380615234375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 5199.736595153809, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 605}, "2": {"k": 2, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 5502.584218978882, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1202}, "3": {"k": 3, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 5746.846675872803, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1830}, "4": {"k": 4, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 5999.9449253082275, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2400}, "5": {"k": 5, "answer": "Light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 5181.522846221924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3039}, "6": {"k": 6, "answer": "Light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 5150.015830993652, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3677}}}
{"qid": "-2060506905455252030", "question": "first jnanpith award was an autor of which language", "gold_answers": ["Malayalam"], "shots": {"0": {"k": 0, "answer": "Gujarati", "latency_ms": 1926.809310913086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Malayalam", "latency_ms": 2769.287347793579, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 669}, "2": {"k": 2, "answer": "Malayalam", "latency_ms": 2112.2336387634277, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1332}, "3": {"k": 3, "answer": "Malayalam", "latency_ms": 1584.4995975494385, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1980}, "4": {"k": 4, "answer": "Malayalam", "latency_ms": 1105.9434413909912, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2625}, "5": {"k": 5, "answer": "Malayalam", "latency_ms": 2099.7204780578613, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3265}, "6": {"k": 6, "answer": "Malayalam", "latency_ms": 1853.8875579833984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3990}}}
{"qid": "4738663028303377024", "question": "who played the mad hatter in the batman tv show", "gold_answers": ["Roddy McDowall", "David Wayne", "Benedict Samuel"], "shots": {"0": {"k": 0, "answer": "David Wayne", "latency_ms": 1478.8143634796143, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2162.175416946411, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 641}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2188.826560974121, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1250}, "3": {"k": 3, "answer": "David Wayne", "latency_ms": 1252.88724899292, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1905}, "4": {"k": 4, "answer": "Benedict Samuel", "latency_ms": 1670.2337265014648, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2543}, "5": {"k": 5, "answer": "David Wayne", "latency_ms": 1763.9961242675781, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3213}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1691.8578147888184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3868}}}
{"qid": "-1650946015201779846", "question": "how many cracker barrels in the united states", "gold_answers": ["639", "more than 600"], "shots": {"0": {"k": 0, "answer": "664", "latency_ms": 480.4081916809082, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "645 stores in 44 states", "latency_ms": 2355.1948070526123, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "645 stores in 44 states", "latency_ms": 2233.25777053833, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1265}, "3": {"k": 3, "answer": "645 stores in 44 states", "latency_ms": 3246.0031509399414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1909}, "4": {"k": 4, "answer": "645 stores in 44 states", "latency_ms": 2936.846971511841, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2593}, "5": {"k": 5, "answer": "645 stores in 44 states", "latency_ms": 1606.764793395996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3218}, "6": {"k": 6, "answer": "645 stores in 44 states", "latency_ms": 1986.2604141235352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3864}}}
{"qid": "-3422350952909582054", "question": "in photosynthesis the carbon in co2 is initially fixed to what molecule", "gold_answers": ["3 - phosphoglycerate"], "shots": {"0": {"k": 0, "answer": "RuBP (Ribulose-1,5-Bisphosphate)", "latency_ms": 5254.395961761475, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 3866.917371749878, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 712}, "2": {"k": 2, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 3444.9822902679443, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1411}, "3": {"k": 3, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 4312.298536300659, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2116}, "4": {"k": 4, "answer": "phosphoenolpyruvate (PEP)", "latency_ms": 3981.6389083862305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2845}, "5": {"k": 5, "answer": "Phosphoenolpyruvate (PEP)", "latency_ms": 3669.152021408081, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3553}, "6": {"k": 6, "answer": "Ribulose 1,5-bisphosphate", "latency_ms": 4842.681407928467, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4269}}}
{"qid": "-2704708103393811482", "question": "who voices randy in f is for family", "gold_answers": ["T.J. Miller"], "shots": {"0": {"k": 0, "answer": "Bill Burr", "latency_ms": 1213.8521671295166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2778.0680656433105, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2188.068628311157, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1269}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2356.3904762268066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1925}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2172.3668575286865, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2554}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2210.407495498657, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3119}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1933.4440231323242, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3688}}}
{"qid": "5482012214308896475", "question": "how many countries does cadbury sell its products", "gold_answers": ["more than 50", "more than 50 countries worldwide"], "shots": {"0": {"k": 0, "answer": "160 countries", "latency_ms": 878.9980411529541, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "30", "latency_ms": 1151.0004997253418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 652}, "2": {"k": 2, "answer": "more than fifty", "latency_ms": 1908.4246158599854, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1356}, "3": {"k": 3, "answer": "50 countries worldwide", "latency_ms": 1601.5620231628418, "em": 0.0, "f1": 0.7499999999999999, "containment": 0.0, "context_chars": 2064}, "4": {"k": 4, "answer": "50 countries worldwide", "latency_ms": 2340.674877166748, "em": 0.0, "f1": 0.7499999999999999, "containment": 0.0, "context_chars": 2698}, "5": {"k": 5, "answer": "50 countries worldwide", "latency_ms": 1875.8084774017334, "em": 0.0, "f1": 0.7499999999999999, "containment": 0.0, "context_chars": 3397}, "6": {"k": 6, "answer": "50", "latency_ms": 1243.2637214660645, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 4125}}}
{"qid": "-5737509619250561344", "question": "ethiopia flight 961 crashes in to the sea", "gold_answers": ["23 November 1996"], "shots": {"0": {"k": 0, "answer": "November 23, 1996", "latency_ms": 2827.4405002593994, "em": 0.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indian Ocean", "latency_ms": 1615.19193649292, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 606}, "2": {"k": 2, "answer": "Indian Ocean", "latency_ms": 1270.679235458374, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1304}, "3": {"k": 3, "answer": "Indian Ocean", "latency_ms": 2102.3313999176025, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1967}, "4": {"k": 4, "answer": "Indian Ocean", "latency_ms": 1262.2191905975342, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2624}, "5": {"k": 5, "answer": "Indian Ocean", "latency_ms": 1823.4648704528809, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3256}, "6": {"k": 6, "answer": "Indian Ocean", "latency_ms": 1808.1910610198975, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3857}}}
{"qid": "-7175456993713717070", "question": "what does istj mean in a personality test", "gold_answers": ["Extroverted Thinking ( Te )", "Extroverted Intuition ( Ne )", "Introverted Feeling ( Fi )", "Introverted Sensing ( Si )"], "shots": {"0": {"k": 0, "answer": "Introverted, Sensing, Thinking, Judging.", "latency_ms": 3791.918992996216, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 3731.5282821655273, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 724}, "2": {"k": 2, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 4491.004467010498, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1385}, "3": {"k": 3, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 5454.485654830933, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2101}, "4": {"k": 4, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 3806.405782699585, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2729}, "5": {"k": 5, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 3938.3015632629395, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3404}, "6": {"k": 6, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 5483.246326446533, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 4115}}}
{"qid": "4437170343606050664", "question": "who does the voice of ash in sing", "gold_answers": ["Scarlett Johansson"], "shots": {"0": {"k": 0, "answer": "Scarlett Johansson", "latency_ms": 1680.0785064697266, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ash King", "latency_ms": 2576.507568359375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 647}, "2": {"k": 2, "answer": "Ash King", "latency_ms": 819.6947574615479, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1275}, "3": {"k": 3, "answer": "Ash King", "latency_ms": 1112.3180389404297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1899}, "4": {"k": 4, "answer": "Ash King", "latency_ms": 1479.0830612182617, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2525}, "5": {"k": 5, "answer": "Ash King", "latency_ms": 1728.9321422576904, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3049}, "6": {"k": 6, "answer": "Ash King", "latency_ms": 1804.2895793914795, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3662}}}
{"qid": "-8773010039077552394", "question": "who scored the most points in their nba career", "gold_answers": ["Kareem Abdul - Jabbar"], "shots": {"0": {"k": 0, "answer": "LeBron James", "latency_ms": 2514.1546726226807, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Nowitzki", "latency_ms": 1773.7176418304443, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 589}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2203.012228012085, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1152}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2084.3660831451416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1709}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2404.006004333496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2263}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2616.6319847106934, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2868}, "6": {"k": 6, "answer": "Kareem Abdul-Jabbar", "latency_ms": 3369.072437286377, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3486}}}
{"qid": "-3278224144622911032", "question": "with a land area of 54 314 square miles where does wisconsin rank among the 50 states", "gold_answers": ["25"], "shots": {"0": {"k": 0, "answer": "23rd", "latency_ms": 946.7673301696777, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1495.74875831604, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 597}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2440.5410289764404, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1178}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2508.9049339294434, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1793}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2771.859884262085, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2405}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2465.423345565796, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2999}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1777.1084308624268, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3592}}}
{"qid": "-3466298422174960041", "question": "who plays charles on have and have nots", "gold_answers": ["Nick Sager", "Nick Sagar"], "shots": {"0": {"k": 0, "answer": "Renée Lawless", "latency_ms": 1735.1937294006348, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1644.7839736938477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2181.0975074768066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1274}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2197.0903873443604, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1923}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2349.2400646209717, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2544}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1901.1247158050537, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3167}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2973.015069961548, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3804}}}
{"qid": "1803248910342766083", "question": "who wrote the book my country my life", "gold_answers": ["L.K. Advani"], "shots": {"0": {"k": 0, "answer": "L.K. Advani", "latency_ms": 2519.624710083008, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "L. K. Advani", "latency_ms": 3044.3804264068604, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "L. K. Advani", "latency_ms": 3586.008071899414, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1158}, "3": {"k": 3, "answer": "L. K. Advani", "latency_ms": 2668.9376831054688, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1798}, "4": {"k": 4, "answer": "L. K. Advani", "latency_ms": 3583.4336280822754, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2463}, "5": {"k": 5, "answer": "L. K. Advani", "latency_ms": 3322.4401473999023, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3173}, "6": {"k": 6, "answer": "L. K. Advani", "latency_ms": 3112.746000289917, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3849}}}
{"qid": "-1176280233636154422", "question": "when did the east india company take control of india", "gold_answers": ["1757", "1799", "1612", "in 1757"], "shots": {"0": {"k": 0, "answer": "1757", "latency_ms": 1862.328290939331, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1757", "latency_ms": 2322.3865032196045, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 647}, "2": {"k": 2, "answer": "1757", "latency_ms": 1264.4317150115967, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1307}, "3": {"k": 3, "answer": "1757", "latency_ms": 1507.8518390655518, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1925}, "4": {"k": 4, "answer": "1757", "latency_ms": 1541.7263507843018, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2557}, "5": {"k": 5, "answer": "1757", "latency_ms": 2054.4610023498535, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3205}, "6": {"k": 6, "answer": "1757", "latency_ms": 1513.2641792297363, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3853}}}
{"qid": "5911119091653953181", "question": "where does the water come from to fill lake eyre", "gold_answers": ["Warburton River"], "shots": {"0": {"k": 0, "answer": "Cooper Creek and Diamantina River", "latency_ms": 4559.988498687744, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "the west, the Channel Country of Queensland", "latency_ms": 4112.411975860596, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 566}, "2": {"k": 2, "answer": "the west, the Channel Country of Queensland", "latency_ms": 3915.829658508301, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1154}, "3": {"k": 3, "answer": "the west, the Channel Country of Queensland", "latency_ms": 4086.0774517059326, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1775}, "4": {"k": 4, "answer": "the west, the Channel Country of Queensland", "latency_ms": 4144.372224807739, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2401}, "5": {"k": 5, "answer": "Channel Country of Queensland", "latency_ms": 1763.7114524841309, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2998}, "6": {"k": 6, "answer": "Channel Country of Queensland", "latency_ms": 2625.448703765869, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3609}}}
{"qid": "5557104327881436095", "question": "who is the founder of google and when was it founded", "gold_answers": ["1998", "Sergey Brin", "Larry Page and Sergey Brin", "Larry Page"], "shots": {"0": {"k": 0, "answer": "Larry Page and Sergey Brin, September 4, 1998", "latency_ms": 6209.3470096588135, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2661.759853363037, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 676}, "2": {"k": 2, "answer": "Larry Page and Sergey Brin, September 4, 1998", "latency_ms": 6420.581102371216, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 1358}, "3": {"k": 3, "answer": "Larry Page and Sergey Brin, September 4, 1998", "latency_ms": 6083.757400512695, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 2002}, "4": {"k": 4, "answer": "Larry Page and Sergey Brin, September 4, 1998", "latency_ms": 5120.561838150024, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 2698}, "5": {"k": 5, "answer": "Larry Page and Sergey Brin, September 4, 1998", "latency_ms": 5280.759572982788, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 3334}, "6": {"k": 6, "answer": "Larry Page and Sergey Brin, September 4, 1998", "latency_ms": 5896.686792373657, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 4003}}}
{"qid": "-1333328812971005142", "question": "who did the dominican republic gain its independence from", "gold_answers": ["Haiti"], "shots": {"0": {"k": 0, "answer": "Haiti", "latency_ms": 2362.7471923828125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Haiti", "latency_ms": 1131.6719055175781, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 725}, "2": {"k": 2, "answer": "Haiti", "latency_ms": 2091.6736125946045, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1394}, "3": {"k": 3, "answer": "Haiti", "latency_ms": 2129.45294380188, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2069}, "4": {"k": 4, "answer": "Haiti and Spain", "latency_ms": 2562.0882511138916, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2783}, "5": {"k": 5, "answer": "Haiti", "latency_ms": 2145.9593772888184, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3420}, "6": {"k": 6, "answer": "Haiti", "latency_ms": 2604.2988300323486, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4106}}}
{"qid": "1122605495295045742", "question": "how did early humans make use of stones during the prehistoric period", "gold_answers": ["start fires", "hunt", "bury their dead"], "shots": {"0": {"k": 0, "answer": "Tool making.", "latency_ms": 1385.5257034301758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "to make implements with an edge, a point, or a percussion surface.", "latency_ms": 6447.375535964966, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "to make implements with an edge, a point, or a percussion surface.", "latency_ms": 7681.347608566284, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1300}, "3": {"k": 3, "answer": "to make implements with an edge, a point, or a percussion surface.", "latency_ms": 5394.630670547485, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1913}, "4": {"k": 4, "answer": "to make implements with an edge, a point, or a percussion surface.", "latency_ms": 5459.349155426025, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2585}, "5": {"k": 5, "answer": "to make implements with an edge, a point, or a percussion surface.", "latency_ms": 6392.622709274292, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3228}, "6": {"k": 6, "answer": "to make implements with an edge, a point, or a percussion surface.", "latency_ms": 7390.682935714722, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3820}}}
{"qid": "7402184777947527945", "question": "who wrote the text for jeanie with the light brown hair", "gold_answers": ["Stephen Foster"], "shots": {"0": {"k": 0, "answer": "Maud Irving", "latency_ms": 1561.3245964050293, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Stephen Foster", "latency_ms": 2548.814535140991, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 632}, "2": {"k": 2, "answer": "Stephen Foster", "latency_ms": 2565.605401992798, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1222}, "3": {"k": 3, "answer": "Stephen Foster", "latency_ms": 2083.0869674682617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1871}, "4": {"k": 4, "answer": "Stephen Foster", "latency_ms": 2080.9547901153564, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2490}, "5": {"k": 5, "answer": "Stephen Foster", "latency_ms": 1294.520616531372, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3107}, "6": {"k": 6, "answer": "Stephen Foster", "latency_ms": 980.1132678985596, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3752}}}
{"qid": "1656745334095178455", "question": "the basic steps of rational decision making include defining goals and", "gold_answers": ["Implementation of plans / policies"], "shots": {"0": {"k": 0, "answer": "identifying alternatives", "latency_ms": 2380.5742263793945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "problem definition", "latency_ms": 2384.991407394409, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 774}, "2": {"k": 2, "answer": "problem definition", "latency_ms": 2187.267780303955, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1428}, "3": {"k": 3, "answer": "problem definition", "latency_ms": 949.962854385376, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2061}, "4": {"k": 4, "answer": "problem definition", "latency_ms": 1967.0968055725098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2713}, "5": {"k": 5, "answer": "problem definition", "latency_ms": 2562.847137451172, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3412}, "6": {"k": 6, "answer": "problem definition", "latency_ms": 1298.9115715026855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4068}}}
{"qid": "-216657635823030751", "question": "when was the design period known as art deco", "gold_answers": ["1910 -- 1939"], "shots": {"0": {"k": 0, "answer": "1920-1940", "latency_ms": 2238.1322383880615, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1930s", "latency_ms": 1794.6135997772217, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 647}, "2": {"k": 2, "answer": "just before World War I", "latency_ms": 2386.8651390075684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1329}, "3": {"k": 3, "answer": "just before World War I", "latency_ms": 3124.5572566986084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2011}, "4": {"k": 4, "answer": "just before World War I", "latency_ms": 2693.603754043579, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2679}, "5": {"k": 5, "answer": "just before World War I", "latency_ms": 3010.5648040771484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3309}, "6": {"k": 6, "answer": "just before World War I", "latency_ms": 2486.886501312256, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3972}}}
{"qid": "250542332339248886", "question": "where does the bob and tom show broadcast from", "gold_answers": ["WFBQ in Indianapolis , Indiana", "Indianapolis , Indiana"], "shots": {"0": {"k": 0, "answer": "Indianapolis, Indiana", "latency_ms": 2688.9243125915527, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indianapolis, Indiana", "latency_ms": 3180.462598800659, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "WFBQ's studios in Indianapolis, Indiana", "latency_ms": 4205.174207687378, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1284}, "3": {"k": 3, "answer": "WFBQ's studios in Indianapolis, Indiana", "latency_ms": 4659.281015396118, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1943}, "4": {"k": 4, "answer": "WFBQ's studios in Indianapolis, Indiana", "latency_ms": 4512.784719467163, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2574}, "5": {"k": 5, "answer": "WFBQ's studios in Indianapolis, Indiana", "latency_ms": 4904.218912124634, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3192}, "6": {"k": 6, "answer": "WFBQ's studios in Indianapolis, Indiana", "latency_ms": 4994.553089141846, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3777}}}
{"qid": "7610758795954724809", "question": "star wars the clone wars season 3 episode 1", "gold_answers": ["Clone Cadets"], "shots": {"0": {"k": 0, "answer": "Clone Cadets", "latency_ms": 852.8258800506592, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2847.707509994507, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 673}, "2": {"k": 2, "answer": "Nightsisters", "latency_ms": 2687.631130218506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1370}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2871.9968795776367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2013}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2779.5305252075195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2693}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2844.1720008850098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3333}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2782.243013381958, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3979}}}
{"qid": "2997223939033016160", "question": "what part of brain is responsible for complex thinking", "gold_answers": ["the frontal lobe"], "shots": {"0": {"k": 0, "answer": "Prefrontal cortex", "latency_ms": 2433.2640171051025, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Frontal lobe", "latency_ms": 3153.087377548218, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 702}, "2": {"k": 2, "answer": "Frontal lobe", "latency_ms": 2432.55615234375, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1345}, "3": {"k": 3, "answer": "Cerebral cortex", "latency_ms": 3265.509843826294, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1975}, "4": {"k": 4, "answer": "Cerebral cortex", "latency_ms": 2923.4659671783447, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2688}, "5": {"k": 5, "answer": "Cerebral cortex", "latency_ms": 2671.651601791382, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3385}, "6": {"k": 6, "answer": "Cerebral cortex", "latency_ms": 2629.3771266937256, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4069}}}
{"qid": "2680196226239522881", "question": "who won the champions league final in 2016", "gold_answers": ["Real Madrid"], "shots": {"0": {"k": 0, "answer": "Real Madrid", "latency_ms": 1737.199068069458, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3233.4675788879395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 662}, "2": {"k": 2, "answer": "Real Madrid", "latency_ms": 1307.180404663086, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1254}, "3": {"k": 3, "answer": "Real Madrid", "latency_ms": 2219.9511528015137, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1880}, "4": {"k": 4, "answer": "Real Madrid", "latency_ms": 2486.048460006714, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2536}, "5": {"k": 5, "answer": "Real Madrid", "latency_ms": 1516.7748928070068, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3169}, "6": {"k": 6, "answer": "Real Madrid", "latency_ms": 2074.604272842407, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3831}}}
{"qid": "-8458006812258546025", "question": "who has the most gold medals in the winter olympics of all time", "gold_answers": ["Norway"], "shots": {"0": {"k": 0, "answer": "Marit Bjoergen", "latency_ms": 2259.016990661621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Canada", "latency_ms": 1130.8906078338623, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Johannes Høsflot Klæbo and Martin Fourcade", "latency_ms": 7150.692462921143, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1316}, "3": {"k": 3, "answer": "Canada", "latency_ms": 690.0923252105713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1941}, "4": {"k": 4, "answer": "Canada", "latency_ms": 1498.6116886138916, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2564}, "5": {"k": 5, "answer": "Canada", "latency_ms": 1740.8719062805176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3175}, "6": {"k": 6, "answer": "Marit Bjørgen", "latency_ms": 3031.3169956207275, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3792}}}
