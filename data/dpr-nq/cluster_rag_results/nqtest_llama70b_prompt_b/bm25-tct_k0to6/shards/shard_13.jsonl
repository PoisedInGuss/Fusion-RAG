{"qid": "175824256524792922", "question": "who plays harry's mom in harry potter", "gold_answers": ["Geraldine Margaret Agnew - Somerville"], "shots": {"0": {"k": 0, "answer": "Fiona Shaw", "latency_ms": 1820.1847076416016, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Molly", "latency_ms": 1691.3814544677734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 607}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2670.6998348236084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1248}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2353.302001953125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1894}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2104.173421859741, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2511}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2983.839988708496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3129}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2925.2166748046875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3760}}}
{"qid": "-8223571798173928629", "question": "who sang you'll be a woman soon in pulp fiction", "gold_answers": ["Urge Overkill", "rock band Urge Overkill"], "shots": {"0": {"k": 0, "answer": "Urge Overkill", "latency_ms": 2583.9486122131348, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Urge Overkill", "latency_ms": 2757.317543029785, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 653}, "2": {"k": 2, "answer": "Urge Overkill", "latency_ms": 2908.6520671844482, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1277}, "3": {"k": 3, "answer": "Urge Overkill", "latency_ms": 3438.8484954833984, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1946}, "4": {"k": 4, "answer": "Urge Overkill", "latency_ms": 3175.72283744812, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2583}, "5": {"k": 5, "answer": "Urge Overkill", "latency_ms": 2648.509979248047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3176}, "6": {"k": 6, "answer": "Urge Overkill", "latency_ms": 2701.3278007507324, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3781}}}
{"qid": "-8095180930653230478", "question": "where does a roadrunner live in the desert", "gold_answers": ["arid lowland", "mountainous shrubland"], "shots": {"0": {"k": 0, "answer": "Southwestern United States and Mexico", "latency_ms": 3452.4881839752197, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "arid lowland or mountainous shrubland", "latency_ms": 5310.630083084106, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 684}, "2": {"k": 2, "answer": "arid lowland or mountainous shrubland", "latency_ms": 5630.137920379639, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1288}, "3": {"k": 3, "answer": "Arid lowland or mountainous shrubland", "latency_ms": 5920.78161239624, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1951}, "4": {"k": 4, "answer": "Arid lowland or mountainous shrubland", "latency_ms": 5181.564092636108, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2601}, "5": {"k": 5, "answer": "Arid lowland or mountainous shrubland", "latency_ms": 4172.126770019531, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3274}, "6": {"k": 6, "answer": "arid lowland or mountainous shrubland", "latency_ms": 5051.687955856323, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3870}}}
{"qid": "-5489426796364143729", "question": "who was the first president of the constituent assembly", "gold_answers": ["Dr. Rajendra Prasad"], "shots": {"0": {"k": 0, "answer": "Dr. Rajendra Prasad", "latency_ms": 2559.6723556518555, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Karl Seitz", "latency_ms": 1682.8622817993164, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 683}, "2": {"k": 2, "answer": "Dr. Rajendra Prasad", "latency_ms": 2693.9337253570557, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1428}, "3": {"k": 3, "answer": "Karl Seitz", "latency_ms": 2247.312307357788, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2127}, "4": {"k": 4, "answer": "Karl Seitz", "latency_ms": 1912.571668624878, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2827}, "5": {"k": 5, "answer": "Karl Seitz", "latency_ms": 2440.6163692474365, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3522}, "6": {"k": 6, "answer": "Dr. Sachchidananda Sinha", "latency_ms": 3127.704620361328, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 4236}}}
{"qid": "7835345385704208729", "question": "when does the new episode of scorpion come on", "gold_answers": ["January 15 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 2193.314552307129, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3005.643844604492, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 657}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 3002.497434616089, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1312}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2511.9948387145996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2002}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2617.7821159362793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2567}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1712.3546600341797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3185}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3250.316381454468, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3803}}}
{"qid": "907238223200158003", "question": "when did first fast and furious come out", "gold_answers": ["2001"], "shots": {"0": {"k": 0, "answer": "June 22, 2001", "latency_ms": 3425.888776779175, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "2001", "latency_ms": 2301.1019229888916, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 640}, "2": {"k": 2, "answer": "2001", "latency_ms": 2017.8816318511963, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1249}, "3": {"k": 3, "answer": "2001", "latency_ms": 1255.802869796753, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1881}, "4": {"k": 4, "answer": "2001", "latency_ms": 1566.1015510559082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2514}, "5": {"k": 5, "answer": "2001", "latency_ms": 1979.9511432647705, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3112}, "6": {"k": 6, "answer": "2001", "latency_ms": 1156.2395095825195, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3738}}}
{"qid": "-6782664238831495856", "question": "when does season 5 of the blacklist resume", "gold_answers": ["January 31 , 2018"], "shots": {"0": {"k": 0, "answer": "March 1, 2018", "latency_ms": 3328.073263168335, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 27, 2017", "latency_ms": 4217.203855514526, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "September 27, 2017", "latency_ms": 4077.1701335906982, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1361}, "3": {"k": 3, "answer": "September 27, 2017", "latency_ms": 4463.099956512451, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "September 27, 2017", "latency_ms": 3110.459327697754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2653}, "5": {"k": 5, "answer": "September 27, 2017", "latency_ms": 2588.329076766968, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3286}, "6": {"k": 6, "answer": "September 27, 2017", "latency_ms": 3090.477228164673, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4033}}}
{"qid": "3492797766982308854", "question": "when do mr schuester and emma get together", "gold_answers": ["the fourth season"], "shots": {"0": {"k": 0, "answer": "Season 3.", "latency_ms": 2727.7233600616455, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2139.6822929382324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 614}, "2": {"k": 2, "answer": "Yes/No episode", "latency_ms": 2450.0668048858643, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1186}, "3": {"k": 3, "answer": "Yes/No episode", "latency_ms": 2234.6925735473633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1775}, "4": {"k": 4, "answer": "In \"The Power of Madonna\"", "latency_ms": 2549.8640537261963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2338}, "5": {"k": 5, "answer": "In \"The Power of Madonna\"", "latency_ms": 3727.491617202759, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2957}, "6": {"k": 6, "answer": "In \"Hell-O\"", "latency_ms": 3274.3377685546875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3580}}}
{"qid": "-2654989306076235475", "question": "when does the next episode of izombie air", "gold_answers": ["May 14 , 2018", "May 7 , 2018"], "shots": {"0": {"k": 0, "answer": "I cannot answer.", "latency_ms": 2493.6749935150146, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2479.6340465545654, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 596}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2443.8939094543457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1192}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2990.9510612487793, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1856}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1680.8199882507324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2524}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2695.8796977996826, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3136}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2676.4934062957764, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3782}}}
{"qid": "3066393401707107613", "question": "where was the u.s.s maine when it exploded in 1898", "gold_answers": ["Havana Harbor"], "shots": {"0": {"k": 0, "answer": "Havana Harbor", "latency_ms": 2401.341199874878, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "in the harbor", "latency_ms": 2125.281572341919, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "the harbor", "latency_ms": 1745.0511455535889, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1218}, "3": {"k": 3, "answer": "Havana Harbor", "latency_ms": 2382.512092590332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1790}, "4": {"k": 4, "answer": "Havana Harbor", "latency_ms": 2159.3289375305176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2364}, "5": {"k": 5, "answer": "Havana Harbor", "latency_ms": 2499.333381652832, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2991}, "6": {"k": 6, "answer": "Havana Harbor", "latency_ms": 2370.492458343506, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3625}}}
{"qid": "6972056626325669866", "question": "in the honour of which god is anant chaturdashi celebrated", "gold_answers": ["Ganesh"], "shots": {"0": {"k": 0, "answer": "Vishnu", "latency_ms": 1177.7329444885254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3032.4320793151855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2704.0956020355225, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1272}, "3": {"k": 3, "answer": "No specific god mentioned", "latency_ms": 2814.363956451416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1935}, "4": {"k": 4, "answer": "No specific god mentioned", "latency_ms": 2778.254508972168, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2576}, "5": {"k": 5, "answer": "None mentioned", "latency_ms": 1751.7714500427246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3149}, "6": {"k": 6, "answer": "Lord Vishnu and Lord Shiva", "latency_ms": 3076.5933990478516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3740}}}
{"qid": "6043389452532567502", "question": "which central american nations border the pacific ocean and the caribbean sea", "gold_answers": ["Costa Rica", "Nicaragua"], "shots": {"0": {"k": 0, "answer": "Costa Rica, Nicaragua, Panama.", "latency_ms": 3747.7288246154785, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "All of them", "latency_ms": 1531.057357788086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "All of them", "latency_ms": 1638.4973526000977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1285}, "3": {"k": 3, "answer": "Costa Rica, Nicaragua, Panama, Guatemala, Honduras, El Salvador, Belize", "latency_ms": 7404.524564743042, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 1944}, "4": {"k": 4, "answer": "Belize and Guatemala", "latency_ms": 2639.6985054016113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2591}, "5": {"k": 5, "answer": "Guatemala, Belize, Costa Rica, Nicaragua, Honduras, Panama, and El Salvador", "latency_ms": 6198.52876663208, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3186}, "6": {"k": 6, "answer": "Guatemala, Belize", "latency_ms": 2130.3350925445557, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3883}}}
{"qid": "2541316470469570888", "question": "what kind of sentence contains an independent clause and a dependent clause", "gold_answers": ["A complex sentence", "complex sentence"], "shots": {"0": {"k": 0, "answer": "Complex sentence.", "latency_ms": 2327.286720275879, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Complex sentence", "latency_ms": 1499.5274543762207, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 704}, "2": {"k": 2, "answer": "A typical sentence", "latency_ms": 2339.86496925354, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1360}, "3": {"k": 3, "answer": "Typical sentence", "latency_ms": 1327.3420333862305, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2042}, "4": {"k": 4, "answer": "Complex sentence", "latency_ms": 2245.190382003784, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2701}, "5": {"k": 5, "answer": "Complex sentence", "latency_ms": 1275.7630348205566, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3365}, "6": {"k": 6, "answer": "Complex sentence", "latency_ms": 2641.2417888641357, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4053}}}
{"qid": "3239205946903665891", "question": "what was the book wealth of nations about", "gold_answers": ["what builds nations ' wealth"], "shots": {"0": {"k": 0, "answer": "Economics and capitalism.", "latency_ms": 3251.7879009246826, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "the nature and causes of the wealth of nations", "latency_ms": 4408.092975616455, "em": 0.0, "f1": 0.36363636363636365, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "Nations' wealth", "latency_ms": 2546.7495918273926, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1265}, "3": {"k": 3, "answer": "Nations' wealth", "latency_ms": 2689.5833015441895, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1912}, "4": {"k": 4, "answer": "The Wealth of Nations", "latency_ms": 2823.8234519958496, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 2541}, "5": {"k": 5, "answer": "Nature and Causes of the Wealth of Nations", "latency_ms": 4389.812469482422, "em": 0.0, "f1": 0.36363636363636365, "containment": 0.0, "context_chars": 3218}, "6": {"k": 6, "answer": "Nature and Causes of the Wealth of Nations", "latency_ms": 3915.8055782318115, "em": 0.0, "f1": 0.36363636363636365, "containment": 0.0, "context_chars": 3881}}}
{"qid": "-4659426898177460954", "question": "when will the flash season 4 be released", "gold_answers": ["October 10 , 2017"], "shots": {"0": {"k": 0, "answer": "October 10, 2017", "latency_ms": 3430.0105571746826, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "August 28, 2018", "latency_ms": 4162.435054779053, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 613}, "2": {"k": 2, "answer": "August 28, 2018", "latency_ms": 3058.231830596924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1205}, "3": {"k": 3, "answer": "August 28, 2018", "latency_ms": 3972.3687171936035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1844}, "4": {"k": 4, "answer": "August 28, 2018", "latency_ms": 3875.6542205810547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2491}, "5": {"k": 5, "answer": "August 28, 2018", "latency_ms": 4401.719093322754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3106}, "6": {"k": 6, "answer": "August 28, 2018", "latency_ms": 3283.93816947937, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3763}}}
{"qid": "-1204722541964024977", "question": "what is australia's location in the world and region", "gold_answers": ["Oceania"], "shots": {"0": {"k": 0, "answer": "Oceania, Southern Hemisphere.", "latency_ms": 4072.781562805176, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Western Australia", "latency_ms": 1560.2607727050781, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 742}, "2": {"k": 2, "answer": "Western Australia", "latency_ms": 1471.2746143341064, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1408}, "3": {"k": 3, "answer": "Western Australia", "latency_ms": 2251.9242763519287, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2036}, "4": {"k": 4, "answer": "Western Australia", "latency_ms": 1240.8819198608398, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2664}, "5": {"k": 5, "answer": "Western Australia", "latency_ms": 1516.25394821167, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3339}, "6": {"k": 6, "answer": "Eastern coast", "latency_ms": 1968.4207439422607, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3998}}}
{"qid": "-6291722830448463311", "question": "who does luke skywalker fall in love with", "gold_answers": ["Mara Jade"], "shots": {"0": {"k": 0, "answer": "Mara Jade", "latency_ms": 2322.459936141968, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Leia", "latency_ms": 1671.870470046997, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "Leia", "latency_ms": 1529.949426651001, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1190}, "3": {"k": 3, "answer": "Leia", "latency_ms": 2002.9370784759521, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1809}, "4": {"k": 4, "answer": "Leia", "latency_ms": 1771.7444896697998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2435}, "5": {"k": 5, "answer": "Mara Jade", "latency_ms": 1206.9926261901855, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3055}, "6": {"k": 6, "answer": "Mara Jade", "latency_ms": 2087.9759788513184, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3620}}}
{"qid": "-5631283176059213935", "question": "who was the original host of jokers wild", "gold_answers": ["Jack Barry"], "shots": {"0": {"k": 0, "answer": "Jack Barry", "latency_ms": 1457.7336311340332, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Jack Barry", "latency_ms": 2250.7379055023193, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 617}, "2": {"k": 2, "answer": "Barry Cryer", "latency_ms": 2381.1752796173096, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1167}, "3": {"k": 3, "answer": "Barry Cryer", "latency_ms": 3119.3108558654785, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1717}, "4": {"k": 4, "answer": "Barry Cryer", "latency_ms": 2423.187017440796, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2312}, "5": {"k": 5, "answer": "Barry Cryer", "latency_ms": 2910.2535247802734, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2905}, "6": {"k": 6, "answer": "Barry Cryer", "latency_ms": 2404.3149948120117, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3488}}}
{"qid": "5360223163814883105", "question": "which organelle is the site of atp production and storage", "gold_answers": ["mitochondrion"], "shots": {"0": {"k": 0, "answer": "Mitochondria", "latency_ms": 3120.8858489990234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "mitochondrial membrane", "latency_ms": 1613.2714748382568, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 690}, "2": {"k": 2, "answer": "mitochondrial membrane", "latency_ms": 2860.279321670532, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1347}, "3": {"k": 3, "answer": "Mitochondria", "latency_ms": 2283.6809158325195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1998}, "4": {"k": 4, "answer": "Mitochondria", "latency_ms": 2414.3521785736084, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2657}, "5": {"k": 5, "answer": "Mitochondria, Chloroplast", "latency_ms": 3051.708698272705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3366}, "6": {"k": 6, "answer": "Mitochondrion", "latency_ms": 2640.103340148926, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4139}}}
{"qid": "-718217992489656032", "question": "five tools of security management to overcome computer crime", "gold_answers": ["post-evaluation", "operative planning", "implementation", "Pre-evaluation", "strategic planning"], "shots": {"0": {"k": 0, "answer": "Firewall, Encryption, Antivirus, Access Control, Intrusion Detection.", "latency_ms": 6023.006916046143, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2097.47052192688, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 753}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1868.084192276001, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1462}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2796.4861392974854, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2155}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1601.8383502960205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2794}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2571.0270404815674, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3477}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2558.3271980285645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4207}}}
{"qid": "5584540254904933863", "question": "who sang what are we doing in love", "gold_answers": ["Dottie West", "Kenny Rogers"], "shots": {"0": {"k": 0, "answer": "Kenny Rogers and Dottie West", "latency_ms": 2661.8731021881104, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2892.1260833740234, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2112.1017932891846, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1263}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2906.531810760498, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1847}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2863.750696182251, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2465}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2202.0981311798096, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3066}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3429.596424102783, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3669}}}
{"qid": "97369777493072047", "question": "at what age are you no longer a toddler", "gold_answers": ["36 months"], "shots": {"0": {"k": 0, "answer": "3 years old", "latency_ms": 1145.0273990631104, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "36 months", "latency_ms": 1744.9779510498047, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 601}, "2": {"k": 2, "answer": "36 months", "latency_ms": 1234.2731952667236, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1220}, "3": {"k": 3, "answer": "36 months", "latency_ms": 2588.413715362549, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1765}, "4": {"k": 4, "answer": "Three years old", "latency_ms": 2355.4818630218506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2456}, "5": {"k": 5, "answer": "36 months", "latency_ms": 1749.8893737792969, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3064}, "6": {"k": 6, "answer": "36 months", "latency_ms": 1757.349967956543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3657}}}
{"qid": "3537093784371401720", "question": "how many episodes are there in ordeal by innocence", "gold_answers": ["three", "three part"], "shots": {"0": {"k": 0, "answer": "3", "latency_ms": 1061.8479251861572, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Three-part", "latency_ms": 1261.087417602539, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 652}, "2": {"k": 2, "answer": "Three-part", "latency_ms": 1516.366720199585, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1268}, "3": {"k": 3, "answer": "Three-part", "latency_ms": 2043.6179637908936, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 1892}, "4": {"k": 4, "answer": "Three-part", "latency_ms": 1258.0347061157227, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 2532}, "5": {"k": 5, "answer": "Three", "latency_ms": 634.5412731170654, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3179}, "6": {"k": 6, "answer": "Three-part", "latency_ms": 1998.1024265289307, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 3783}}}
{"qid": "-2126094349681352450", "question": "who played the hobbits in the lord of the rings", "gold_answers": ["Ian Holm", "Dominic Monaghan", "Elijah Wood as Frodo Baggins", "Sean Astin", "Elijah Wood", "Billy Boyd"], "shots": {"0": {"k": 0, "answer": "Elijah Wood, Sean Astin, Billy Boyd, Dominic Monaghan.", "latency_ms": 5413.728475570679, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2559.8573684692383, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 666}, "2": {"k": 2, "answer": "Frodo Baggins, Samwise Gamgee, Peregrin Took, Meriadoc Brandybuck", "latency_ms": 9205.056428909302, "em": 0.0, "f1": 0.3076923076923077, "containment": 0.0, "context_chars": 1260}, "3": {"k": 3, "answer": "Frodo Baggins, Samwise Gamgee, Peregrin Took, Meriadoc Brandybuck", "latency_ms": 9916.029453277588, "em": 0.0, "f1": 0.3076923076923077, "containment": 0.0, "context_chars": 1926}, "4": {"k": 4, "answer": "Frodo Baggins, Samwise Gamgee, Peregrin Took, Meriadoc Brandybuck", "latency_ms": 8760.50853729248, "em": 0.0, "f1": 0.3076923076923077, "containment": 0.0, "context_chars": 2512}, "5": {"k": 5, "answer": "Frodo Baggins, Samwise Gamgee, Peregrin Took, Meriadoc Brandybuck", "latency_ms": 10558.51149559021, "em": 0.0, "f1": 0.3076923076923077, "containment": 0.0, "context_chars": 3118}, "6": {"k": 6, "answer": "Elijah Wood, Ian Holm", "latency_ms": 3923.0945110321045, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3747}}}
{"qid": "-6982991192528081677", "question": "what does fancy dress mean in the uk", "gold_answers": ["costumes", "A costume", "guests dress up in costumes"], "shots": {"0": {"k": 0, "answer": "Costume party.", "latency_ms": 2767.717123031616, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Costume.", "latency_ms": 2572.848320007324, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 569}, "2": {"k": 2, "answer": "Costume.", "latency_ms": 1829.7057151794434, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1181}, "3": {"k": 3, "answer": "Costume.", "latency_ms": 2096.1966514587402, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1750}, "4": {"k": 4, "answer": "Costume.", "latency_ms": 2783.905506134033, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2363}, "5": {"k": 5, "answer": "Costume", "latency_ms": 2503.527879714966, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3033}, "6": {"k": 6, "answer": "Costume", "latency_ms": 1760.1852416992188, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3705}}}
{"qid": "-6751220433242447969", "question": "how many seasons of the bastard executioner are there", "gold_answers": ["one season", "one"], "shots": {"0": {"k": 0, "answer": "1", "latency_ms": 1935.6122016906738, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "One", "latency_ms": 1182.3337078094482, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 604}, "2": {"k": 2, "answer": "One", "latency_ms": 1189.312219619751, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1204}, "3": {"k": 3, "answer": "One", "latency_ms": 1732.5849533081055, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1902}, "4": {"k": 4, "answer": "One", "latency_ms": 1226.5048027038574, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2527}, "5": {"k": 5, "answer": "One", "latency_ms": 1199.0633010864258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3150}, "6": {"k": 6, "answer": "One", "latency_ms": 1654.9060344696045, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3776}}}
{"qid": "-7607921794902030568", "question": "why did jean valjean take care of cosette", "gold_answers": ["he had promised"], "shots": {"0": {"k": 0, "answer": "Gratitude to Fantine", "latency_ms": 1781.958818435669, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2403.0094146728516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 604}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2732.624053955078, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1242}, "3": {"k": 3, "answer": "He pitied her.", "latency_ms": 2747.638940811157, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1854}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1900.7477760314941, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2467}, "5": {"k": 5, "answer": "Fantine's letter authorizing him to take Cosette", "latency_ms": 4837.964296340942, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3117}, "6": {"k": 6, "answer": "Fantine's letter authorizing him to take Cosette", "latency_ms": 3934.0970516204834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3717}}}
{"qid": "-5341729935918652247", "question": "who does the democratic republic of congo trade with", "gold_answers": ["Belgium", "World Bank", "China", "Zambia", "African Development Bank", "SADC", "South Africa", "AU", "WTO", "France", "Kenya", "Zimbabwe", "Group of 77", "IMF"], "shots": {"0": {"k": 0, "answer": "China, Belgium, South Korea, Zambia, United States.", "latency_ms": 4682.786226272583, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Russia", "latency_ms": 1161.0887050628662, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 699}, "2": {"k": 2, "answer": "Russia", "latency_ms": 1228.2812595367432, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1393}, "3": {"k": 3, "answer": "Russia", "latency_ms": 1916.2514209747314, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2112}, "4": {"k": 4, "answer": "Kenya", "latency_ms": 1257.9028606414795, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2802}, "5": {"k": 5, "answer": "Russia, United Kingdom, Kenya, Canada", "latency_ms": 4223.215818405151, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3529}, "6": {"k": 6, "answer": "Kenya, Canada, Russia, United Kingdom", "latency_ms": 4513.747692108154, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 4223}}}
{"qid": "5518515944351776633", "question": "what was the purpose of the bantu education act", "gold_answers": ["enforcing racially separated educational facilities"], "shots": {"0": {"k": 0, "answer": "Segregation and oppression.", "latency_ms": 3279.482364654541, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "to consolidate Bantu education", "latency_ms": 2252.31671333313, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 714}, "2": {"k": 2, "answer": "to consolidate Bantu education", "latency_ms": 2531.648874282837, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1413}, "3": {"k": 3, "answer": "to consolidate Bantu education", "latency_ms": 3426.2709617614746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2090}, "4": {"k": 4, "answer": "to ensure blacks had only enough education to work as unskilled laborers", "latency_ms": 7091.314077377319, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2704}, "5": {"k": 5, "answer": "to ensure blacks would have only just enough education to work as unskilled laborers", "latency_ms": 7243.1981563568115, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3382}, "6": {"k": 6, "answer": "to consolidate Bantu education and implement discriminatory practices uniformly", "latency_ms": 6511.200189590454, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4022}}}
{"qid": "-4271099756149693907", "question": "what type of snake is kaa in jungle book", "gold_answers": ["Indian rock python"], "shots": {"0": {"k": 0, "answer": "Indian Rock Python", "latency_ms": 2190.0293827056885, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Huge and powerful snake", "latency_ms": 1812.4215602874756, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "Huge and powerful snake", "latency_ms": 4017.0485973358154, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1174}, "3": {"k": 3, "answer": "Huge and powerful snake", "latency_ms": 3038.055181503296, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1759}, "4": {"k": 4, "answer": "Python", "latency_ms": 2480.971336364746, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2384}, "5": {"k": 5, "answer": "Python", "latency_ms": 1223.6628532409668, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3007}, "6": {"k": 6, "answer": "Python", "latency_ms": 1254.2839050292969, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3616}}}
{"qid": "-6527462410412242988", "question": "what is the snow on mars made of", "gold_answers": ["water ice", "Frozen carbon dioxide"], "shots": {"0": {"k": 0, "answer": "Dry ice", "latency_ms": 2083.245038986206, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "frost or snow mixed with dust", "latency_ms": 2864.100217819214, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "frost or snow mixed generously with dust", "latency_ms": 3918.104887008667, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1229}, "3": {"k": 3, "answer": "frost or snow mixed generously with dust", "latency_ms": 4202.5346755981445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1874}, "4": {"k": 4, "answer": "Ice with dust", "latency_ms": 2622.02525138855, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2522}, "5": {"k": 5, "answer": "Ice with surface debris", "latency_ms": 2187.295913696289, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3140}, "6": {"k": 6, "answer": "water-ice", "latency_ms": 1613.7690544128418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3727}}}
{"qid": "-2155345050664800035", "question": "when does elena turn into a vampire in the tv series", "gold_answers": ["season four", "fourth season"], "shots": {"0": {"k": 0, "answer": "Season 3, Episode 22.", "latency_ms": 4974.135637283325, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "End of season 3", "latency_ms": 3295.7773208618164, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 621}, "2": {"k": 2, "answer": "In the hospital.", "latency_ms": 2458.7230682373047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1222}, "3": {"k": 3, "answer": "End of season 3", "latency_ms": 3033.055305480957, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1786}, "4": {"k": 4, "answer": "End of season 3", "latency_ms": 2248.3198642730713, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2348}, "5": {"k": 5, "answer": "End of season 3", "latency_ms": 2551.136016845703, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2913}, "6": {"k": 6, "answer": "End of season 3", "latency_ms": 2724.306106567383, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3477}}}
{"qid": "5491319565508469860", "question": "who gets first pick of fa cup games", "gold_answers": ["the BBC", "BBC"], "shots": {"0": {"k": 0, "answer": "The home team", "latency_ms": 1886.9683742523193, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "None", "latency_ms": 2297.454833984375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 578}, "2": {"k": 2, "answer": "Premier League clubs", "latency_ms": 2376.6260147094727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1168}, "3": {"k": 3, "answer": "Premier League clubs", "latency_ms": 3159.902572631836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1749}, "4": {"k": 4, "answer": "Premier League clubs", "latency_ms": 2045.3617572784424, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2396}, "5": {"k": 5, "answer": "the team named first", "latency_ms": 2865.534543991089, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2983}, "6": {"k": 6, "answer": "the team named first", "latency_ms": 1836.451530456543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3592}}}
{"qid": "-816889472900831237", "question": "what part of dc is howard university in", "gold_answers": ["northwest", "northwest Washington", "in northwest Washington"], "shots": {"0": {"k": 0, "answer": "Shaw.", "latency_ms": 2099.4818210601807, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Northwest", "latency_ms": 2243.6769008636475, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 670}, "2": {"k": 2, "answer": "Northwest Washington, DC", "latency_ms": 2656.9855213165283, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 1346}, "3": {"k": 3, "answer": "Northwest", "latency_ms": 2768.7759399414062, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2040}, "4": {"k": 4, "answer": "Northwest Washington, DC", "latency_ms": 2702.8584480285645, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2831}, "5": {"k": 5, "answer": "Northwest", "latency_ms": 1524.1761207580566, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3517}, "6": {"k": 6, "answer": "Northwest Washington, DC", "latency_ms": 3018.955707550049, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 4166}}}
{"qid": "479254158383415496", "question": "what is the first covenant in the old testament", "gold_answers": ["the Noahic Covenant"], "shots": {"0": {"k": 0, "answer": "Adamic Covenant", "latency_ms": 1371.718168258667, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2757.6990127563477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Judaism's understanding of the covenant", "latency_ms": 3302.4613857269287, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1234}, "3": {"k": 3, "answer": "Mosaic covenant", "latency_ms": 2344.2764282226562, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1845}, "4": {"k": 4, "answer": "Mosaic covenant", "latency_ms": 1573.3394622802734, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2483}, "5": {"k": 5, "answer": "Mosaic covenant", "latency_ms": 2083.5719108581543, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3140}, "6": {"k": 6, "answer": "Mosaic covenant", "latency_ms": 1805.6895732879639, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3810}}}
{"qid": "-8025421046364238920", "question": "who sings the theme song for living single", "gold_answers": ["Queen Latifah"], "shots": {"0": {"k": 0, "answer": "Queen Latifah", "latency_ms": 1509.6020698547363, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1503.115177154541, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2254.2316913604736, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1188}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2298.290014266968, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1768}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2721.9085693359375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2371}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1883.1963539123535, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2968}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2174.516439437866, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3539}}}
{"qid": "-2347826883564742884", "question": "who said life is like a bowl of cherries", "gold_answers": ["Lew Brown"], "shots": {"0": {"k": 0, "answer": "Yip Harburg", "latency_ms": 1386.9941234588623, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Rudy Vallée", "latency_ms": 3182.853937149048, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 590}, "2": {"k": 2, "answer": "Rudy Vallée", "latency_ms": 2165.173053741455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1180}, "3": {"k": 3, "answer": "Rudy Vallée", "latency_ms": 2839.876890182495, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1785}, "4": {"k": 4, "answer": "Rudy Vallée", "latency_ms": 2443.1495666503906, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2383}, "5": {"k": 5, "answer": "Rudy Vallée", "latency_ms": 2139.145612716675, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3018}, "6": {"k": 6, "answer": "Rudy Vallée", "latency_ms": 2488.9473915100098, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3669}}}
{"qid": "2467509477831469982", "question": "where is fort myers located at in florida", "gold_answers": ["Lee County", "the Southwest"], "shots": {"0": {"k": 0, "answer": "Southwest Florida", "latency_ms": 2295.5479621887207, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lee County", "latency_ms": 1469.097375869751, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 642}, "2": {"k": 2, "answer": "Lee County", "latency_ms": 2519.023895263672, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1265}, "3": {"k": 3, "answer": "Lee County", "latency_ms": 2051.438808441162, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1882}, "4": {"k": 4, "answer": "Lee County", "latency_ms": 2043.1900024414062, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2529}, "5": {"k": 5, "answer": "Lee County", "latency_ms": 1510.5915069580078, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3149}, "6": {"k": 6, "answer": "Lee County", "latency_ms": 1377.6285648345947, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3746}}}
{"qid": "4440911260005876349", "question": "state the position of india in terms of population size", "gold_answers": ["second"], "shots": {"0": {"k": 0, "answer": "2nd", "latency_ms": 2830.432415008545, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3125.119686126709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 3992.1321868896484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1273}, "3": {"k": 3, "answer": "77%", "latency_ms": 1782.5124263763428, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1981}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2331.399917602539, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2572}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2661.545991897583, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3255}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2492.61736869812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3922}}}
{"qid": "3172949410241605868", "question": "who became a leader of the indian national congress", "gold_answers": ["Mahatma Gandhi"], "shots": {"0": {"k": 0, "answer": "Mahatma Gandhi", "latency_ms": 2771.169900894165, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Mahatma Gandhi", "latency_ms": 2105.1297187805176, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 694}, "2": {"k": 2, "answer": "Mahatma Gandhi", "latency_ms": 3161.3502502441406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1402}, "3": {"k": 3, "answer": "Mahatma Gandhi", "latency_ms": 1824.639081954956, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2124}, "4": {"k": 4, "answer": "Mahatma Gandhi", "latency_ms": 2097.1460342407227, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2818}, "5": {"k": 5, "answer": "Sonia Gandhi", "latency_ms": 2101.497173309326, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3492}, "6": {"k": 6, "answer": "Mahatma Gandhi", "latency_ms": 2277.782440185547, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4202}}}
{"qid": "-5136645336730213305", "question": "winner of worst cooks in america season 5", "gold_answers": ["Amber Brauner"], "shots": {"0": {"k": 0, "answer": "Jenae Anderson", "latency_ms": 2181.6556453704834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1926.452398300171, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 603}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2463.5183811187744, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1211}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2564.3057823181152, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1857}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 3017.270088195801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2448}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2221.8756675720215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3034}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2424.013376235962, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3645}}}
{"qid": "159123015669900402", "question": "what is the filename extension used for all java source files", "gold_answers": [". java"], "shots": {"0": {"k": 0, "answer": ".java", "latency_ms": 661.5772247314453, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": ".java", "latency_ms": 1731.1840057373047, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": ".java", "latency_ms": 1653.6262035369873, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1275}, "3": {"k": 3, "answer": ".java", "latency_ms": 1224.8806953430176, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1934}, "4": {"k": 4, "answer": ".java", "latency_ms": 1265.6924724578857, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2549}, "5": {"k": 5, "answer": ".java", "latency_ms": 1504.685878753662, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3152}, "6": {"k": 6, "answer": ".java", "latency_ms": 1516.158103942871, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3772}}}
{"qid": "6860341019198485637", "question": "who cracked the enigma code in world war 2", "gold_answers": ["Turing"], "shots": {"0": {"k": 0, "answer": "Alan Turing", "latency_ms": 1753.089189529419, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Poland", "latency_ms": 2343.3399200439453, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 683}, "2": {"k": 2, "answer": "Poland", "latency_ms": 1413.0730628967285, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1300}, "3": {"k": 3, "answer": "Poland", "latency_ms": 2546.861410140991, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1995}, "4": {"k": 4, "answer": "Poland", "latency_ms": 1742.8786754608154, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2696}, "5": {"k": 5, "answer": "Poland", "latency_ms": 1807.7855110168457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3423}, "6": {"k": 6, "answer": "Poland", "latency_ms": 2562.6885890960693, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4102}}}
{"qid": "7990281571644070368", "question": "what is the job of the whip in congress", "gold_answers": ["to ensure party discipline"], "shots": {"0": {"k": 0, "answer": "Enforcing party discipline and counting votes.", "latency_ms": 3518.146753311157, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "to offer promotion or threaten demotion", "latency_ms": 4367.734432220459, "em": 0.0, "f1": 0.2, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "Offer promotion or threaten demotion within the committee system.", "latency_ms": 6026.784181594849, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1309}, "3": {"k": 3, "answer": "Offer promotion or threaten demotion within the committee system.", "latency_ms": 6420.4676151275635, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1952}, "4": {"k": 4, "answer": "ensure members adhere to party policies", "latency_ms": 3265.9895420074463, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 2586}, "5": {"k": 5, "answer": "ensure members adhere to party policies", "latency_ms": 3545.872211456299, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 3310}, "6": {"k": 6, "answer": "ensure members adhere to party policies", "latency_ms": 3287.4832153320312, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 4003}}}
{"qid": "-3089533754017582743", "question": "when was the last time unc did not make the ncaa tournament", "gold_answers": ["2003"], "shots": {"0": {"k": 0, "answer": "2010", "latency_ms": 1523.6270427703857, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2355.790615081787, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 579}, "2": {"k": 2, "answer": "1978", "latency_ms": 1761.8145942687988, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1157}, "3": {"k": 3, "answer": "1979", "latency_ms": 2092.9694175720215, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1776}, "4": {"k": 4, "answer": "1973", "latency_ms": 1497.211217880249, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2331}, "5": {"k": 5, "answer": "1973", "latency_ms": 2060.746192932129, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3013}, "6": {"k": 6, "answer": "1973", "latency_ms": 1247.3485469818115, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3677}}}
{"qid": "-3530165048900528552", "question": "which foreign currency option is the​ right but not the​ obligation to buy foreign​ currency", "gold_answers": ["foreign exchange option"], "shots": {"0": {"k": 0, "answer": "Call option", "latency_ms": 1836.7838859558105, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "FX option", "latency_ms": 990.6368255615234, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 676}, "2": {"k": 2, "answer": "FX option", "latency_ms": 2077.3167610168457, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1364}, "3": {"k": 3, "answer": "FX option", "latency_ms": 1721.3702201843262, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2054}, "4": {"k": 4, "answer": "FX option", "latency_ms": 945.3401565551758, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2709}, "5": {"k": 5, "answer": "FX option", "latency_ms": 1249.5191097259521, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3368}, "6": {"k": 6, "answer": "FX option", "latency_ms": 1476.1364459991455, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3985}}}
{"qid": "2795932191977151266", "question": "how many house of representative do we have in nigeria", "gold_answers": ["360", "360 members"], "shots": {"0": {"k": 0, "answer": "360 members in 109 constituencies", "latency_ms": 3160.254716873169, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "360 members", "latency_ms": 1826.267957687378, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 721}, "2": {"k": 2, "answer": "360 members", "latency_ms": 1798.3765602111816, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1380}, "3": {"k": 3, "answer": "360", "latency_ms": 1210.2210521697998, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2030}, "4": {"k": 4, "answer": "360", "latency_ms": 1568.1166648864746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2724}, "5": {"k": 5, "answer": "360", "latency_ms": 1745.9039688110352, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3379}, "6": {"k": 6, "answer": "360 members", "latency_ms": 1801.328182220459, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4027}}}
{"qid": "4714895284721516061", "question": "who was the actor who played grizzly adams", "gold_answers": ["`` Dan '' Haggerty"], "shots": {"0": {"k": 0, "answer": "Dan Haggerty", "latency_ms": 2492.3431873321533, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dan Haggerty", "latency_ms": 2880.5224895477295, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "Dan Haggerty", "latency_ms": 2639.148473739624, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1285}, "3": {"k": 3, "answer": "Dan Haggerty", "latency_ms": 2381.368398666382, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1953}, "4": {"k": 4, "answer": "Dan Haggerty", "latency_ms": 2769.2158222198486, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2599}, "5": {"k": 5, "answer": "Dan Haggerty", "latency_ms": 2389.220952987671, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3240}, "6": {"k": 6, "answer": "Dan Haggerty", "latency_ms": 2221.5592861175537, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3952}}}
{"qid": "-6101054679785992133", "question": "who sings blame it on the bossa nova", "gold_answers": ["Eydie Gormé"], "shots": {"0": {"k": 0, "answer": "Eydie Gorme", "latency_ms": 2837.545156478882, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Gormé", "latency_ms": 1306.5986633300781, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 599}, "2": {"k": 2, "answer": "Eydie Gormé", "latency_ms": 3198.5957622528076, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1181}, "3": {"k": 3, "answer": "Eydie Gormé", "latency_ms": 2799.4799613952637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1757}, "4": {"k": 4, "answer": "Eydie Gormé", "latency_ms": 1768.9790725708008, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2334}, "5": {"k": 5, "answer": "Eydie Gormé", "latency_ms": 3096.087694168091, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2899}, "6": {"k": 6, "answer": "Eydie Gormé", "latency_ms": 2977.8544902801514, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3505}}}
{"qid": "1882117704293632224", "question": "who is the supreme court judge was a former chief justice of kerala high court", "gold_answers": ["Konakuppakatil Gopinathan Balakrishnan"], "shots": {"0": {"k": 0, "answer": "Kurian Joseph", "latency_ms": 2440.769672393799, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "H.L. Dattu", "latency_ms": 3059.262752532959, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 613}, "2": {"k": 2, "answer": "H.L. Dattu", "latency_ms": 2750.6580352783203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "H.L. Dattu", "latency_ms": 3517.927408218384, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1838}, "4": {"k": 4, "answer": "M. Jagannadha Rao", "latency_ms": 2615.966320037842, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2406}, "5": {"k": 5, "answer": "M. Jagannadha Rao", "latency_ms": 3406.682014465332, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3081}, "6": {"k": 6, "answer": "M. Jagannadha Rao", "latency_ms": 2658.1053733825684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3658}}}
{"qid": "2524852627963543698", "question": "who has won more grand slam titles in tennis", "gold_answers": ["Roger Federer"], "shots": {"0": {"k": 0, "answer": "Margaret Court", "latency_ms": 1627.5551319122314, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1725.6357669830322, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "Nadal", "latency_ms": 1493.8554763793945, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "Nadal", "latency_ms": 1682.755708694458, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "Serena Williams", "latency_ms": 1797.346830368042, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2489}, "5": {"k": 5, "answer": "Serena Williams", "latency_ms": 2128.3233165740967, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3128}, "6": {"k": 6, "answer": "Serena Williams", "latency_ms": 1632.2243213653564, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3728}}}
{"qid": "-6230694167244306035", "question": "how many times has the saints won the super bowl", "gold_answers": ["1"], "shots": {"0": {"k": 0, "answer": "1", "latency_ms": 1748.9428520202637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1", "latency_ms": 630.9309005737305, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 608}, "2": {"k": 2, "answer": "1", "latency_ms": 1691.8776035308838, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1228}, "3": {"k": 3, "answer": "1", "latency_ms": 1473.8314151763916, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1827}, "4": {"k": 4, "answer": "1", "latency_ms": 1341.9184684753418, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2434}, "5": {"k": 5, "answer": "1", "latency_ms": 1475.7676124572754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3017}, "6": {"k": 6, "answer": "1", "latency_ms": 1718.5273170471191, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3624}}}
{"qid": "4186485631454362041", "question": "when was the last time the los angeles lakers won a championship", "gold_answers": ["2010", "in 2010"], "shots": {"0": {"k": 0, "answer": "2020", "latency_ms": 1511.085033416748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "2010", "latency_ms": 1524.3213176727295, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 653}, "2": {"k": 2, "answer": "2010", "latency_ms": 1536.9248390197754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1267}, "3": {"k": 3, "answer": "2010", "latency_ms": 1732.591152191162, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1883}, "4": {"k": 4, "answer": "2010", "latency_ms": 1568.089485168457, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2534}, "5": {"k": 5, "answer": "2010", "latency_ms": 2305.148124694824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3130}, "6": {"k": 6, "answer": "2010", "latency_ms": 1508.1143379211426, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3751}}}
{"qid": "6332740420650663139", "question": "how many gold medals did australia win in the 2000 olympics", "gold_answers": ["16"], "shots": {"0": {"k": 0, "answer": "17", "latency_ms": 1713.9265537261963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2276.1964797973633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 627}, "2": {"k": 2, "answer": "17", "latency_ms": 1484.4753742218018, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1233}, "3": {"k": 3, "answer": "17", "latency_ms": 1220.9646701812744, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1881}, "4": {"k": 4, "answer": "17", "latency_ms": 1201.2689113616943, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2519}, "5": {"k": 5, "answer": "17", "latency_ms": 1462.9881381988525, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3180}, "6": {"k": 6, "answer": "17", "latency_ms": 667.9186820983887, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3831}}}
{"qid": "-2418032234376469176", "question": "where is the eagle creek fire located in oregon", "gold_answers": ["Cascade Locks", "Columbia River Gorge"], "shots": {"0": {"k": 0, "answer": "Columbia River Gorge", "latency_ms": 2771.7573642730713, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2622.09415435791, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 628}, "2": {"k": 2, "answer": "Columbia River Gorge", "latency_ms": 2518.1243419647217, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1254}, "3": {"k": 3, "answer": "Columbia River Gorge", "latency_ms": 1769.9203491210938, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1896}, "4": {"k": 4, "answer": "Columbia River Gorge", "latency_ms": 2939.2457008361816, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2513}, "5": {"k": 5, "answer": "Columbia River Gorge", "latency_ms": 2441.6186809539795, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3144}, "6": {"k": 6, "answer": "Columbia River Gorge", "latency_ms": 2497.0033168792725, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3795}}}
{"qid": "-2622854017829301835", "question": "different ways to spell corey for a boy", "gold_answers": ["Kori", "Cory", "Kory", "Coire", "Corrie", "Correy", "Corie", "Corey", "Khouri"], "shots": {"0": {"k": 0, "answer": "Corey, Cory, Korey, Kory", "latency_ms": 4427.231311798096, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Corey, Cory", "latency_ms": 2399.4646072387695, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 619}, "2": {"k": 2, "answer": "Corey, Cory", "latency_ms": 2645.226001739502, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1255}, "3": {"k": 3, "answer": "Corey, Cory", "latency_ms": 1612.0727062225342, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1857}, "4": {"k": 4, "answer": "Corey, Cory", "latency_ms": 1314.7718906402588, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2430}, "5": {"k": 5, "answer": "Corey, Cory", "latency_ms": 2126.1115074157715, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3038}, "6": {"k": 6, "answer": "Corey, Cory", "latency_ms": 2461.604356765747, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3682}}}
{"qid": "3480908309420822259", "question": "who sang the song should have been a cowboy", "gold_answers": ["Toby Keith"], "shots": {"0": {"k": 0, "answer": "Toby Keith", "latency_ms": 1600.9507179260254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Randy Houser", "latency_ms": 2136.5389823913574, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2178.347110748291, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1206}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2121.8559741973877, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1852}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2705.2316665649414, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2450}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2461.3771438598633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3084}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2437.776803970337, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3751}}}
{"qid": "-4198273171611748168", "question": "how many counties does the state of georgia have", "gold_answers": ["159", "159 counties"], "shots": {"0": {"k": 0, "answer": "159", "latency_ms": 1042.2098636627197, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "159", "latency_ms": 955.7838439941406, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "159", "latency_ms": 1867.931604385376, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1333}, "3": {"k": 3, "answer": "159", "latency_ms": 677.2005558013916, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1974}, "4": {"k": 4, "answer": "159", "latency_ms": 1484.1949939727783, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2617}, "5": {"k": 5, "answer": "159", "latency_ms": 662.3106002807617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3252}, "6": {"k": 6, "answer": "159", "latency_ms": 2273.8659381866455, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3867}}}
{"qid": "2077711785168359308", "question": "who sang picking up pebbles and throwing them into the sea", "gold_answers": ["Matt Flinders"], "shots": {"0": {"k": 0, "answer": "Bobby Vinton", "latency_ms": 2127.638339996338, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2073.2076168060303, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2312.612533569336, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1174}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2063.429832458496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1758}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1889.9857997894287, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2349}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2425.452947616577, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2979}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2402.0373821258545, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3514}}}
{"qid": "9062886943819260445", "question": "who made the most free throws in nba history", "gold_answers": ["Karl Malone"], "shots": {"0": {"k": 0, "answer": "Karl Malone", "latency_ms": 2351.179599761963, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2592.219591140747, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 626}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2663.9084815979004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1219}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2218.5614109039307, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1821}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2725.020170211792, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2415}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2686.617612838745, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3155}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2467.402935028076, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3741}}}
{"qid": "1278533364941960992", "question": "which is the ring finger for male in india", "gold_answers": ["the left ring finger", "left ring finger"], "shots": {"0": {"k": 0, "answer": "Anamika finger", "latency_ms": 1757.0304870605469, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Left hand fourth finger", "latency_ms": 2195.3701972961426, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 552}, "2": {"k": 2, "answer": "Right hand", "latency_ms": 2020.9143161773682, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1137}, "3": {"k": 3, "answer": "Right hand", "latency_ms": 1468.2774543762207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1691}, "4": {"k": 4, "answer": "Right hand", "latency_ms": 1257.725715637207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2293}, "5": {"k": 5, "answer": "Right hand", "latency_ms": 2058.123826980591, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2857}, "6": {"k": 6, "answer": "Right hand", "latency_ms": 1704.8671245574951, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3471}}}
{"qid": "-8788207958970648395", "question": "what disney cartoon character's middle name is fauntleroy", "gold_answers": ["Donald Fauntleroy Duck", "Donald Duck"], "shots": {"0": {"k": 0, "answer": "Donald Fauntleroy Duck", "latency_ms": 2853.0139923095703, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Donald Duck", "latency_ms": 2066.544532775879, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 647}, "2": {"k": 2, "answer": "Donald Duck", "latency_ms": 1268.6758041381836, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1282}, "3": {"k": 3, "answer": "Donald Duck", "latency_ms": 1852.184534072876, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1934}, "4": {"k": 4, "answer": "Donald Duck", "latency_ms": 1539.6180152893066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2561}, "5": {"k": 5, "answer": "Donald Duck", "latency_ms": 1973.2880592346191, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3199}, "6": {"k": 6, "answer": "Donald Duck", "latency_ms": 1787.08815574646, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3868}}}
{"qid": "-4323000711881657078", "question": "who played king theoden in lord of the rings", "gold_answers": ["Bernard Hill"], "shots": {"0": {"k": 0, "answer": "Bernard Mayes", "latency_ms": 2785.338878631592, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2361.0613346099854, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2989.2287254333496, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1238}, "3": {"k": 3, "answer": "Bernard Mayes", "latency_ms": 2715.839147567749, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1860}, "4": {"k": 4, "answer": "Bernard Mayes", "latency_ms": 2932.5811862945557, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2466}, "5": {"k": 5, "answer": "Bernard Mayes", "latency_ms": 2621.5953826904297, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3067}, "6": {"k": 6, "answer": "Bernard Mayes", "latency_ms": 2428.359270095825, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3680}}}
{"qid": "-2664158830468622213", "question": "dark parables return of the salt princess release date", "gold_answers": ["September 2017"], "shots": {"0": {"k": 0, "answer": "March 22, 2018", "latency_ms": 4460.080623626709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2186.624050140381, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 616}, "2": {"k": 2, "answer": "March 15, 2018", "latency_ms": 2627.826452255249, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1191}, "3": {"k": 3, "answer": "March 15, 2018", "latency_ms": 3244.2851066589355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1792}, "4": {"k": 4, "answer": "March 15, 2018", "latency_ms": 4057.739734649658, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2426}, "5": {"k": 5, "answer": "March 15, 2018", "latency_ms": 3880.509614944458, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3068}, "6": {"k": 6, "answer": "March 15, 2018", "latency_ms": 3376.0948181152344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3656}}}
{"qid": "9078092620239351027", "question": "where was the movie 500 days of summer filmed", "gold_answers": ["Los Angeles"], "shots": {"0": {"k": 0, "answer": "Los Angeles", "latency_ms": 1698.5621452331543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1948.2648372650146, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 650}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1887.1433734893799, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1305}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2429.591417312622, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1996}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1905.7753086090088, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2635}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2781.323194503784, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3260}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1827.2340297698975, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3875}}}
{"qid": "4747795168824294053", "question": "who played emma in h2o just add water", "gold_answers": ["Claire Rhiannon Holt"], "shots": {"0": {"k": 0, "answer": "Claire Holt", "latency_ms": 1883.2979202270508, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Claire Holt", "latency_ms": 1721.6150760650635, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1949.737787246704, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1252}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1663.4366512298584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2180.7971000671387, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2564}, "5": {"k": 5, "answer": "Claire Holt", "latency_ms": 2616.8181896209717, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3135}, "6": {"k": 6, "answer": "Claire Holt", "latency_ms": 2080.2605152130127, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3752}}}
{"qid": "-1368633715963532113", "question": "where can carbon be found in the biosphere", "gold_answers": ["other living organisms", "plants", "soil", "The terrestrial biosphere"], "shots": {"0": {"k": 0, "answer": "All living organisms.", "latency_ms": 2500.9231567382812, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "plants, animals, dead plant matter", "latency_ms": 3589.848756790161, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 621}, "2": {"k": 2, "answer": "plants, soils, animals, organisms", "latency_ms": 3959.345817565918, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1271}, "3": {"k": 3, "answer": "plants, animals, soils", "latency_ms": 3457.8144550323486, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1956}, "4": {"k": 4, "answer": "in plants, soils, and organisms", "latency_ms": 3695.646047592163, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 2663}, "5": {"k": 5, "answer": "in plants, soils, and organisms", "latency_ms": 3913.0592346191406, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 3396}, "6": {"k": 6, "answer": "in plants, soils, and organisms", "latency_ms": 3884.896755218506, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 4116}}}
{"qid": "-1173252802989599019", "question": "who is the winner of bigg boss kannada season", "gold_answers": ["rapper Chandan Shetty", "Chandan Shetty"], "shots": {"0": {"k": 0, "answer": "Vijay Raghavendra", "latency_ms": 3590.642213821411, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1905.463457107544, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2628.2196044921875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1187}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 3136.4831924438477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1761}, "4": {"k": 4, "answer": "Chandan Shetty", "latency_ms": 2692.5506591796875, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2373}, "5": {"k": 5, "answer": "Chandan Shetty", "latency_ms": 3410.5703830718994, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2968}, "6": {"k": 6, "answer": "Chandan Shetty", "latency_ms": 2033.778190612793, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3611}}}
{"qid": "-8950878510839541193", "question": "where was the outlaw of josey wales filmed", "gold_answers": ["Utah", "Arizona", "Kanab , Utah", "Lake Powell", "Wyoming", "Oroville , California"], "shots": {"0": {"k": 0, "answer": "Arizona, California, Utah, and Mexico.", "latency_ms": 4861.222505569458, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2121.114730834961, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2906.121015548706, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1247}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 3256.0317516326904, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1866}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 3151.554584503174, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2499}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2765.3088569641113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3126}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3002.072811126709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3744}}}
{"qid": "445108246725869963", "question": "who plays the dad in the new beauty and the beast", "gold_answers": ["Kevin Kline"], "shots": {"0": {"k": 0, "answer": "Kevin Kline", "latency_ms": 1216.939926147461, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2894.146203994751, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2630.1498413085938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1172}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 3127.159357070923, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1831}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1851.4010906219482, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2457}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2691.408634185791, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3100}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2451.294183731079, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3777}}}
{"qid": "-5745040113272333526", "question": "where does the saskatchewan river start and end", "gold_answers": ["the Hudson Bay", "central Saskatchewan", "Saskatchewan River Forks , Saskatchewan", "Saskatchewan Glacier , Alberta"], "shots": {"0": {"k": 0, "answer": "Saskatchewan, Canada to Lake Winnipeg.", "latency_ms": 4297.099351882935, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Canadian Rockies to Hudson Bay", "latency_ms": 2978.1036376953125, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 653}, "2": {"k": 2, "answer": "Rocky Mountains to Lake Winnipeg", "latency_ms": 3786.7681980133057, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1383}, "3": {"k": 3, "answer": "central Saskatchewan to Lake Winnipeg", "latency_ms": 2998.7950325012207, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2093}, "4": {"k": 4, "answer": "Central Saskatchewan to Lake Winnipeg", "latency_ms": 3193.2897567749023, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2737}, "5": {"k": 5, "answer": "Central Saskatchewan to Lake Winnipeg", "latency_ms": 3228.6148071289062, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3342}, "6": {"k": 6, "answer": "Central Saskatchewan to Lake Winnipeg", "latency_ms": 3122.7846145629883, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 4010}}}
{"qid": "2370076891269438265", "question": "where are the spanish steps located in italy", "gold_answers": ["Rome"], "shots": {"0": {"k": 0, "answer": "Rome", "latency_ms": 1332.287073135376, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Rome", "latency_ms": 2026.1330604553223, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 649}, "2": {"k": 2, "answer": "Rome", "latency_ms": 2002.2897720336914, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1262}, "3": {"k": 3, "answer": "Rome", "latency_ms": 1466.4359092712402, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1916}, "4": {"k": 4, "answer": "Rome, Italy", "latency_ms": 3005.1727294921875, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2547}, "5": {"k": 5, "answer": "Rome, Italy", "latency_ms": 2207.4179649353027, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3180}, "6": {"k": 6, "answer": "Rome, Italy", "latency_ms": 2526.6923904418945, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3778}}}
{"qid": "-1701212376306091221", "question": "what does the m number mean on a pint glass", "gold_answers": ["imperial fluid ounces", "the manufacturing company or site"], "shots": {"0": {"k": 0, "answer": "CE Mark or Measure number", "latency_ms": 2988.1062507629395, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "manufacturing company or site", "latency_ms": 2516.5441036224365, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "the manufacturing company or site", "latency_ms": 3217.339277267456, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1285}, "3": {"k": 3, "answer": "manufacturing company or site", "latency_ms": 2204.1988372802734, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1903}, "4": {"k": 4, "answer": "the manufacturing company or site", "latency_ms": 3177.4232387542725, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2507}, "5": {"k": 5, "answer": "the manufacturing company or site", "latency_ms": 3467.262029647827, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3117}, "6": {"k": 6, "answer": "the manufacturing company or site", "latency_ms": 2458.2834243774414, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3724}}}
{"qid": "3631867991857999976", "question": "other than water what else has hydrogen bonds", "gold_answers": ["inorganic molecules such as water"], "shots": {"0": {"k": 0, "answer": "Ammonia, methanol.", "latency_ms": 4659.720659255981, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2650.369644165039, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "Hydrogen fluoride, ammonia, methanol", "latency_ms": 4259.540796279907, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1267}, "3": {"k": 3, "answer": "Hydrogen fluoride, ammonia, methanol, hydrogen sulfide", "latency_ms": 5043.529748916626, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1940}, "4": {"k": 4, "answer": "Hydrogen fluoride, ammonia, methanol, hydrogen sulfide", "latency_ms": 7014.183044433594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2625}, "5": {"k": 5, "answer": "Hydrogen fluoride, ammonia, methanol, hydrogen sulfide", "latency_ms": 5387.616395950317, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3216}, "6": {"k": 6, "answer": "Hydrogen fluoride, ammonia, methanol", "latency_ms": 4409.497499465942, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3820}}}
{"qid": "-1665528830781573810", "question": "when boy meets girl waiting for a star to fall", "gold_answers": ["1988"], "shots": {"0": {"k": 0, "answer": "Once in a Lifetime", "latency_ms": 2626.7175674438477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "April 1989", "latency_ms": 2395.1728343963623, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "1988", "latency_ms": 1998.3408451080322, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1173}, "3": {"k": 3, "answer": "1988", "latency_ms": 1729.6757698059082, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1830}, "4": {"k": 4, "answer": "1988", "latency_ms": 2284.3379974365234, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2433}, "5": {"k": 5, "answer": "1988", "latency_ms": 1696.1030960083008, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3094}, "6": {"k": 6, "answer": "1988", "latency_ms": 2087.613344192505, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3718}}}
{"qid": "4289442278067074619", "question": "trick taking card game name derived from spanish for man", "gold_answers": ["Skat"], "shots": {"0": {"k": 0, "answer": "Hombre", "latency_ms": 1265.3930187225342, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Hombre", "latency_ms": 1586.1058235168457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "Hombre", "latency_ms": 2034.7745418548584, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1234}, "3": {"k": 3, "answer": "Hombre", "latency_ms": 2118.6797618865967, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1835}, "4": {"k": 4, "answer": "Hombre", "latency_ms": 1546.18239402771, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2424}, "5": {"k": 5, "answer": "Hombre", "latency_ms": 2121.1185455322266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3063}, "6": {"k": 6, "answer": "Hombre", "latency_ms": 1316.422462463379, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3628}}}
{"qid": "-7311149600272472027", "question": "what percentage of the population is naturally blonde", "gold_answers": ["2 %"], "shots": {"0": {"k": 0, "answer": "2-6%", "latency_ms": 2764.9013996124268, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Five percent", "latency_ms": 1788.355827331543, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "5%", "latency_ms": 1265.9602165222168, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1317}, "3": {"k": 3, "answer": "2% to 16%", "latency_ms": 2790.8575534820557, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1927}, "4": {"k": 4, "answer": "2% to 16%", "latency_ms": 2798.764705657959, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2556}, "5": {"k": 5, "answer": "2% to 16%", "latency_ms": 3528.03897857666, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3261}, "6": {"k": 6, "answer": "2% to 16%", "latency_ms": 2628.929853439331, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3867}}}
{"qid": "7110705857253017304", "question": "when was the last time miss texas won miss america", "gold_answers": ["1975"], "shots": {"0": {"k": 0, "answer": "1971", "latency_ms": 1288.0213260650635, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2214.5190238952637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 573}, "2": {"k": 2, "answer": "1975", "latency_ms": 2178.5411834716797, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1178}, "3": {"k": 3, "answer": "1975", "latency_ms": 1206.2897682189941, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1756}, "4": {"k": 4, "answer": "1975", "latency_ms": 1244.7261810302734, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2368}, "5": {"k": 5, "answer": "1975", "latency_ms": 1747.4770545959473, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2949}, "6": {"k": 6, "answer": "1975", "latency_ms": 1805.262804031372, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3539}}}
{"qid": "-3653614166480549899", "question": "who sang first line of we are the world", "gold_answers": ["Billy Joel", "Stevie Wonder", "Lionel Richie", "James Ingram", "Tina Turner", "Paul Simon", "Kenny Rogers"], "shots": {"0": {"k": 0, "answer": "Lionel Richie", "latency_ms": 1944.1471099853516, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1656.958818435669, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 605}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2435.8971118927, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1175}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2322.0486640930176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1769}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 3257.335662841797, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2356}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3202.9197216033936, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2941}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2706.752300262451, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3537}}}
{"qid": "-6284503594240226071", "question": "who formed and first came to the colony of maryland", "gold_answers": ["the English"], "shots": {"0": {"k": 0, "answer": "Cecil Calvert, 2nd Baron Baltimore", "latency_ms": 3738.196611404419, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lord Baltimore", "latency_ms": 2308.701992034912, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "Lord Baltimore", "latency_ms": 1206.592082977295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1302}, "3": {"k": 3, "answer": "Lord Baltimore and Leonard Calvert", "latency_ms": 3047.002077102661, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1992}, "4": {"k": 4, "answer": "Lord Baltimore and settlers", "latency_ms": 2502.370595932007, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2603}, "5": {"k": 5, "answer": "Lord Baltimore", "latency_ms": 1548.5436916351318, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3254}, "6": {"k": 6, "answer": "Cecil Calvert, the second Lord Baltimore of England, and his brother Leonard Calvert.", "latency_ms": 8997.03311920166, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3916}}}
{"qid": "6948592513354589991", "question": "who has played in the most masters tournaments", "gold_answers": ["Gary Player"], "shots": {"0": {"k": 0, "answer": "Gary Player and Fred Couples and Gary Player and  Miguel Angel Jimenez and  Bernhard Langer", "latency_ms": 6875.43797492981, "em": 0.0, "f1": 0.2352941176470588, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2755.7458877563477, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 648}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1874.5677471160889, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1298}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2278.4056663513184, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1972}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1943.5021877288818, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2632}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2208.3821296691895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3302}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2888.636827468872, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3997}}}
{"qid": "3311962143974666464", "question": "how much venom can a king cobra produce", "gold_answers": ["200 to 500 mg", "420 mg", "up to 7 ml"], "shots": {"0": {"k": 0, "answer": "400-600 mg", "latency_ms": 2163.583993911743, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2109.0188026428223, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 618}, "2": {"k": 2, "answer": "200 to 500 mg", "latency_ms": 2941.781759262085, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1240}, "3": {"k": 3, "answer": "200 to 500 mg", "latency_ms": 2300.333261489868, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1878}, "4": {"k": 4, "answer": "200 to 500 mg", "latency_ms": 3044.156074523926, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2466}, "5": {"k": 5, "answer": "200 to 500 mg", "latency_ms": 2651.860237121582, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3105}, "6": {"k": 6, "answer": "200 to 500 mg", "latency_ms": 2940.2921199798584, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3765}}}
{"qid": "4829683075369336935", "question": "last team to win fa cup outside top flight", "gold_answers": ["Arsenal", "West Ham United"], "shots": {"0": {"k": 0, "answer": "Wimbledon (1988)", "latency_ms": 3317.903757095337, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "West Ham", "latency_ms": 1533.1618785858154, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 578}, "2": {"k": 2, "answer": "West Ham United", "latency_ms": 2108.3738803863525, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1166}, "3": {"k": 3, "answer": "West Ham United", "latency_ms": 2111.7377281188965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1758}, "4": {"k": 4, "answer": "West Ham United", "latency_ms": 1821.0768699645996, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2356}, "5": {"k": 5, "answer": "West Ham United", "latency_ms": 2663.578510284424, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2944}, "6": {"k": 6, "answer": "West Ham United", "latency_ms": 2150.1245498657227, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3536}}}
{"qid": "947113025953164512", "question": "where did you go to drink during prohibition", "gold_answers": ["Speakeasies"], "shots": {"0": {"k": 0, "answer": "Speakeasy", "latency_ms": 2365.0524616241455, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2156.9883823394775, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 674}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1943.695306777954, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1395}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2110.8503341674805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2113}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2924.373149871826, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2806}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2870.60546875, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3481}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2353.325128555298, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4165}}}
{"qid": "2162587275527723755", "question": "where does the edinburgh fringe festival take place", "gold_answers": ["in Edinburgh , Scotland"], "shots": {"0": {"k": 0, "answer": "Edinburgh, Scotland", "latency_ms": 1828.7055492401123, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Edinburgh, Scotland", "latency_ms": 2480.269432067871, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 694}, "2": {"k": 2, "answer": "Edinburgh, Scotland", "latency_ms": 3012.284755706787, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 1379}, "3": {"k": 3, "answer": "Edinburgh, Scotland", "latency_ms": 1826.4625072479248, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2052}, "4": {"k": 4, "answer": "Edinburgh, Scotland", "latency_ms": 2340.212345123291, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 2728}, "5": {"k": 5, "answer": "Edinburgh, Scotland", "latency_ms": 2496.8626499176025, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 3406}, "6": {"k": 6, "answer": "Edinburgh, Scotland", "latency_ms": 2499.1612434387207, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 4109}}}
{"qid": "1875157261182375834", "question": "who won the wwe championship elimination chamber 2018", "gold_answers": ["Roman Reigns"], "shots": {"0": {"k": 0, "answer": "Roman Reigns", "latency_ms": 1592.1006202697754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3094.9151515960693, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 658}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2217.6287174224854, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1354}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2157.1028232574463, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2028}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2162.6763343811035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2688}, "5": {"k": 5, "answer": "Roman Reigns", "latency_ms": 2373.10528755188, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3374}, "6": {"k": 6, "answer": "Roman Reigns", "latency_ms": 1598.0896949768066, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4068}}}
{"qid": "8926902974073457645", "question": "where does safe haven take place in the book", "gold_answers": ["Southport , North Carolina"], "shots": {"0": {"k": 0, "answer": "Southport, North Carolina", "latency_ms": 2365.6246662139893, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Boston", "latency_ms": 1703.8812637329102, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 601}, "2": {"k": 2, "answer": "Boston", "latency_ms": 756.5159797668457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1206}, "3": {"k": 3, "answer": "Southport, North Carolina", "latency_ms": 3137.5648975372314, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1848}, "4": {"k": 4, "answer": "Southport, North Carolina.", "latency_ms": 2808.877944946289, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2440}, "5": {"k": 5, "answer": "Southport, North Carolina", "latency_ms": 2631.739616394043, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3091}, "6": {"k": 6, "answer": "Southport, North Carolina.", "latency_ms": 3642.4124240875244, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3725}}}
{"qid": "8445703091969850957", "question": "who came up with the term baby boomer", "gold_answers": ["The Washington Post"], "shots": {"0": {"k": 0, "answer": "Landon Jones", "latency_ms": 1613.1861209869385, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2655.84135055542, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2153.519868850708, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1299}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2475.264072418213, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1908}, "4": {"k": 4, "answer": "Sylvia F. Porter", "latency_ms": 1981.4412593841553, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2521}, "5": {"k": 5, "answer": "Sylvia F. Porter", "latency_ms": 3486.1505031585693, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3180}, "6": {"k": 6, "answer": "Sylvia F. Porter", "latency_ms": 3898.7765312194824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3861}}}
{"qid": "-2961860751699827965", "question": "who was elected president of the united states in 1928", "gold_answers": ["Herbert Hoover"], "shots": {"0": {"k": 0, "answer": "Herbert Hoover", "latency_ms": 2713.458776473999, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Herbert Hoover", "latency_ms": 1662.7423763275146, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 669}, "2": {"k": 2, "answer": "Herbert Hoover", "latency_ms": 2125.4827976226807, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1306}, "3": {"k": 3, "answer": "Herbert Hoover", "latency_ms": 1626.0313987731934, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2012}, "4": {"k": 4, "answer": "Herbert Hoover", "latency_ms": 2028.4733772277832, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2728}, "5": {"k": 5, "answer": "Herbert Hoover", "latency_ms": 2281.0769081115723, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3465}, "6": {"k": 6, "answer": "Herbert Hoover", "latency_ms": 2899.4181156158447, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4170}}}
{"qid": "-3997272399784107764", "question": "dogs name in the grinch who stole christmas", "gold_answers": ["Max"], "shots": {"0": {"k": 0, "answer": "Max", "latency_ms": 2517.2319412231445, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Max", "latency_ms": 981.5952777862549, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 643}, "2": {"k": 2, "answer": "Max", "latency_ms": 1735.048770904541, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1285}, "3": {"k": 3, "answer": "Max", "latency_ms": 1747.0862865447998, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1967}, "4": {"k": 4, "answer": "Max", "latency_ms": 1189.1474723815918, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2615}, "5": {"k": 5, "answer": "Max", "latency_ms": 1546.3738441467285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3269}, "6": {"k": 6, "answer": "Max", "latency_ms": 1458.8418006896973, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3871}}}
{"qid": "119543419987074002", "question": "who played anne in anne with an e", "gold_answers": ["Amybeth McNulty"], "shots": {"0": {"k": 0, "answer": "Amybeth McNulty", "latency_ms": 2142.270565032959, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3228.4891605377197, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "Dawn O'Day", "latency_ms": 2168.895959854126, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1212}, "3": {"k": 3, "answer": "Mary Miles Minter", "latency_ms": 2937.591791152954, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1816}, "4": {"k": 4, "answer": "Mary Miles Minter", "latency_ms": 2440.8161640167236, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2412}, "5": {"k": 5, "answer": "Mary Miles Minter", "latency_ms": 2155.96866607666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3019}, "6": {"k": 6, "answer": "Megan Follows", "latency_ms": 2713.042736053467, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3676}}}
{"qid": "4362450141225960365", "question": "what was going on in the world in 1900", "gold_answers": ["Boxer Rebellion", "U.S. population exceeds 75 million", "Galveston hurricane"], "shots": {"0": {"k": 0, "answer": "Boxer Rebellion.", "latency_ms": 3240.328788757324, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Women starting to serve in limited military roles.", "latency_ms": 4819.636583328247, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 608}, "2": {"k": 2, "answer": "No liberal democracy with universal suffrage", "latency_ms": 2735.480308532715, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1250}, "3": {"k": 3, "answer": "No liberal democracy with universal suffrage", "latency_ms": 2824.9902725219727, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1855}, "4": {"k": 4, "answer": "Women starting to serve in limited military roles.", "latency_ms": 4723.560094833374, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2502}, "5": {"k": 5, "answer": "World War I and World War II had not yet started.", "latency_ms": 5697.2856521606445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3133}, "6": {"k": 6, "answer": "World War I and World War II had not yet started.", "latency_ms": 5224.491834640503, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3699}}}
{"qid": "-599106694350296477", "question": "theme of the song roar by katy perry", "gold_answers": ["standing up for oneself", "self - empowerment"], "shots": {"0": {"k": 0, "answer": "Empowerment", "latency_ms": 2387.2766494750977, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "self-empowerment", "latency_ms": 3121.009111404419, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "self-empowerment", "latency_ms": 3303.196668624878, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1283}, "3": {"k": 3, "answer": "self-empowerment", "latency_ms": 2559.077262878418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1865}, "4": {"k": 4, "answer": "self-empowerment", "latency_ms": 3557.543992996216, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2540}, "5": {"k": 5, "answer": "self-empowerment", "latency_ms": 4015.4247283935547, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3165}, "6": {"k": 6, "answer": "self-empowerment", "latency_ms": 3280.202627182007, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3772}}}
{"qid": "6341281025332348318", "question": "who picks the players in the nfl draft", "gold_answers": ["each team"], "shots": {"0": {"k": 0, "answer": "NFL teams' front offices", "latency_ms": 2876.8227100372314, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "National Football League franchises", "latency_ms": 2254.2521953582764, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 576}, "2": {"k": 2, "answer": "National Football League (NFL) franchises", "latency_ms": 3441.654682159424, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1175}, "3": {"k": 3, "answer": "National Football League (NFL) franchises", "latency_ms": 4421.376705169678, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1822}, "4": {"k": 4, "answer": "National Football League (NFL) franchises", "latency_ms": 3410.573720932007, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2440}, "5": {"k": 5, "answer": "National Football League franchises", "latency_ms": 2932.7690601348877, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3074}, "6": {"k": 6, "answer": "National Football League franchises", "latency_ms": 2671.825170516968, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3714}}}
{"qid": "1656636013570720778", "question": "who discovered gold in the sierra nevada of california", "gold_answers": ["James W. Marshall"], "shots": {"0": {"k": 0, "answer": "James W. Marshall", "latency_ms": 1977.2520065307617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2118.241548538208, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 644}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2417.9394245147705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1300}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 3229.877471923828, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1962}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2732.102632522583, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2552}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3372.00927734375, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3138}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2053.762674331665, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3789}}}
{"qid": "6797662554091549033", "question": "which mirror is used in vehicles for rear view", "gold_answers": ["rear - view mirror"], "shots": {"0": {"k": 0, "answer": "Convex Mirror", "latency_ms": 1454.589605331421, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Rear-view mirror", "latency_ms": 2679.4612407684326, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 612}, "2": {"k": 2, "answer": "Rear-view mirror", "latency_ms": 3221.7016220092773, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1233}, "3": {"k": 3, "answer": "Rear-view mirror", "latency_ms": 3210.784912109375, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1897}, "4": {"k": 4, "answer": "Rear-view mirror", "latency_ms": 2070.7385540008545, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2528}, "5": {"k": 5, "answer": "Rear-view mirror", "latency_ms": 2132.9333782196045, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3126}, "6": {"k": 6, "answer": "Rear-view mirror", "latency_ms": 2948.1518268585205, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3768}}}
{"qid": "7788386234127027369", "question": "how many medals did austria win in the 2011 alpine skiing world championships", "gold_answers": ["8"], "shots": {"0": {"k": 0, "answer": "9", "latency_ms": 1501.8916130065918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2994.868755340576, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 579}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2370.222568511963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1243}, "3": {"k": 3, "answer": "5", "latency_ms": 1486.0610961914062, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1859}, "4": {"k": 4, "answer": "All five ski jumping gold medals.", "latency_ms": 3806.8599700927734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2513}, "5": {"k": 5, "answer": "All five ski jumping gold medals.", "latency_ms": 3650.9056091308594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3163}, "6": {"k": 6, "answer": "5", "latency_ms": 2007.1563720703125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3850}}}
{"qid": "647605647914971565", "question": "who wrote lyrics for phantom of the opera", "gold_answers": ["Richard Stilgoe", "Charles Hart and Richard Stilgoe", "Charles Hart"], "shots": {"0": {"k": 0, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 3693.808078765869, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 4309.298992156982, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 634}, "2": {"k": 2, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 3786.163568496704, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1253}, "3": {"k": 3, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 3856.654167175293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1844}, "4": {"k": 4, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 3742.6095008850098, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2508}, "5": {"k": 5, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 3522.6523876190186, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3165}, "6": {"k": 6, "answer": "Charles Hart and Richard Stilgoe", "latency_ms": 3706.5443992614746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3840}}}
{"qid": "-7980433566693172177", "question": "what culture region is germany a part of", "gold_answers": ["Central Germany"], "shots": {"0": {"k": 0, "answer": "Western Europe", "latency_ms": 2127.514362335205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Central Germany", "latency_ms": 1294.088363647461, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 629}, "2": {"k": 2, "answer": "Central Germany", "latency_ms": 2022.7162837982178, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1277}, "3": {"k": 3, "answer": "Central German", "latency_ms": 2370.6741333007812, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1923}, "4": {"k": 4, "answer": "Central European", "latency_ms": 1327.8157711029053, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2568}, "5": {"k": 5, "answer": "Central European", "latency_ms": 2010.495901107788, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3222}, "6": {"k": 6, "answer": "Central European cultures", "latency_ms": 2930.1934242248535, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3933}}}
{"qid": "6268706985209815767", "question": "mount and blade with fire and sword time period", "gold_answers": ["1648 - 51"], "shots": {"0": {"k": 0, "answer": "17th century", "latency_ms": 1429.917335510254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "post-medieval era", "latency_ms": 2372.621774673462, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 659}, "2": {"k": 2, "answer": "post-medieval era", "latency_ms": 3179.177761077881, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1312}, "3": {"k": 3, "answer": "post-medieval era", "latency_ms": 2433.671712875366, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1954}, "4": {"k": 4, "answer": "post-medieval era", "latency_ms": 2377.7172565460205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2631}, "5": {"k": 5, "answer": "post-medieval era", "latency_ms": 2138.9307975769043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3248}, "6": {"k": 6, "answer": "Post-medieval era", "latency_ms": 2350.5890369415283, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3845}}}
