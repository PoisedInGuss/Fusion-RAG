{"qid": "5306300389470118812", "question": "what was the religion in persia before islam", "gold_answers": ["the Zoroastrian religion", "Zoroastrian"], "shots": {"0": {"k": 0, "answer": "Zoroastrianism", "latency_ms": 2497.124195098877, "em": 0.0, "f1": 0.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Zoroastrian", "latency_ms": 2104.158401489258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 637}, "2": {"k": 2, "answer": "Zoroastrian", "latency_ms": 1942.5365924835205, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1302}, "3": {"k": 3, "answer": "Zoroastrian", "latency_ms": 3088.763475418091, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1940}, "4": {"k": 4, "answer": "Zoroastrian", "latency_ms": 2451.3161182403564, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2601}, "5": {"k": 5, "answer": "Zoroastrian", "latency_ms": 1669.764757156372, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3286}, "6": {"k": 6, "answer": "Zoroastrian", "latency_ms": 2496.156692504883, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3917}}}
{"qid": "4492088526674104172", "question": "why does kerosene oil rise up in the wick of lantern", "gold_answers": ["capillary action"], "shots": {"0": {"k": 0, "answer": "Capillary action.", "latency_ms": 2222.308397293091, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1602.4141311645508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 653}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2028.1267166137695, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1270}, "3": {"k": 3, "answer": "Airflow control", "latency_ms": 2315.0792121887207, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1905}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2427.586555480957, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2580}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2495.5501556396484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3248}, "6": {"k": 6, "answer": "Capillary action", "latency_ms": 2249.3929862976074, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3851}}}
{"qid": "-4373262132142058334", "question": "mention the chemical change that proinsulin undergo to be able to act as mature insulin", "gold_answers": ["post translational modification"], "shots": {"0": {"k": 0, "answer": "Proteolytic cleavage.", "latency_ms": 2886.4893913269043, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cleavage of C peptide-A chain bond", "latency_ms": 4023.292303085327, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 606}, "2": {"k": 2, "answer": "Cleavage of C peptide-A chain bond and removal of 35 amino acids.", "latency_ms": 7711.946487426758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1253}, "3": {"k": 3, "answer": "Cleavage of C peptide", "latency_ms": 2489.967107772827, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1886}, "4": {"k": 4, "answer": "Cleavage of C peptide", "latency_ms": 2998.525619506836, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2523}, "5": {"k": 5, "answer": "Cleavage of C peptide", "latency_ms": 2479.421615600586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3227}, "6": {"k": 6, "answer": "Cleavage of C peptide", "latency_ms": 2445.7461833953857, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3776}}}
{"qid": "-5298560410953979569", "question": "where does the sweetness of fruit come from", "gold_answers": ["Fructose , or fruit sugar", "Fructose"], "shots": {"0": {"k": 0, "answer": "Fructose", "latency_ms": 1568.1872367858887, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1960.1340293884277, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 677}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2718.4250354766846, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1341}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2181.032419204712, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1980}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2907.2937965393066, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2589}, "5": {"k": 5, "answer": "Mogrosides", "latency_ms": 2449.2433071136475, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3293}, "6": {"k": 6, "answer": "Mogrosides", "latency_ms": 2445.1022148132324, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3911}}}
{"qid": "4344648368832551136", "question": "an object that moves around an external axis is said to be", "gold_answers": ["orbit", "revolution or orbital revolution"], "shots": {"0": {"k": 0, "answer": "rotating", "latency_ms": 1541.6667461395264, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "revolving", "latency_ms": 1505.8565139770508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 600}, "2": {"k": 2, "answer": "revolving", "latency_ms": 1475.5375385284424, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1169}, "3": {"k": 3, "answer": "revolving", "latency_ms": 1354.0306091308594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1803}, "4": {"k": 4, "answer": "undergoing a revolution or orbital revolution", "latency_ms": 3321.7716217041016, "em": 0.0, "f1": 0.888888888888889, "containment": 1.0, "context_chars": 2409}, "5": {"k": 5, "answer": "undergoing a revolution or orbital revolution", "latency_ms": 3169.0595149993896, "em": 0.0, "f1": 0.888888888888889, "containment": 1.0, "context_chars": 3088}, "6": {"k": 6, "answer": "undergoing a revolution or orbital revolution", "latency_ms": 3335.627317428589, "em": 0.0, "f1": 0.888888888888889, "containment": 1.0, "context_chars": 3748}}}
{"qid": "-724426540432437711", "question": "what was the name of the plant on little shop of horrors", "gold_answers": ["Audrey II"], "shots": {"0": {"k": 0, "answer": "Audrey II", "latency_ms": 1569.509506225586, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Audrey II", "latency_ms": 1328.5064697265625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 615}, "2": {"k": 2, "answer": "Audrey II", "latency_ms": 2124.22513961792, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1244}, "3": {"k": 3, "answer": "Audrey II", "latency_ms": 1341.8018817901611, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1852}, "4": {"k": 4, "answer": "Audrey II", "latency_ms": 2397.568702697754, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2522}, "5": {"k": 5, "answer": "Audrey II", "latency_ms": 2172.363519668579, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3124}, "6": {"k": 6, "answer": "Audrey II", "latency_ms": 1752.0856857299805, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3773}}}
{"qid": "-5541610970112826260", "question": "who signed the declaration of independence for maryland", "gold_answers": ["Samuel Chase", "Charles Carroll of Carrollton", "Thomas Stone", "William Paca"], "shots": {"0": {"k": 0, "answer": "Samuel Chase, William Paca, Thomas Stone, and Charles Carroll.", "latency_ms": 5662.687540054321, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Samuel Chase", "latency_ms": 2065.4852390289307, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 690}, "2": {"k": 2, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll", "latency_ms": 5158.235549926758, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1365}, "3": {"k": 3, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll of Carrollton", "latency_ms": 6421.619176864624, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2050}, "4": {"k": 4, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll of Carrollton", "latency_ms": 6609.019041061401, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2704}, "5": {"k": 5, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll of Carrollton", "latency_ms": 6186.389923095703, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3324}, "6": {"k": 6, "answer": "Samuel Chase, William Paca, Thomas Stone, Charles Carroll of Carrollton", "latency_ms": 7060.669183731079, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3940}}}
{"qid": "-86997217831791339", "question": "when does wonder woman come out on blue ray", "gold_answers": ["September 19 , 2017"], "shots": {"0": {"k": 0, "answer": "August 29, 2017", "latency_ms": 3562.579870223999, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 19, 2017", "latency_ms": 4303.527355194092, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 606}, "2": {"k": 2, "answer": "September 19, 2017", "latency_ms": 3365.549325942993, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1206}, "3": {"k": 3, "answer": "September 19, 2017", "latency_ms": 4181.702375411987, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1870}, "4": {"k": 4, "answer": "September 19, 2017", "latency_ms": 3966.8338298797607, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2458}, "5": {"k": 5, "answer": "September 19, 2017", "latency_ms": 3660.226345062256, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3085}, "6": {"k": 6, "answer": "September 19, 2017", "latency_ms": 2419.697046279907, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3739}}}
{"qid": "-8062812506672279762", "question": "what nba team did derrick rose play for", "gold_answers": ["New York Knicks", "Minnesota Timberwolves", "Chicago Bulls", "Cleveland Cavaliers"], "shots": {"0": {"k": 0, "answer": "Chicago Bulls, Cleveland Cavaliers, Minnesota Timberwolves, Detroit Pistons, New York Knicks.", "latency_ms": 8314.096927642822, "em": 0.0, "f1": 0.42857142857142855, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Chicago Bulls, Minnesota Timberwolves", "latency_ms": 4143.825054168701, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 630}, "2": {"k": 2, "answer": "Minnesota Timberwolves, Chicago Bulls, Cleveland Cavaliers, Boston Celtics", "latency_ms": 4626.519203186035, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1238}, "3": {"k": 3, "answer": "Minnesota Timberwolves, Chicago Bulls, Cleveland Cavaliers, Boston Celtics", "latency_ms": 6565.2501583099365, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1871}, "4": {"k": 4, "answer": "Minnesota Timberwolves, Chicago Bulls, Cleveland Cavaliers, Boston Celtics", "latency_ms": 4335.299491882324, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2467}, "5": {"k": 5, "answer": "Minnesota Timberwolves, Chicago Bulls, Cleveland Cavaliers, New York Knicks", "latency_ms": 5170.551061630249, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3045}, "6": {"k": 6, "answer": "Minnesota Timberwolves, Chicago Bulls, Cleveland Cavaliers, New York Knicks", "latency_ms": 5577.34227180481, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3615}}}
{"qid": "7933002036740390435", "question": "where did the term liberal arts come from", "gold_answers": ["the Roman Empire"], "shots": {"0": {"k": 0, "answer": "Ancient Rome", "latency_ms": 2048.654079437256, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Roman Empire", "latency_ms": 1600.1403331756592, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Roman Empire", "latency_ms": 1195.5509185791016, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1259}, "3": {"k": 3, "answer": "Roman Empire", "latency_ms": 1576.4803886413574, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1921}, "4": {"k": 4, "answer": "Roman Empire", "latency_ms": 1800.436019897461, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2579}, "5": {"k": 5, "answer": "Roman Empire", "latency_ms": 1220.402717590332, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3313}, "6": {"k": 6, "answer": "Roman Empire", "latency_ms": 765.6590938568115, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 4008}}}
{"qid": "-2991400700385487821", "question": "who heads the executive department of west virginia government", "gold_answers": ["the Governor of West Virginia", "the governor of West Virginia", "Jim Justice"], "shots": {"0": {"k": 0, "answer": "The Governor", "latency_ms": 2236.7420196533203, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "The governor", "latency_ms": 1483.5269451141357, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "The Governor of West Virginia", "latency_ms": 2599.2355346679688, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1309}, "3": {"k": 3, "answer": "The Governor of West Virginia", "latency_ms": 2785.726547241211, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2022}, "4": {"k": 4, "answer": "The Governor of West Virginia", "latency_ms": 2451.0579109191895, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2703}, "5": {"k": 5, "answer": "The Governor of West Virginia", "latency_ms": 2963.595151901245, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3380}, "6": {"k": 6, "answer": "The Governor of West Virginia", "latency_ms": 2276.210069656372, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3982}}}
{"qid": "-9186689755642837558", "question": "how long is the bridge between new brunswick and prince edward island", "gold_answers": ["12.9 - kilometre"], "shots": {"0": {"k": 0, "answer": "13 kilometers", "latency_ms": 1804.5670986175537, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2359.0049743652344, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1696.6559886932373, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1321}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2880.469560623169, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2009}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2749.5205402374268, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2732}, "5": {"k": 5, "answer": "13 kilometres", "latency_ms": 1755.892276763916, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3398}, "6": {"k": 6, "answer": "13 kilometres", "latency_ms": 1045.1955795288086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4073}}}
{"qid": "8029901619424338449", "question": "when did the uk and us become allies", "gold_answers": ["1940", "Since 1940"], "shots": {"0": {"k": 0, "answer": "1778, then 1814, and fully in 1941", "latency_ms": 6594.05779838562, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "After the end of the Second World War in 1945", "latency_ms": 5042.880296707153, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 583}, "2": {"k": 2, "answer": "After the end of the Second World War in 1945", "latency_ms": 5815.772533416748, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1205}, "3": {"k": 3, "answer": "After the end of the Second World War in 1945", "latency_ms": 4857.158660888672, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1821}, "4": {"k": 4, "answer": "After 1945", "latency_ms": 2397.4764347076416, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2459}, "5": {"k": 5, "answer": "After 1945", "latency_ms": 1899.1351127624512, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3057}, "6": {"k": 6, "answer": "After 1945", "latency_ms": 2416.4111614227295, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3696}}}
{"qid": "4828649525820722736", "question": "who sang the theme song for the man with the golden gun", "gold_answers": ["Lulu"], "shots": {"0": {"k": 0, "answer": "Lulu", "latency_ms": 2066.0524368286133, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lulu", "latency_ms": 1763.3469104766846, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 582}, "2": {"k": 2, "answer": "Lulu", "latency_ms": 1220.3035354614258, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1225}, "3": {"k": 3, "answer": "Lulu", "latency_ms": 2292.9277420043945, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1838}, "4": {"k": 4, "answer": "Lulu", "latency_ms": 1477.00834274292, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2431}, "5": {"k": 5, "answer": "Lulu", "latency_ms": 1857.874870300293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3016}, "6": {"k": 6, "answer": "Lulu", "latency_ms": 1069.5583820343018, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3673}}}
{"qid": "-7924663090486742985", "question": "american horror story freak show girl kidnapped by clown", "gold_answers": ["Bonnie Lipton"], "shots": {"0": {"k": 0, "answer": "Pennywise is not correct, the answer is: Twisty.", "latency_ms": 6446.036338806152, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Twisty", "latency_ms": 1735.2168560028076, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "Ma Petite", "latency_ms": 2057.8088760375977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1288}, "3": {"k": 3, "answer": "Dot Tattler", "latency_ms": 2723.829984664917, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1987}, "4": {"k": 4, "answer": "Dot Tattler", "latency_ms": 3229.368209838867, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2681}, "5": {"k": 5, "answer": "Dot Tattler", "latency_ms": 2414.3946170806885, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3340}, "6": {"k": 6, "answer": "Dot Tattler", "latency_ms": 2538.6433601379395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3919}}}
{"qid": "-360675773049242516", "question": "the probability of making a type i error when retaining ho at the .05 level of significance is", "gold_answers": ["5 %"], "shots": {"0": {"k": 0, "answer": "0.05", "latency_ms": 1610.1343631744385, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "0.05", "latency_ms": 1847.8131294250488, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 575}, "2": {"k": 2, "answer": "0.05", "latency_ms": 2009.1698169708252, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1143}, "3": {"k": 3, "answer": "0.05", "latency_ms": 2469.398260116577, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1778}, "4": {"k": 4, "answer": "0.05", "latency_ms": 2359.2135906219482, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2423}, "5": {"k": 5, "answer": "0", "latency_ms": 433.98046493530273, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3015}, "6": {"k": 6, "answer": "0.05", "latency_ms": 2668.0171489715576, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3591}}}
{"qid": "5819386267283467034", "question": "what year did the us hockey team won the olympics", "gold_answers": ["1960", "1980", "1960 and 1980"], "shots": {"0": {"k": 0, "answer": "1980", "latency_ms": 1556.92458152771, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1959.3188762664795, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 674}, "2": {"k": 2, "answer": "1980", "latency_ms": 1826.6191482543945, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1306}, "3": {"k": 3, "answer": "1980", "latency_ms": 1818.3395862579346, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2025}, "4": {"k": 4, "answer": "1960, 1980", "latency_ms": 2506.953001022339, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 2686}, "5": {"k": 5, "answer": "1960, 1980", "latency_ms": 3271.4598178863525, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 3365}, "6": {"k": 6, "answer": "1960, 1980", "latency_ms": 3028.559684753418, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 4027}}}
{"qid": "3916394566028297311", "question": "who wrote it's a long long way to pasadena", "gold_answers": ["Harry Vanda", "John Young", "David Hemmings", "George Young"], "shots": {"0": {"k": 0, "answer": "Albert Von Tilzer and Edgar Leslie", "latency_ms": 4001.584768295288, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2384.3307495117188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 604}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1928.591251373291, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1970.0603485107422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1857}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2716.9418334960938, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2492}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2461.3447189331055, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3146}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2638.108968734741, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3781}}}
{"qid": "-2432292250757146771", "question": "who wrote the song i hate you i love you", "gold_answers": ["Angel Benitez", "Olivia O'Brien", "Garrett Nash"], "shots": {"0": {"k": 0, "answer": "Gnash and Olivia O'Brien", "latency_ms": 3830.643653869629, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ne-Yo and StarGate", "latency_ms": 2883.230209350586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "Ne-Yo and StarGate", "latency_ms": 2527.50825881958, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2630.4612159729004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1810}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2420.513391494751, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2451}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3095.2632427215576, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3046}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2731.0686111450195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3704}}}
{"qid": "2393389448844632496", "question": "what is loihi and where does it originate from", "gold_answers": ["from the Hawaii hotspot"], "shots": {"0": {"k": 0, "answer": "Loihi, Hawaii, USA", "latency_ms": 2855.9303283691406, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lodhi, Uttar Pradesh", "latency_ms": 2356.882333755493, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 639}, "2": {"k": 2, "answer": "Lōihi Seamount, Hawaii hotspot", "latency_ms": 4259.995937347412, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1310}, "3": {"k": 3, "answer": "Lōihi Seamount, Hawaii", "latency_ms": 3862.8478050231934, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1902}, "4": {"k": 4, "answer": "Hawaiian seamount", "latency_ms": 3282.134532928467, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2627}, "5": {"k": 5, "answer": "Lōihi Seamount, Hawaii", "latency_ms": 3471.7977046966553, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3287}, "6": {"k": 6, "answer": "Hawaiian seamount", "latency_ms": 3070.737838745117, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3919}}}
{"qid": "-6677440871585683891", "question": "who rebuilt the temple after the babylonian captivity", "gold_answers": ["Herod"], "shots": {"0": {"k": 0, "answer": "Zerubbabel", "latency_ms": 1958.8587284088135, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3035.8567237854004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "The Israelites", "latency_ms": 2352.2818088531494, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "Cyrus", "latency_ms": 1748.1787204742432, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1823}, "4": {"k": 4, "answer": "The Israelites", "latency_ms": 2348.6433029174805, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2493}, "5": {"k": 5, "answer": "The Israelites", "latency_ms": 2036.3185405731201, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3112}, "6": {"k": 6, "answer": "The Israelites", "latency_ms": 2596.3637828826904, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3710}}}
{"qid": "-6750750235985613277", "question": "who was executed for being an american spy during the revolutionary war", "gold_answers": ["Nathan Hale"], "shots": {"0": {"k": 0, "answer": "Nathan Hale", "latency_ms": 1935.2047443389893, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Nathan Hale", "latency_ms": 1833.7044715881348, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 593}, "2": {"k": 2, "answer": "Nathan Hale", "latency_ms": 1564.1911029815674, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1271}, "3": {"k": 3, "answer": "Nathan Hale", "latency_ms": 1513.1068229675293, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1870}, "4": {"k": 4, "answer": "Nathan Hale", "latency_ms": 2226.66072845459, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2542}, "5": {"k": 5, "answer": "Nathan Hale", "latency_ms": 1806.2512874603271, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3236}, "6": {"k": 6, "answer": "Nathan Hale", "latency_ms": 2051.190137863159, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3859}}}
{"qid": "8770209312170080158", "question": "who sang the song suddenly with olivia newton john", "gold_answers": ["Cliff Richard"], "shots": {"0": {"k": 0, "answer": "Cliff Richard", "latency_ms": 1324.803352355957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Cliff Richard", "latency_ms": 2311.0415935516357, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 622}, "2": {"k": 2, "answer": "Cliff Richard", "latency_ms": 1636.6550922393799, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1270}, "3": {"k": 3, "answer": "Cliff Richard", "latency_ms": 2114.2115592956543, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1899}, "4": {"k": 4, "answer": "Cliff Richard", "latency_ms": 1822.9444026947021, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2553}, "5": {"k": 5, "answer": "Cliff Richard", "latency_ms": 1384.3610286712646, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3206}, "6": {"k": 6, "answer": "Cliff Richard", "latency_ms": 1163.7604236602783, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3893}}}
{"qid": "-180549795106449014", "question": "who is command sergeant major of the army", "gold_answers": ["Daniel A. Dailey"], "shots": {"0": {"k": 0, "answer": "Michael A. Grinston", "latency_ms": 2258.7387561798096, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2902.756690979004, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 669}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2694.6933269500732, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1316}, "3": {"k": 3, "answer": "Jon R. Stanley", "latency_ms": 2477.3993492126465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2128}, "4": {"k": 4, "answer": "Raymond F. Chandler III", "latency_ms": 2822.808265686035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2750}, "5": {"k": 5, "answer": "Raymond F. Chandler III", "latency_ms": 3366.91951751709, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3427}, "6": {"k": 6, "answer": "Raymond F. Chandler III", "latency_ms": 2900.8097648620605, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4034}}}
{"qid": "7815138213728640354", "question": "who sings the original windmills of your mind", "gold_answers": ["Noel Harrison"], "shots": {"0": {"k": 0, "answer": "Noel Harrison", "latency_ms": 1598.7122058868408, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Noel Harrison", "latency_ms": 2634.7975730895996, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 603}, "2": {"k": 2, "answer": "No singer mentioned", "latency_ms": 1924.5848655700684, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1237}, "3": {"k": 3, "answer": "Noel Harrison", "latency_ms": 1887.2156143188477, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1918}, "4": {"k": 4, "answer": "Noel Harrison", "latency_ms": 1550.1232147216797, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2554}, "5": {"k": 5, "answer": "Noel Harrison", "latency_ms": 1597.5100994110107, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3206}, "6": {"k": 6, "answer": "Noel Harrison", "latency_ms": 2137.392520904541, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3795}}}
{"qid": "2098168902147822379", "question": "where will the next summer and winter olympics be held", "gold_answers": ["Beijing", "Tokyo"], "shots": {"0": {"k": 0, "answer": "Paris 2024 and Milan-Cortina 2026.", "latency_ms": 4971.239328384399, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "East Asia", "latency_ms": 1801.0048866271973, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 669}, "2": {"k": 2, "answer": "Tokyo, Japan and Beijing, China", "latency_ms": 2494.711399078369, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1294}, "3": {"k": 3, "answer": "Tokyo, Japan and Beijing, China", "latency_ms": 3477.027177810669, "em": 0.0, "f1": 0.33333333333333337, "containment": 1.0, "context_chars": 1887}, "4": {"k": 4, "answer": "Los Angeles, Beijing, Tokyo", "latency_ms": 2794.220209121704, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2520}, "5": {"k": 5, "answer": "Los Angeles, Beijing, Tokyo", "latency_ms": 3032.2425365448, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 3118}, "6": {"k": 6, "answer": "Los Angeles, Beijing", "latency_ms": 1847.3505973815918, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3779}}}
{"qid": "-1458476052377668874", "question": "when did ole miss beat alabama in football", "gold_answers": ["September 19 , 2015", "October 3 , 1970", "October 27 , 1894", "October 13 , 2001", "October 5 , 1968", "October 4 , 2014", "September 11 , 1976", "October 8 , 1988", "October 18 , 2003", "November 5 , 1910"], "shots": {"0": {"k": 0, "answer": "October 10, 2014", "latency_ms": 3422.7120876312256, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "1988", "latency_ms": 1525.8336067199707, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 588}, "2": {"k": 2, "answer": "1988", "latency_ms": 1005.0582885742188, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1229}, "3": {"k": 3, "answer": "1988, 2014", "latency_ms": 3221.6556072235107, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1864}, "4": {"k": 4, "answer": "1988, 2014, 2003", "latency_ms": 5285.206079483032, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 2466}, "5": {"k": 5, "answer": "1988, 2014, 1968, 1910", "latency_ms": 5729.17366027832, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3094}, "6": {"k": 6, "answer": "1910, 1988, 2014", "latency_ms": 4238.759279251099, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 3713}}}
{"qid": "-6861734002351236912", "question": "who sang the song how far is heaven", "gold_answers": ["Los Lonely Boys"], "shots": {"0": {"k": 0, "answer": "Los Lonely Boys", "latency_ms": 1897.5589275360107, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Justin Moore", "latency_ms": 1573.8201141357422, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 587}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2448.052167892456, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1213}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2659.1687202453613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1863}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2462.806224822998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2474}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1996.0052967071533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3142}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1634.4165802001953, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3759}}}
{"qid": "-1722216190521937227", "question": "where are the organ mountains in new mexico", "gold_answers": ["southern New Mexico"], "shots": {"0": {"k": 0, "answer": "Las Cruces", "latency_ms": 1843.665599822998, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "southern New Mexico", "latency_ms": 2095.4947471618652, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 606}, "2": {"k": 2, "answer": "southern New Mexico", "latency_ms": 2007.7497959136963, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1325}, "3": {"k": 3, "answer": "southern New Mexico", "latency_ms": 2103.195905685425, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1986}, "4": {"k": 4, "answer": "southern New Mexico", "latency_ms": 2662.452220916748, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2607}, "5": {"k": 5, "answer": "southern New Mexico", "latency_ms": 1941.5171146392822, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3264}, "6": {"k": 6, "answer": "southern New Mexico", "latency_ms": 1920.3450679779053, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3960}}}
{"qid": "-2717119588239727064", "question": "where are the 10 plagues found in the bible", "gold_answers": ["Book of Exodus", "Exodus"], "shots": {"0": {"k": 0, "answer": "Exodus 7-12", "latency_ms": 2920.933723449707, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Book of Exodus", "latency_ms": 1542.5896644592285, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 645}, "2": {"k": 2, "answer": "Book of Exodus", "latency_ms": 1345.698595046997, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1279}, "3": {"k": 3, "answer": "Book of Exodus", "latency_ms": 1757.664442062378, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1887}, "4": {"k": 4, "answer": "Book of Exodus", "latency_ms": 1933.2449436187744, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2459}, "5": {"k": 5, "answer": "Book of Exodus", "latency_ms": 1823.0395317077637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3127}, "6": {"k": 6, "answer": "Book of Exodus", "latency_ms": 2329.928159713745, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3814}}}
{"qid": "2988284357714596500", "question": "where are red blood cells made in adults", "gold_answers": ["in the bone marrow"], "shots": {"0": {"k": 0, "answer": "Bone marrow", "latency_ms": 1772.5958824157715, "em": 0.0, "f1": 0.8, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "red bone marrow", "latency_ms": 2110.4817390441895, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 622}, "2": {"k": 2, "answer": "the red bone marrow", "latency_ms": 2618.4985637664795, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1295}, "3": {"k": 3, "answer": "the red bone marrow", "latency_ms": 3130.4421424865723, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1911}, "4": {"k": 4, "answer": "the larger bones", "latency_ms": 1346.1120128631592, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2609}, "5": {"k": 5, "answer": "the red bone marrow", "latency_ms": 2181.0462474823, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3259}, "6": {"k": 6, "answer": "the larger bones", "latency_ms": 1777.0884037017822, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3901}}}
{"qid": "-4616596799374362422", "question": "who was the pinkerton detective agency's first female detective", "gold_answers": ["Kate Warne"], "shots": {"0": {"k": 0, "answer": "Kate Warne", "latency_ms": 1084.245204925537, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kate Warne", "latency_ms": 1589.6317958831787, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 609}, "2": {"k": 2, "answer": "Kate Warne", "latency_ms": 2104.9580574035645, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1235}, "3": {"k": 3, "answer": "Kate Warne", "latency_ms": 2342.707633972168, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1918}, "4": {"k": 4, "answer": "Kate Warne", "latency_ms": 2445.678472518921, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2543}, "5": {"k": 5, "answer": "Kate Warne", "latency_ms": 1355.2098274230957, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3232}, "6": {"k": 6, "answer": "Kate Warne", "latency_ms": 1634.5570087432861, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3910}}}
{"qid": "-3650291155113659146", "question": "how many episodes are there in modern family", "gold_answers": ["232", "201"], "shots": {"0": {"k": 0, "answer": "250 episodes", "latency_ms": 1275.463581085205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "13", "latency_ms": 928.9627075195312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 685}, "2": {"k": 2, "answer": "24 episodes", "latency_ms": 1293.5917377471924, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1325}, "3": {"k": 3, "answer": "24 episodes", "latency_ms": 1023.669958114624, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1970}, "4": {"k": 4, "answer": "24 episodes", "latency_ms": 2318.0127143859863, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2641}, "5": {"k": 5, "answer": "24 episodes", "latency_ms": 716.1078453063965, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3268}, "6": {"k": 6, "answer": "24 episodes", "latency_ms": 2196.620225906372, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3917}}}
{"qid": "3381924381590631417", "question": "who built the first temple for god in jerusalem", "gold_answers": ["Solomon"], "shots": {"0": {"k": 0, "answer": "King Solomon", "latency_ms": 1772.6080417633057, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "King Solomon", "latency_ms": 1257.6093673706055, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 643}, "2": {"k": 2, "answer": "King Solomon", "latency_ms": 1292.7029132843018, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1278}, "3": {"k": 3, "answer": "King Solomon", "latency_ms": 2296.4067459106445, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1868}, "4": {"k": 4, "answer": "King Solomon", "latency_ms": 1211.4784717559814, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 2469}, "5": {"k": 5, "answer": "King Solomon", "latency_ms": 1309.239387512207, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3095}, "6": {"k": 6, "answer": "King Solomon", "latency_ms": 2596.1592197418213, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 3758}}}
{"qid": "-154783694579651082", "question": "what is a dropped pin on google maps for", "gold_answers": ["marks locations", "marks locations in Google Maps"], "shots": {"0": {"k": 0, "answer": "Saving a location.", "latency_ms": 922.8413105010986, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "marking locations", "latency_ms": 2815.1352405548096, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 668}, "2": {"k": 2, "answer": "to mark a point on a map", "latency_ms": 2795.4418659210205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1244}, "3": {"k": 3, "answer": "to mark a point on a map", "latency_ms": 3590.8703804016113, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1838}, "4": {"k": 4, "answer": "to mark a point on a map", "latency_ms": 2834.4523906707764, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2466}, "5": {"k": 5, "answer": "to mark a point on a map", "latency_ms": 2757.6375007629395, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3073}, "6": {"k": 6, "answer": "to mark a point on a map", "latency_ms": 2623.5129833221436, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3668}}}
{"qid": "6915606477668963399", "question": "what do the 3 dots mean in math", "gold_answers": ["the therefore sign", "therefore sign"], "shots": {"0": {"k": 0, "answer": "Ellipsis", "latency_ms": 997.9205131530762, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Therefore", "latency_ms": 638.0782127380371, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 649}, "2": {"k": 2, "answer": "Therefore", "latency_ms": 1531.2561988830566, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1298}, "3": {"k": 3, "answer": "Therefore", "latency_ms": 1476.3517379760742, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1896}, "4": {"k": 4, "answer": "Therefore", "latency_ms": 1183.039903640747, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2592}, "5": {"k": 5, "answer": "Therefore", "latency_ms": 1719.7999954223633, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3251}, "6": {"k": 6, "answer": "Therefore", "latency_ms": 684.3159198760986, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3868}}}
{"qid": "-8366545547296627039", "question": "who wrote the song photograph by ringo starr", "gold_answers": ["Richard Starkey", "George Harrison", "Ringo Starr"], "shots": {"0": {"k": 0, "answer": "Ringo Starr and George Harrison.", "latency_ms": 3697.744131088257, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Ringo Starr and George Harrison", "latency_ms": 3827.8865814208984, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 671}, "2": {"k": 2, "answer": "Ringo Starr and George Harrison", "latency_ms": 3129.5924186706543, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1329}, "3": {"k": 3, "answer": "Ringo Starr and George Harrison", "latency_ms": 2778.211832046509, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1957}, "4": {"k": 4, "answer": "Ringo Starr and George Harrison", "latency_ms": 2866.375207901001, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 2613}, "5": {"k": 5, "answer": "Ringo Starr and George Harrison", "latency_ms": 3146.717309951782, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3308}, "6": {"k": 6, "answer": "Ringo Starr and George Harrison", "latency_ms": 3581.007719039917, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3963}}}
{"qid": "-5004457603684974952", "question": "who is playing the halftime show at super bowl 2016", "gold_answers": ["Beyoncé", "Coldplay", "Bruno Mars"], "shots": {"0": {"k": 0, "answer": "Coldplay", "latency_ms": 821.8410015106201, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Coldplay, Beyoncé and Bruno Mars", "latency_ms": 3950.032949447632, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 634}, "2": {"k": 2, "answer": "Coldplay", "latency_ms": 1874.934196472168, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1271}, "3": {"k": 3, "answer": "Coldplay", "latency_ms": 1345.4384803771973, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1924}, "4": {"k": 4, "answer": "Coldplay", "latency_ms": 2086.082935333252, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2560}, "5": {"k": 5, "answer": "Coldplay, Beyoncé, and Bruno Mars.", "latency_ms": 6127.0692348480225, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 3222}, "6": {"k": 6, "answer": "Coldplay", "latency_ms": 1769.8652744293213, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3882}}}
{"qid": "7420920477821898435", "question": "to aru kagaku no railgun s episode 3", "gold_answers": ["Project Radio Noise"], "shots": {"0": {"k": 0, "answer": "Ability and Power", "latency_ms": 1707.7040672302246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2316.708564758301, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 599}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2632.5719356536865, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1204}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 3243.1957721710205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1846}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2414.5774841308594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2510}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2103.626251220703, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3130}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2592.9691791534424, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3766}}}
{"qid": "7478795216476346339", "question": "who won the 2017 sports personality of the year", "gold_answers": ["Mo Farah"], "shots": {"0": {"k": 0, "answer": "Mo Farah", "latency_ms": 1713.7699127197266, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2100.440263748169, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 667}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2222.459316253662, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2222.9201793670654, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1976}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1940.6929016113281, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2587}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 3371.7446327209473, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3252}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1478.698968887329, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3891}}}
{"qid": "7217222058435937287", "question": "where was the world economic forum held this year", "gold_answers": ["Davos"], "shots": {"0": {"k": 0, "answer": "Davos, Switzerland", "latency_ms": 2715.3642177581787, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Dubai", "latency_ms": 1529.2043685913086, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 689}, "2": {"k": 2, "answer": "Dubai", "latency_ms": 1478.7986278533936, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1360}, "3": {"k": 3, "answer": "Dubai", "latency_ms": 2096.796989440918, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2024}, "4": {"k": 4, "answer": "Dubai", "latency_ms": 1236.2654209136963, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2725}, "5": {"k": 5, "answer": "Dubai", "latency_ms": 1537.6031398773193, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3424}, "6": {"k": 6, "answer": "Dubai", "latency_ms": 1766.7686939239502, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4103}}}
{"qid": "-143054837169120955", "question": "where are the giant redwoods located in california", "gold_answers": ["the coast of northern California", "Del Norte County", "Humboldt County"], "shots": {"0": {"k": 0, "answer": "Northern California.", "latency_ms": 1558.0658912658691, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 4284.921884536743, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 633}, "2": {"k": 2, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 5520.925521850586, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1276}, "3": {"k": 3, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 3593.644618988037, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "Kings Canyon National Park", "latency_ms": 3000.774621963501, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2536}, "5": {"k": 5, "answer": "Kings Canyon National Park and Giant Sequoia National Monument", "latency_ms": 4974.703550338745, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3192}, "6": {"k": 6, "answer": "Sierra Nevada", "latency_ms": 2946.4821815490723, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3801}}}
{"qid": "4892429640540595424", "question": "who has made the most premier league appearances", "gold_answers": ["Gareth Barry"], "shots": {"0": {"k": 0, "answer": "Gareth Barry", "latency_ms": 1590.9817218780518, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2301.9859790802, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2178.4653663635254, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1286}, "3": {"k": 3, "answer": "Paul Scholes", "latency_ms": 1443.727731704712, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1867}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2918.6058044433594, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2516}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2398.350238800049, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3116}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2670.5291271209717, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3770}}}
{"qid": "5533906981191706877", "question": "who was the first chief minister of west bengal", "gold_answers": ["Prafulla Chandra Ghosh"], "shots": {"0": {"k": 0, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3797.9824542999268, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3178.9047718048096, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 693}, "2": {"k": 2, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3669.9740886688232, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1408}, "3": {"k": 3, "answer": "Prafulla Chandra Ghosh", "latency_ms": 4569.446086883545, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2035}, "4": {"k": 4, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3278.9814472198486, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2764}, "5": {"k": 5, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3417.0892238616943, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3366}, "6": {"k": 6, "answer": "Prafulla Chandra Ghosh", "latency_ms": 3922.133684158325, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3984}}}
{"qid": "8823076125131408628", "question": "who sings got my mind set on you", "gold_answers": ["George Harrison", "James Ray"], "shots": {"0": {"k": 0, "answer": "George Harrison", "latency_ms": 1012.5491619110107, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "James Ray and George Harrison", "latency_ms": 3003.735065460205, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 541}, "2": {"k": 2, "answer": "James Ray and George Harrison", "latency_ms": 2242.3441410064697, "em": 0.0, "f1": 0.5714285714285715, "containment": 1.0, "context_chars": 1101}, "3": {"k": 3, "answer": "James Ray, George Harrison, Shakin' Stevens, Lee Matthews", "latency_ms": 5949.057579040527, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1688}, "4": {"k": 4, "answer": "James Ray, George Harrison, Shakin' Stevens, Lee Matthews", "latency_ms": 5593.738794326782, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2279}, "5": {"k": 5, "answer": "James Ray, George Harrison, Shakin' Stevens, Lee Matthews", "latency_ms": 6525.654077529907, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2861}, "6": {"k": 6, "answer": "George Harrison", "latency_ms": 1233.7992191314697, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3418}}}
{"qid": "9168660859139861094", "question": "two main types of research methods used in psychology", "gold_answers": ["Correlational research", "Descriptive research", "quantitative data", "Quantitative psychological research", "qualitative", "quantitative", "Qualitative psychological research", "Experimental research", "qualitative data"], "shots": {"0": {"k": 0, "answer": "Qualitative and Quantitative.", "latency_ms": 3486.398220062256, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Qualitative and quantitative", "latency_ms": 2271.808862686157, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 807}, "2": {"k": 2, "answer": "Qualitative and Quantitative", "latency_ms": 2993.840217590332, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 1631}, "3": {"k": 3, "answer": "Qualitative and Quantitative", "latency_ms": 3169.283628463745, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2323}, "4": {"k": 4, "answer": "Qualitative or quantitative", "latency_ms": 1422.5013256072998, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3060}, "5": {"k": 5, "answer": "Qualitative or quantitative", "latency_ms": 1584.221601486206, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3863}, "6": {"k": 6, "answer": "Qualitative or quantitative", "latency_ms": 2437.4852180480957, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 4560}}}
{"qid": "-7073063191952855764", "question": "who plays peter in what we do in the shadows", "gold_answers": ["Ben Fransham"], "shots": {"0": {"k": 0, "answer": "Kayvan Novak", "latency_ms": 2259.1350078582764, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2119.776487350464, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 634}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2645.747184753418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1276}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2729.861259460449, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1909}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2379.0931701660156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2540}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2453.413963317871, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3196}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2767.4221992492676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3816}}}
{"qid": "7901746249864619718", "question": "when does the 14th season of grey's anatomy come out", "gold_answers": ["September 28 , 2017"], "shots": {"0": {"k": 0, "answer": "September 28, 2017", "latency_ms": 2082.681655883789, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 28, 2017", "latency_ms": 4414.576768875122, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 629}, "2": {"k": 2, "answer": "September 28, 2017", "latency_ms": 3857.7089309692383, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 1327}, "3": {"k": 3, "answer": "September 28, 2017", "latency_ms": 3182.584524154663, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2033}, "4": {"k": 4, "answer": "September 28, 2017", "latency_ms": 3886.6066932678223, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 2706}, "5": {"k": 5, "answer": "September 28, 2017", "latency_ms": 3364.574909210205, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3389}, "6": {"k": 6, "answer": "September 28, 2017", "latency_ms": 3884.396553039551, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 3976}}}
{"qid": "-896127746191949637", "question": "when was the first hunger games book published", "gold_answers": ["September 14 , 2008", "2008"], "shots": {"0": {"k": 0, "answer": "2008", "latency_ms": 1083.777666091919, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "September 14, 2008", "latency_ms": 3833.6167335510254, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 670}, "2": {"k": 2, "answer": "September 14, 2008", "latency_ms": 2910.3057384490967, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1287}, "3": {"k": 3, "answer": "September 14, 2008", "latency_ms": 3664.6764278411865, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1947}, "4": {"k": 4, "answer": "September 14, 2008", "latency_ms": 3257.185459136963, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2599}, "5": {"k": 5, "answer": "September 14, 2008", "latency_ms": 3320.0438022613525, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3282}, "6": {"k": 6, "answer": "September 14, 2008", "latency_ms": 2556.065082550049, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3956}}}
{"qid": "8044909700499569711", "question": "how much is the united states in debt to china", "gold_answers": ["$1.06 trillion", "1,200.5"], "shots": {"0": {"k": 0, "answer": "$1.08 trillion", "latency_ms": 2149.740695953369, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "21 percent", "latency_ms": 1437.270164489746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "$1.16 trillion", "latency_ms": 3428.804636001587, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1278}, "3": {"k": 3, "answer": "$1.16 trillion", "latency_ms": 2017.6079273223877, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1898}, "4": {"k": 4, "answer": "$1.18 trillion", "latency_ms": 2473.625659942627, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2551}, "5": {"k": 5, "answer": "$1.18 trillion", "latency_ms": 1973.9301204681396, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3141}, "6": {"k": 6, "answer": "$1.18 trillion", "latency_ms": 2771.5063095092773, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 3789}}}
{"qid": "-3123234845590594961", "question": "where did the race in grease take place", "gold_answers": ["Thunder Road"], "shots": {"0": {"k": 0, "answer": "The Thunder Road Race", "latency_ms": 2201.266288757324, "em": 0.0, "f1": 0.8, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Los Angeles River", "latency_ms": 1310.8577728271484, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 625}, "2": {"k": 2, "answer": "Los Angeles River", "latency_ms": 2045.0005531311035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1223}, "3": {"k": 3, "answer": "Los Angeles River", "latency_ms": 1870.0032234191895, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1871}, "4": {"k": 4, "answer": "Los Angeles River", "latency_ms": 2134.5431804656982, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2557}, "5": {"k": 5, "answer": "Los Angeles River", "latency_ms": 1581.4454555511475, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3190}, "6": {"k": 6, "answer": "Los Angeles River", "latency_ms": 1579.4775485992432, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3778}}}
{"qid": "4326992806019599297", "question": "what is the maximum data rate for the 802.11a standard select one", "gold_answers": ["54 Mbit / s"], "shots": {"0": {"k": 0, "answer": "54 Mbps", "latency_ms": 998.9690780639648, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "54 Mbit/s", "latency_ms": 2729.7394275665283, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 628}, "2": {"k": 2, "answer": "54 Mbit/s", "latency_ms": 2127.408504486084, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1274}, "3": {"k": 3, "answer": "54 Mbit/s", "latency_ms": 2190.3412342071533, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1950}, "4": {"k": 4, "answer": "54 Mbit/s", "latency_ms": 2052.4380207061768, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2643}, "5": {"k": 5, "answer": "54 Mbit/s", "latency_ms": 1618.8092231750488, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3265}, "6": {"k": 6, "answer": "54 Mbit/s", "latency_ms": 1626.3666152954102, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3927}}}
{"qid": "6144099837933283715", "question": "a good that can be used in place of another good", "gold_answers": ["A substitute good", "substitute good"], "shots": {"0": {"k": 0, "answer": "Substitute", "latency_ms": 1231.0411930084229, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3147.5086212158203, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 577}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2125.3089904785156, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1178}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1371.9258308410645, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1801}, "4": {"k": 4, "answer": "Equivalent new items", "latency_ms": 2561.473608016968, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2328}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1778.3353328704834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3033}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 3025.653600692749, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3623}}}
{"qid": "-4621948696356225060", "question": "where in the bible can i find the lord's prayer", "gold_answers": ["the Gospel of Luke", "the Gospel of Matthew"], "shots": {"0": {"k": 0, "answer": "Matthew 6:9-13, Luke 11:2-4", "latency_ms": 4719.809770584106, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2647.8898525238037, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 562}, "2": {"k": 2, "answer": "Gospel of Matthew and Luke", "latency_ms": 3286.9534492492676, "em": 0.0, "f1": 0.7499999999999999, "containment": 0.0, "context_chars": 1152}, "3": {"k": 3, "answer": "Gospel of Matthew, Gospel of Luke", "latency_ms": 4546.03123664856, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1813}, "4": {"k": 4, "answer": "Gospel of Matthew, Gospel of Luke", "latency_ms": 2920.6318855285645, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2417}, "5": {"k": 5, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 3455.7840824127197, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 2998}, "6": {"k": 6, "answer": "Gospel of Matthew and Gospel of Luke", "latency_ms": 2373.997211456299, "em": 0.0, "f1": 0.6, "containment": 0.0, "context_chars": 3544}}}
{"qid": "6941993254369956277", "question": "when was the last time nebraska voted democrat", "gold_answers": ["1964"], "shots": {"0": {"k": 0, "answer": "1964", "latency_ms": 1580.3260803222656, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "1908", "latency_ms": 1504.9805641174316, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 695}, "2": {"k": 2, "answer": "1908", "latency_ms": 1243.7102794647217, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1376}, "3": {"k": 3, "answer": "1908", "latency_ms": 1199.432611465454, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1948}, "4": {"k": 4, "answer": "1908", "latency_ms": 1033.0901145935059, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2656}, "5": {"k": 5, "answer": "1908", "latency_ms": 1850.142002105713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3307}, "6": {"k": 6, "answer": "1908", "latency_ms": 1256.9615840911865, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4009}}}
{"qid": "-8955197065025093046", "question": "who become the ceo of it wipro company in 2016", "gold_answers": ["Abid Ali Neemuchwala"], "shots": {"0": {"k": 0, "answer": "Abidali Neemuchwala", "latency_ms": 2731.125831604004, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Abidali Neemuchwala", "latency_ms": 4029.0417671203613, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 678}, "2": {"k": 2, "answer": "Abidali Neemuchwala", "latency_ms": 2694.4894790649414, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1363}, "3": {"k": 3, "answer": "Abidali Neemuchwala", "latency_ms": 2968.0678844451904, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2047}, "4": {"k": 4, "answer": "Abidali Neemuchwala", "latency_ms": 3544.8780059814453, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2653}, "5": {"k": 5, "answer": "Abidali Neemuchwala", "latency_ms": 3980.3926944732666, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3260}, "6": {"k": 6, "answer": "Abidali Neemuchwala", "latency_ms": 3229.729175567627, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3888}}}
{"qid": "-3010425579521817304", "question": "who does eric end up with in gossip girl", "gold_answers": ["Jenny"], "shots": {"0": {"k": 0, "answer": "Vanessa and then Serena", "latency_ms": 1499.565601348877, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2574.6309757232666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 610}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2879.39453125, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1234}, "3": {"k": 3, "answer": "Jenny", "latency_ms": 2020.533800125122, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1819}, "4": {"k": 4, "answer": "Jenny", "latency_ms": 986.9563579559326, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2445}, "5": {"k": 5, "answer": "Jenny", "latency_ms": 2043.9181327819824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3050}, "6": {"k": 6, "answer": "Jenny", "latency_ms": 1560.638189315796, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3673}}}
{"qid": "7264112453286983469", "question": "who plays unis in she's the man", "gold_answers": ["Emily Perkins"], "shots": {"0": {"k": 0, "answer": "Amanda Bynes", "latency_ms": 2125.692844390869, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Amanda Bynes", "latency_ms": 2188.283681869507, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 611}, "2": {"k": 2, "answer": "Amanda Bynes", "latency_ms": 2092.9508209228516, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1922.4298000335693, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1801}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1403.7599563598633, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2369}, "5": {"k": 5, "answer": "Amanda Bynes", "latency_ms": 1630.0616264343262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2944}, "6": {"k": 6, "answer": "Amanda Bynes", "latency_ms": 2154.7024250030518, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3531}}}
{"qid": "2627695648792729859", "question": "who played booster in jingle all the way", "gold_answers": ["Curtis Armstrong"], "shots": {"0": {"k": 0, "answer": "Eric Lloyd", "latency_ms": 781.6414833068848, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2666.942834854126, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 679}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1937.6418590545654, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1342}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 1432.5494766235352, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2010}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2386.1160278320312, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2699}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1849.6928215026855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3321}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2097.4972248077393, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3954}}}
{"qid": "-9169976182548289414", "question": "who is recognized as the founder of islam", "gold_answers": ["the Islamic prophet Muhammad", "Muhammad"], "shots": {"0": {"k": 0, "answer": "Muhammad", "latency_ms": 1199.8019218444824, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Muhammad", "latency_ms": 1765.8674716949463, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 606}, "2": {"k": 2, "answer": "Muhammad", "latency_ms": 1730.38911819458, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1211}, "3": {"k": 3, "answer": "Muhammad", "latency_ms": 1529.2079448699951, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1837}, "4": {"k": 4, "answer": "Muhammad", "latency_ms": 1498.1927871704102, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2432}, "5": {"k": 5, "answer": "Muhammad", "latency_ms": 1013.2706165313721, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3070}, "6": {"k": 6, "answer": "Muhammad", "latency_ms": 1270.751953125, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3674}}}
{"qid": "-6696747560626271522", "question": "who plays emma in air bud world pup", "gold_answers": ["Brittany Paige Bouck"], "shots": {"0": {"k": 0, "answer": "Brittany Paige Bouck", "latency_ms": 2316.9846534729004, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Brittany Paige Bouck", "latency_ms": 3016.9365406036377, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 592}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2712.690830230713, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1208}, "3": {"k": 3, "answer": "Brittany Paige Bouck", "latency_ms": 3006.2530040740967, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1838}, "4": {"k": 4, "answer": "Emma", "latency_ms": 1543.7321662902832, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2511}, "5": {"k": 5, "answer": "Brittany Paige Bouck", "latency_ms": 2626.7220973968506, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3092}, "6": {"k": 6, "answer": "Emma", "latency_ms": 1484.3215942382812, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3742}}}
{"qid": "5871681964137793716", "question": "how old was sasuke when his clan died", "gold_answers": ["seven"], "shots": {"0": {"k": 0, "answer": "7 years old", "latency_ms": 1075.7946968078613, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2889.8725509643555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 594}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2387.009859085083, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1242}, "3": {"k": 3, "answer": "Seven", "latency_ms": 934.1716766357422, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1885}, "4": {"k": 4, "answer": "Seven", "latency_ms": 1503.0348300933838, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2487}, "5": {"k": 5, "answer": "Seven", "latency_ms": 2058.2478046417236, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3102}, "6": {"k": 6, "answer": "Seven", "latency_ms": 468.4121608734131, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3730}}}
{"qid": "8552176802923865025", "question": "where do some of the natural air pollutants come from", "gold_answers": ["wildfires", "Dust", "Vegetation", "Carbon dioxide", "Volcanic activity"], "shots": {"0": {"k": 0, "answer": "Volcanic eruptions.", "latency_ms": 2426.1908531188965, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2172.7781295776367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 718}, "2": {"k": 2, "answer": "dust storms", "latency_ms": 941.8284893035889, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 1335}, "3": {"k": 3, "answer": "Dust storms, volcanic eruption", "latency_ms": 4353.079557418823, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 1994}, "4": {"k": 4, "answer": "Dust storms, volcanic eruption", "latency_ms": 3390.5999660491943, "em": 0.0, "f1": 0.4, "containment": 1.0, "context_chars": 2656}, "5": {"k": 5, "answer": "Dust storms, volcanic ash, sea spray, meteoroids", "latency_ms": 4320.276975631714, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 3341}, "6": {"k": 6, "answer": "dust storms, volcanic ash, sea spray, meteoroids", "latency_ms": 4464.4455909729, "em": 0.0, "f1": 0.25, "containment": 1.0, "context_chars": 4027}}}
{"qid": "-2630525064809360052", "question": "who is the pastor of new birth missionary baptist church now", "gold_answers": ["Stephen A. Davis"], "shots": {"0": {"k": 0, "answer": "Jamal Harrison Bryant", "latency_ms": 948.5518932342529, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2138.1664276123047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 635}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2177.49285697937, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1266}, "3": {"k": 3, "answer": "Jamal Bryant", "latency_ms": 2805.8524131774902, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1944}, "4": {"k": 4, "answer": "Jamal Bryant", "latency_ms": 1559.096097946167, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2621}, "5": {"k": 5, "answer": "Jamal Bryant", "latency_ms": 2582.714796066284, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3286}, "6": {"k": 6, "answer": "Jamal Bryant", "latency_ms": 2031.1496257781982, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3926}}}
{"qid": "6434931911289860123", "question": "who is doing 2018 super bowl half time show", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Justin Timberlake", "latency_ms": 1427.7732372283936, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2182.828187942505, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 686}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1832.782506942749, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1311}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2576.2202739715576, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1907}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2117.579698562622, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2581}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2331.596612930298, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3151}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2580.317497253418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3756}}}
{"qid": "8869318258439148973", "question": "who is playing the halftime show for the superbowl", "gold_answers": ["Justin Timberlake"], "shots": {"0": {"k": 0, "answer": "Rihanna", "latency_ms": 1603.731632232666, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Maroon 5", "latency_ms": 2117.825508117676, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 630}, "2": {"k": 2, "answer": "Maroon 5", "latency_ms": 1605.7803630828857, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1302}, "3": {"k": 3, "answer": "Maroon 5", "latency_ms": 2438.668727874756, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1954}, "4": {"k": 4, "answer": "Maroon 5", "latency_ms": 2138.259172439575, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2587}, "5": {"k": 5, "answer": "Maroon 5", "latency_ms": 2355.907678604126, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3223}, "6": {"k": 6, "answer": "Maroon 5", "latency_ms": 1396.1312770843506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3864}}}
{"qid": "-8359866474144720253", "question": "who hosted they think it's all over", "gold_answers": ["Des Lynam", "Lee Mack", "comedian Nick Hancock", "Nick Hancock"], "shots": {"0": {"k": 0, "answer": "Nick Hancock", "latency_ms": 1070.20902633667, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Lee Mack", "latency_ms": 1779.7935009002686, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 678}, "2": {"k": 2, "answer": "Lee Mack", "latency_ms": 925.3807067871094, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1279}, "3": {"k": 3, "answer": "Lee Mack", "latency_ms": 1445.077896118164, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1908}, "4": {"k": 4, "answer": "Nick Hancock", "latency_ms": 1246.3088035583496, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2542}, "5": {"k": 5, "answer": "Nick Hancock", "latency_ms": 1728.2617092132568, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3181}, "6": {"k": 6, "answer": "Nick Hancock", "latency_ms": 1528.8238525390625, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3827}}}
{"qid": "-1206653570097564556", "question": "where does the movie proof of life take place", "gold_answers": ["The Republic of Tecala"], "shots": {"0": {"k": 0, "answer": "South America", "latency_ms": 1087.3122215270996, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Tecala", "latency_ms": 1621.4921474456787, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 640}, "2": {"k": 2, "answer": "The Republic of Tecala", "latency_ms": 2117.114543914795, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1268}, "3": {"k": 3, "answer": "Tecala", "latency_ms": 2558.9396953582764, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1872}, "4": {"k": 4, "answer": "Tecala", "latency_ms": 1346.7068672180176, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2523}, "5": {"k": 5, "answer": "Tecala, a fictional South American country", "latency_ms": 5111.688613891602, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 3121}, "6": {"k": 6, "answer": "Tecala, a fictional South American country", "latency_ms": 2794.834613800049, "em": 0.0, "f1": 0.25, "containment": 0.0, "context_chars": 3756}}}
{"qid": "6144521615402262404", "question": "where does us highway 1 start and end", "gold_answers": ["Fort Kent , Maine", "to Key West , Florida", "Key West , Florida"], "shots": {"0": {"k": 0, "answer": "Fort Kent, Maine to Key West, Florida", "latency_ms": 3982.863664627075, "em": 0.0, "f1": 0.7272727272727273, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2097.3873138427734, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 645}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2239.762783050537, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1273}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2204.427480697632, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1934}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 1616.361379623413, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2548}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2397.277355194092, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3142}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1784.775972366333, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3751}}}
{"qid": "-8631190913794477985", "question": "who scored the most points in a single game in the nba", "gold_answers": ["Wilt Chamberlain"], "shots": {"0": {"k": 0, "answer": "Wilt Chamberlain, 100 points", "latency_ms": 3501.636028289795, "em": 0.0, "f1": 0.6666666666666665, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Wilt Chamberlain", "latency_ms": 2234.7934246063232, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 664}, "2": {"k": 2, "answer": "Wilt Chamberlain", "latency_ms": 2116.7500019073486, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1274}, "3": {"k": 3, "answer": "Wilt Chamberlain", "latency_ms": 1546.039342880249, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1896}, "4": {"k": 4, "answer": "Wilt Chamberlain", "latency_ms": 1845.2727794647217, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2505}, "5": {"k": 5, "answer": "Wilt Chamberlain", "latency_ms": 1957.221269607544, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3102}, "6": {"k": 6, "answer": "Wilt Chamberlain", "latency_ms": 3096.999406814575, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3741}}}
{"qid": "199896377614951764", "question": "where is natural gas found in the us", "gold_answers": ["Texas", "Pennsylvania", "Colorado", "New Mexico", "West Virginia", "Arkansas", "Wyoming", "Louisiana"], "shots": {"0": {"k": 0, "answer": "Texas, Oklahoma, Louisiana, Alaska, and Pennsylvania.", "latency_ms": 4401.639461517334, "em": 0.0, "f1": 0.2857142857142857, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Shale formations", "latency_ms": 2089.808702468872, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "Pennsylvania", "latency_ms": 1479.6192646026611, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1253}, "3": {"k": 3, "answer": "Pennsylvania", "latency_ms": 1528.2773971557617, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1895}, "4": {"k": 4, "answer": "Pennsylvania", "latency_ms": 1243.8080310821533, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2490}, "5": {"k": 5, "answer": "Pennsylvania, Ohio, Kentucky, New York, West Virginia", "latency_ms": 4723.452568054199, "em": 0.0, "f1": 0.4444444444444445, "containment": 1.0, "context_chars": 3149}, "6": {"k": 6, "answer": "Pennsylvania, Texas, Michigan, Ohio, Kentucky, New York, West Virginia", "latency_ms": 5500.833511352539, "em": 0.0, "f1": 0.3636363636363636, "containment": 1.0, "context_chars": 3765}}}
{"qid": "8986775680996674901", "question": "when were the first general elections held in independent india", "gold_answers": ["1951 -- 52"], "shots": {"0": {"k": 0, "answer": "1951-52", "latency_ms": 1169.0020561218262, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "25 October 1951 – 27 March 1952", "latency_ms": 5159.594774246216, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 663}, "2": {"k": 2, "answer": "25 October 1951 to 27 March 1952", "latency_ms": 4807.236909866333, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 1338}, "3": {"k": 3, "answer": "25 October 1951 and 27 March 1952", "latency_ms": 3227.773427963257, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 2001}, "4": {"k": 4, "answer": "25 October 1951 and 27 March 1952", "latency_ms": 3834.6402645111084, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 2649}, "5": {"k": 5, "answer": "25 October 1951 and 27 March 1952", "latency_ms": 4151.363849639893, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 3283}, "6": {"k": 6, "answer": "25 October 1951 and 27 March 1952", "latency_ms": 3615.405321121216, "em": 0.0, "f1": 0.22222222222222224, "containment": 0.0, "context_chars": 3944}}}
{"qid": "700604097171850168", "question": "what was the tower of london originally used for", "gold_answers": ["as a royal residence", "a royal residence"], "shots": {"0": {"k": 0, "answer": "Royal Palace and Fortress", "latency_ms": 1919.191598892212, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "A grand palace", "latency_ms": 1736.9608879089355, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 592}, "2": {"k": 2, "answer": "A grand palace and royal residence", "latency_ms": 2052.7236461639404, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1176}, "3": {"k": 3, "answer": "A grand palace and royal residence", "latency_ms": 2561.279773712158, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 1804}, "4": {"k": 4, "answer": "A royal residence", "latency_ms": 1038.2118225097656, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2399}, "5": {"k": 5, "answer": "A royal residence", "latency_ms": 1828.0038833618164, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3055}, "6": {"k": 6, "answer": "A royal residence", "latency_ms": 1268.8007354736328, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3701}}}
{"qid": "6745303307988470742", "question": "what type of fuel goes in a zippo", "gold_answers": ["butane", "lighter fluid"], "shots": {"0": {"k": 0, "answer": "Lighter fluid", "latency_ms": 1422.1980571746826, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2609.3485355377197, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 620}, "2": {"k": 2, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 5587.778091430664, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1227}, "3": {"k": 3, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 6116.708040237427, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1883}, "4": {"k": 4, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 5026.874542236328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2476}, "5": {"k": 5, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 6383.075714111328, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3114}, "6": {"k": 6, "answer": "light petroleum distillate or synthetic isoparaffinic hydrocarbon", "latency_ms": 5323.734283447266, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3760}}}
{"qid": "-2060506905455252030", "question": "first jnanpith award was an autor of which language", "gold_answers": ["Malayalam"], "shots": {"0": {"k": 0, "answer": "Gujarati", "latency_ms": 1946.885108947754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Malayalam", "latency_ms": 2403.130531311035, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 669}, "2": {"k": 2, "answer": "Malayalam", "latency_ms": 2160.9668731689453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1332}, "3": {"k": 3, "answer": "Malayalam", "latency_ms": 1600.4152297973633, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1972}, "4": {"k": 4, "answer": "Malayalam", "latency_ms": 2132.366180419922, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2617}, "5": {"k": 5, "answer": "Malayalam", "latency_ms": 1945.7745552062988, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3293}, "6": {"k": 6, "answer": "Malayalam", "latency_ms": 1864.192247390747, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 4018}}}
{"qid": "4738663028303377024", "question": "who played the mad hatter in the batman tv show", "gold_answers": ["Roddy McDowall", "David Wayne", "Benedict Samuel"], "shots": {"0": {"k": 0, "answer": "David Wayne", "latency_ms": 1096.8494415283203, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2408.447742462158, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 641}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1434.5929622650146, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1279}, "3": {"k": 3, "answer": "David Wayne", "latency_ms": 2054.861307144165, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1943}, "4": {"k": 4, "answer": "David Wayne", "latency_ms": 1412.585735321045, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2598}, "5": {"k": 5, "answer": "David Wayne", "latency_ms": 1077.2147178649902, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3250}, "6": {"k": 6, "answer": "David Wayne", "latency_ms": 2106.9889068603516, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3905}}}
{"qid": "-1650946015201779846", "question": "how many cracker barrels in the united states", "gold_answers": ["639", "more than 600"], "shots": {"0": {"k": 0, "answer": "664", "latency_ms": 1465.324878692627, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "645 stores in 44 states", "latency_ms": 3310.0719451904297, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 636}, "2": {"k": 2, "answer": "645 stores in 44 states", "latency_ms": 2842.724323272705, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1280}, "3": {"k": 3, "answer": "645 stores in 44 states", "latency_ms": 3327.119827270508, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1964}, "4": {"k": 4, "answer": "645 stores in 44 states", "latency_ms": 4017.636299133301, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2610}, "5": {"k": 5, "answer": "645 stores in 44 states", "latency_ms": 3088.8068675994873, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3282}, "6": {"k": 6, "answer": "645 stores in 44 states", "latency_ms": 3308.8650703430176, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3899}}}
{"qid": "-3422350952909582054", "question": "in photosynthesis the carbon in co2 is initially fixed to what molecule", "gold_answers": ["3 - phosphoglycerate"], "shots": {"0": {"k": 0, "answer": "RuBP (Ribulose-1,5-Bisphosphate)", "latency_ms": 6279.981374740601, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 5521.497964859009, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 727}, "2": {"k": 2, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 4840.160369873047, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1441}, "3": {"k": 3, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 5348.59037399292, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2203}, "4": {"k": 4, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 3121.1514472961426, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2882}, "5": {"k": 5, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 5537.8899574279785, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3587}, "6": {"k": 6, "answer": "ribulose 1,5-bisphosphate", "latency_ms": 5295.401096343994, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4249}}}
{"qid": "-2704708103393811482", "question": "who voices randy in f is for family", "gold_answers": ["T.J. Miller"], "shots": {"0": {"k": 0, "answer": "Bill Burr", "latency_ms": 1154.0162563323975, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1930.9008121490479, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 582}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1915.0707721710205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1254}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2190.929174423218, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1872}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2193.138599395752, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2535}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2388.3793354034424, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3138}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2198.007822036743, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3785}}}
{"qid": "5482012214308896475", "question": "how many countries does cadbury sell its products", "gold_answers": ["more than 50", "more than 50 countries worldwide"], "shots": {"0": {"k": 0, "answer": "160 countries", "latency_ms": 2037.9507541656494, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "more than fifty", "latency_ms": 1628.3435821533203, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 702}, "2": {"k": 2, "answer": "more than fifty", "latency_ms": 1851.8247604370117, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1371}, "3": {"k": 3, "answer": "more than fifty", "latency_ms": 2371.9944953918457, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2079}, "4": {"k": 4, "answer": "more than fifty", "latency_ms": 2072.2479820251465, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2750}, "5": {"k": 5, "answer": "more than fifty", "latency_ms": 2399.5726108551025, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3449}, "6": {"k": 6, "answer": "more than fifty", "latency_ms": 1638.012409210205, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 4113}}}
{"qid": "-5737509619250561344", "question": "ethiopia flight 961 crashes in to the sea", "gold_answers": ["23 November 1996"], "shots": {"0": {"k": 0, "answer": "November 23, 1996", "latency_ms": 3163.0349159240723, "em": 0.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Indian Ocean", "latency_ms": 2054.8269748687744, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 696}, "2": {"k": 2, "answer": "Indian Ocean", "latency_ms": 2032.5102806091309, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1353}, "3": {"k": 3, "answer": "Indian Ocean", "latency_ms": 1571.0062980651855, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2001}, "4": {"k": 4, "answer": "Indian Ocean", "latency_ms": 1501.377820968628, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2664}, "5": {"k": 5, "answer": "Indian Ocean", "latency_ms": 1609.5645427703857, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3325}, "6": {"k": 6, "answer": "Indian Ocean", "latency_ms": 2113.144874572754, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3991}}}
{"qid": "-7175456993713717070", "question": "what does istj mean in a personality test", "gold_answers": ["Extroverted Thinking ( Te )", "Extroverted Intuition ( Ne )", "Introverted Feeling ( Fi )", "Introverted Sensing ( Si )"], "shots": {"0": {"k": 0, "answer": "Introverted, Sensing, Thinking, Judging.", "latency_ms": 4862.996816635132, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 5056.00643157959, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 724}, "2": {"k": 2, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 4820.324659347534, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 1385}, "3": {"k": 3, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 3538.4609699249268, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2101}, "4": {"k": 4, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 4652.958631515503, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 2776}, "5": {"k": 5, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 4211.954593658447, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 3504}, "6": {"k": 6, "answer": "Introversion, Sensing, Thinking, Judgment", "latency_ms": 3848.3564853668213, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 4132}}}
{"qid": "4437170343606050664", "question": "who does the voice of ash in sing", "gold_answers": ["Scarlett Johansson"], "shots": {"0": {"k": 0, "answer": "Scarlett Johansson", "latency_ms": 2620.974540710449, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1593.1713581085205, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 618}, "2": {"k": 2, "answer": "Scarlett Johansson", "latency_ms": 2602.325916290283, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1282}, "3": {"k": 3, "answer": "Scarlett Johansson", "latency_ms": 2912.343978881836, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1893}, "4": {"k": 4, "answer": "Scarlett Johansson", "latency_ms": 2607.8875064849854, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2528}, "5": {"k": 5, "answer": "Scarlett Johansson", "latency_ms": 2228.888988494873, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3152}, "6": {"k": 6, "answer": "Scarlett Johansson", "latency_ms": 2620.002031326294, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3768}}}
{"qid": "-8773010039077552394", "question": "who scored the most points in their nba career", "gold_answers": ["Kareem Abdul - Jabbar"], "shots": {"0": {"k": 0, "answer": "LeBron James", "latency_ms": 2095.4558849334717, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Danny Finn", "latency_ms": 1582.9191207885742, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 535}, "2": {"k": 2, "answer": "Deron Williams", "latency_ms": 1774.4252681732178, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1127}, "3": {"k": 3, "answer": "Ray Allen", "latency_ms": 1270.1239585876465, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1680}, "4": {"k": 4, "answer": "Ray Allen", "latency_ms": 1831.197738647461, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2273}, "5": {"k": 5, "answer": "Ray Allen", "latency_ms": 1496.5994358062744, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2857}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 1376.542091369629, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3414}}}
{"qid": "-3278224144622911032", "question": "with a land area of 54 314 square miles where does wisconsin rank among the 50 states", "gold_answers": ["25"], "shots": {"0": {"k": 0, "answer": "23rd", "latency_ms": 1777.7094841003418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 1651.158332824707, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 585}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1595.8750247955322, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1226}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2297.558546066284, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1809}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2344.952344894409, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2388}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 2049.740791320801, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3028}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2703.590154647827, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3633}}}
{"qid": "-3466298422174960041", "question": "who plays charles on have and have nots", "gold_answers": ["Nick Sager", "Nick Sagar"], "shots": {"0": {"k": 0, "answer": "Renée Lawless", "latency_ms": 2263.7040615081787, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2432.0693016052246, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 650}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 1936.4631175994873, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1280}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 2212.9106521606445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1924}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 3200.697898864746, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2547}, "5": {"k": 5, "answer": "Angela Robinson", "latency_ms": 1381.3247680664062, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3129}, "6": {"k": 6, "answer": "Angela Robinson", "latency_ms": 2150.160074234009, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3779}}}
{"qid": "1803248910342766083", "question": "who wrote the book my country my life", "gold_answers": ["L.K. Advani"], "shots": {"0": {"k": 0, "answer": "L.K. Advani", "latency_ms": 2189.5313262939453, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "L. K. Advani", "latency_ms": 3815.4244422912598, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 586}, "2": {"k": 2, "answer": "L. K. Advani", "latency_ms": 4075.693368911743, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1158}, "3": {"k": 3, "answer": "L. K. Advani", "latency_ms": 2455.7485580444336, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1767}, "4": {"k": 4, "answer": "L. K. Advani", "latency_ms": 2104.069471359253, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 2408}, "5": {"k": 5, "answer": "L. K. Advani", "latency_ms": 2736.422538757324, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3040}, "6": {"k": 6, "answer": "L. K. Advani", "latency_ms": 2674.740791320801, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 3673}}}
{"qid": "-1176280233636154422", "question": "when did the east india company take control of india", "gold_answers": ["1757", "1799", "1612", "in 1757"], "shots": {"0": {"k": 0, "answer": "1757", "latency_ms": 1528.3730030059814, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "18th century", "latency_ms": 1263.0891799926758, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 646}, "2": {"k": 2, "answer": "1757", "latency_ms": 1730.0219535827637, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1295}, "3": {"k": 3, "answer": "1757", "latency_ms": 1790.6455993652344, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1955}, "4": {"k": 4, "answer": "1757", "latency_ms": 1213.9687538146973, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2657}, "5": {"k": 5, "answer": "1757", "latency_ms": 1560.0826740264893, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3305}, "6": {"k": 6, "answer": "1757", "latency_ms": 1497.7011680603027, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3960}}}
{"qid": "5911119091653953181", "question": "where does the water come from to fill lake eyre", "gold_answers": ["Warburton River"], "shots": {"0": {"k": 0, "answer": "Cooper Creek and Diamantina River", "latency_ms": 4439.697265625, "em": 0.0, "f1": 0.28571428571428575, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Murray-Darling", "latency_ms": 2989.31622505188, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 624}, "2": {"k": 2, "answer": "Georgina River", "latency_ms": 3958.681106567383, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1212}, "3": {"k": 3, "answer": "Georgina River", "latency_ms": 3245.323419570923, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 1823}, "4": {"k": 4, "answer": "Georgina River", "latency_ms": 2737.7169132232666, "em": 0.0, "f1": 0.5, "containment": 0.0, "context_chars": 2441}, "5": {"k": 5, "answer": "Cooper Creek", "latency_ms": 1346.6594219207764, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3009}, "6": {"k": 6, "answer": "Cooper Creek", "latency_ms": 2577.5861740112305, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3622}}}
{"qid": "5557104327881436095", "question": "who is the founder of google and when was it founded", "gold_answers": ["1998", "Sergey Brin", "Larry Page and Sergey Brin", "Larry Page"], "shots": {"0": {"k": 0, "answer": "Larry Page and Sergey Brin, September 4, 1998", "latency_ms": 7511.301755905151, "em": 0.0, "f1": 0.7692307692307694, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3241.028308868408, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 676}, "2": {"k": 2, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 3850.1322269439697, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 1356}, "3": {"k": 3, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 5165.1904582977295, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 2015}, "4": {"k": 4, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 4425.993204116821, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 2669}, "5": {"k": 5, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 4364.766597747803, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 3320}, "6": {"k": 6, "answer": "Larry Page and Sergey Brin, 1998", "latency_ms": 5875.727653503418, "em": 0.0, "f1": 0.9090909090909091, "containment": 1.0, "context_chars": 4002}}}
{"qid": "-1333328812971005142", "question": "who did the dominican republic gain its independence from", "gold_answers": ["Haiti"], "shots": {"0": {"k": 0, "answer": "Haiti", "latency_ms": 2316.0760402679443, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Haiti", "latency_ms": 1821.082592010498, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 673}, "2": {"k": 2, "answer": "Haiti", "latency_ms": 1853.637933731079, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1400}, "3": {"k": 3, "answer": "Haiti and Spain", "latency_ms": 2562.368869781494, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2069}, "4": {"k": 4, "answer": "Haiti and Spain", "latency_ms": 3786.1080169677734, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 2749}, "5": {"k": 5, "answer": "Haiti and Spain", "latency_ms": 2794.5199012756348, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 3396}, "6": {"k": 6, "answer": "Haiti and Spain", "latency_ms": 3302.456855773926, "em": 0.0, "f1": 0.5, "containment": 1.0, "context_chars": 4091}}}
{"qid": "1122605495295045742", "question": "how did early humans make use of stones during the prehistoric period", "gold_answers": ["start fires", "hunt", "bury their dead"], "shots": {"0": {"k": 0, "answer": "Tool making.", "latency_ms": 1843.0860042572021, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "making stone tools", "latency_ms": 1838.1075859069824, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 663}, "2": {"k": 2, "answer": "making stone tools and using jewellery", "latency_ms": 3536.858320236206, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1310}, "3": {"k": 3, "answer": "Making stone tools and jewellery.", "latency_ms": 3839.6708965301514, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1995}, "4": {"k": 4, "answer": "Making stone tools and jewellery", "latency_ms": 2459.213972091675, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2649}, "5": {"k": 5, "answer": "Making stone tools", "latency_ms": 2746.8745708465576, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3287}, "6": {"k": 6, "answer": "To make tools.", "latency_ms": 3198.1446743011475, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3913}}}
{"qid": "7402184777947527945", "question": "who wrote the text for jeanie with the light brown hair", "gold_answers": ["Stephen Foster"], "shots": {"0": {"k": 0, "answer": "Maud Irving", "latency_ms": 1362.8413677215576, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Stephen Foster", "latency_ms": 2139.1448974609375, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 632}, "2": {"k": 2, "answer": "Stephen Foster", "latency_ms": 1791.9580936431885, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1222}, "3": {"k": 3, "answer": "Stephen Foster", "latency_ms": 1778.0776023864746, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 1839}, "4": {"k": 4, "answer": "Stephen Foster", "latency_ms": 2016.5460109710693, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2458}, "5": {"k": 5, "answer": "Stephen Foster", "latency_ms": 2082.275867462158, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3103}, "6": {"k": 6, "answer": "Stephen Foster", "latency_ms": 2313.476085662842, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3752}}}
{"qid": "1656745334095178455", "question": "the basic steps of rational decision making include defining goals and", "gold_answers": ["Implementation of plans / policies"], "shots": {"0": {"k": 0, "answer": "identifying alternatives", "latency_ms": 1610.985279083252, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "problem definition", "latency_ms": 2581.840753555298, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 774}, "2": {"k": 2, "answer": "problem definition", "latency_ms": 1778.8286209106445, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1428}, "3": {"k": 3, "answer": "problem definition", "latency_ms": 1487.8265857696533, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2146}, "4": {"k": 4, "answer": "problem definition", "latency_ms": 2021.9876766204834, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2779}, "5": {"k": 5, "answer": "problem definition", "latency_ms": 1735.335350036621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3431}, "6": {"k": 6, "answer": "problem definition", "latency_ms": 2089.0724658966064, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4056}}}
{"qid": "-216657635823030751", "question": "when was the design period known as art deco", "gold_answers": ["1910 -- 1939"], "shots": {"0": {"k": 0, "answer": "1920-1940", "latency_ms": 2500.8249282836914, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "20th century", "latency_ms": 3049.438238143921, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 661}, "2": {"k": 2, "answer": "1910 and 1914", "latency_ms": 4062.124252319336, "em": 0.0, "f1": 0.4, "containment": 0.0, "context_chars": 1299}, "3": {"k": 3, "answer": "between 1910 and 1914", "latency_ms": 3396.6150283813477, "em": 0.0, "f1": 0.33333333333333326, "containment": 0.0, "context_chars": 1934}, "4": {"k": 4, "answer": "just before World War I", "latency_ms": 2674.1838455200195, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2616}, "5": {"k": 5, "answer": "just before World War I", "latency_ms": 3167.4046516418457, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3249}, "6": {"k": 6, "answer": "just before World War I", "latency_ms": 2477.4844646453857, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3931}}}
{"qid": "250542332339248886", "question": "where does the bob and tom show broadcast from", "gold_answers": ["WFBQ in Indianapolis , Indiana", "Indianapolis , Indiana"], "shots": {"0": {"k": 0, "answer": "Indianapolis, Indiana", "latency_ms": 2461.63010597229, "em": 1.0, "f1": 1.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "WFBQ, 94.7, Indianapolis", "latency_ms": 4921.07367515564, "em": 0.0, "f1": 0.5714285714285715, "containment": 0.0, "context_chars": 583}, "2": {"k": 2, "answer": "WFBQ, Indianapolis", "latency_ms": 3048.2540130615234, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1229}, "3": {"k": 3, "answer": "WFBQ, Indianapolis", "latency_ms": 3016.163110733032, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 1888}, "4": {"k": 4, "answer": "WFBQ, Indianapolis", "latency_ms": 3732.287645339966, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 2506}, "5": {"k": 5, "answer": "WFBQ, Indianapolis", "latency_ms": 3546.579360961914, "em": 0.0, "f1": 0.6666666666666665, "containment": 0.0, "context_chars": 3099}, "6": {"k": 6, "answer": "WFBQ, Indianapolis, Indiana", "latency_ms": 3840.0628566741943, "em": 0.0, "f1": 0.8571428571428571, "containment": 0.0, "context_chars": 3730}}}
{"qid": "7610758795954724809", "question": "star wars the clone wars season 3 episode 1", "gold_answers": ["Clone Cadets"], "shots": {"0": {"k": 0, "answer": "Clone Cadets", "latency_ms": 1804.1095733642578, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 2587.751626968384, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 641}, "2": {"k": 2, "answer": "I cannot answer.", "latency_ms": 2632.537603378296, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1338}, "3": {"k": 3, "answer": "I cannot answer.", "latency_ms": 3237.679958343506, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1978}, "4": {"k": 4, "answer": "I cannot answer.", "latency_ms": 2654.8547744750977, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2658}, "5": {"k": 5, "answer": "I cannot answer.", "latency_ms": 1862.288475036621, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3333}, "6": {"k": 6, "answer": "I cannot answer.", "latency_ms": 2088.671922683716, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 4014}}}
{"qid": "2997223939033016160", "question": "what part of brain is responsible for complex thinking", "gold_answers": ["the frontal lobe"], "shots": {"0": {"k": 0, "answer": "Prefrontal cortex", "latency_ms": 2045.7329750061035, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "Neocortex", "latency_ms": 2544.152021408081, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 711}, "2": {"k": 2, "answer": "Neocortex", "latency_ms": 2599.0936756134033, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1354}, "3": {"k": 3, "answer": "Neocortex", "latency_ms": 2315.516948699951, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2058}, "4": {"k": 4, "answer": "Cerebral cortex", "latency_ms": 2405.635356903076, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2688}, "5": {"k": 5, "answer": "Cerebral cortex", "latency_ms": 1581.538200378418, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3362}, "6": {"k": 6, "answer": "Cerebral cortex", "latency_ms": 2707.516670227051, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3978}}}
{"qid": "2680196226239522881", "question": "who won the champions league final in 2016", "gold_answers": ["Real Madrid"], "shots": {"0": {"k": 0, "answer": "Real Madrid", "latency_ms": 1298.269510269165, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 0}, "1": {"k": 1, "answer": "Jeonbuk Hyundai Motors", "latency_ms": 4010.727643966675, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 598}, "2": {"k": 2, "answer": "Jeonbuk Hyundai Motors", "latency_ms": 3396.5258598327637, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1262}, "3": {"k": 3, "answer": "Jeonbuk Hyundai Motors", "latency_ms": 2801.5189170837402, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1900}, "4": {"k": 4, "answer": "Real Madrid", "latency_ms": 1555.065631866455, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 2526}, "5": {"k": 5, "answer": "Real Madrid", "latency_ms": 2041.4283275604248, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3141}, "6": {"k": 6, "answer": "Real Madrid", "latency_ms": 1798.7494468688965, "em": 1.0, "f1": 1.0, "containment": 1.0, "context_chars": 3792}}}
{"qid": "-8458006812258546025", "question": "who has the most gold medals in the winter olympics of all time", "gold_answers": ["Norway"], "shots": {"0": {"k": 0, "answer": "Marit Bjoergen", "latency_ms": 3360.635757446289, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 0}, "1": {"k": 1, "answer": "I cannot answer.", "latency_ms": 3170.7231998443604, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 617}, "2": {"k": 2, "answer": "Marit Bjørgen", "latency_ms": 2679.4919967651367, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1222}, "3": {"k": 3, "answer": "Marit Bjørgen", "latency_ms": 2718.376636505127, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 1877}, "4": {"k": 4, "answer": "Marit Bjørgen", "latency_ms": 2456.8417072296143, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 2482}, "5": {"k": 5, "answer": "Marit Bjørgen", "latency_ms": 2772.5629806518555, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3169}, "6": {"k": 6, "answer": "Ole Einar Bjørndalen", "latency_ms": 3670.832395553589, "em": 0.0, "f1": 0.0, "containment": 0.0, "context_chars": 3814}}}
